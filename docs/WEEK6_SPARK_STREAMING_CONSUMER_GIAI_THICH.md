# Giáº£i thÃ­ch chi tiáº¿t: spark_streaming_consumer.py

**File:** `week6_streaming/spark_streaming_consumer.py`  
**Chá»©c nÄƒng:** Speed Layer Consumer - Spark Structured Streaming real-time processing  
**TÃ¡c giáº£:** ÄoÃ n Tháº¿ TÃ­n  
**NgÃ y:** Week 6 - Lambda Architecture

---

## ğŸ“‹ Má»¥c lá»¥c
1. [Configuration vÃ  Spark Session](#1-configuration-vÃ -spark-session)
2. [Schema Definition](#2-schema-definition)
3. [Read Stream from Kafka](#3-read-stream-from-kafka)
4. [Parse JSON Data](#4-parse-json-data)
5. [Data Transformation](#5-data-transformation)
6. [Watermarking](#6-watermarking)
7. [Window Aggregation](#7-window-aggregation)
8. [Write Streams](#8-write-streams)
9. [Monitoring vÃ  Termination](#9-monitoring-vÃ -termination)
10. [TÃ³m táº¯t](#tÃ³m-táº¯t-tá»•ng-quan)

---

## 1. Configuration vÃ  Spark Session

### DÃ²ng 1-11: Docstring
```python
"""
spark_streaming_consumer.py - Spark Structured Streaming Consumer

ÄÃ‚Y LÃ€ STRUCTURED STREAMING THáº¬T Sá»°:
- Äá»c continuous stream tá»« Kafka
- Micro-batch processing (trigger 10s)
- Watermarking (xá»­ lÃ½ late data)
- Window aggregation (1 day)
- Stateful operations
- Checkpoint (fault tolerance)
"""
```
**Giáº£i thÃ­ch:**
- **Structured Streaming:** Spark API cho real-time processing
- **Micro-batch:** Chia stream thÃ nh batches nhá» (10s/batch)
- **Watermarking:** Xá»­ lÃ½ data Ä‘áº¿n muá»™n (late arrival)
- **Window aggregation:** TÃ­nh toÃ¡n theo cá»­a sá»• thá»i gian (1 ngÃ y)
- **Stateful:** LÆ°u tráº¡ng thÃ¡i giá»¯a cÃ¡c batches
- **Checkpoint:** LÆ°u progress Ä‘á»ƒ recovery khi fail

---

### DÃ²ng 12-15: Import
```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
import os
```
**Giáº£i thÃ­ch:**
- `SparkSession`: Entry point cá»§a Spark
- `functions`: Táº¥t cáº£ functions (col, max, min, sum, count, ...)
- `types`: Data types (StringType, LongType, DoubleType, ...)
- `os`: Xá»­ lÃ½ Ä‘Æ°á»ng dáº«n file

---

### DÃ²ng 17-23: Configuration
```python
# ============================================================================
# CONFIGURATION
# ============================================================================
KAFKA_BOOTSTRAP_SERVERS = "localhost:9092"
KAFKA_TOPIC = "crypto-prices"
OUTPUT_PATH = "streaming_output_spark"
CHECKPOINT_PATH = "checkpoint_spark"
```
**Giáº£i thÃ­ch:**

| Variable | Value | Ã nghÄ©a |
|----------|-------|---------|
| `KAFKA_BOOTSTRAP_SERVERS` | `localhost:9092` | Kafka broker address |
| `KAFKA_TOPIC` | `crypto-prices` | Topic Ä‘á»ƒ subscribe |
| `OUTPUT_PATH` | `streaming_output_spark` | ThÆ° má»¥c lÆ°u output Parquet |
| `CHECKPOINT_PATH` | `checkpoint_spark` | ThÆ° má»¥c checkpoint (fault tolerance) |

**Checkpoint lÃ  gÃ¬?**
- LÆ°u **offset Kafka** Ä‘Ã£ process
- LÆ°u **state cá»§a aggregations**
- Khi restart â†’ Continue tá»« checkpoint (khÃ´ng duplicate)

---

### DÃ²ng 25-32: Banner
```python
# ============================================================================
# SPARK SESSION
# ============================================================================
print("=" * 80)
print("SPARK STRUCTURED STREAMING - Crypto Price Analysis")
print("=" * 80)
```
**Giáº£i thÃ­ch:** In header cho console.

---

### DÃ²ng 34-39: Spark Session Configuration
```python
spark = SparkSession.builder \
    .appName("CryptoPriceStructuredStreaming") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.3") \
    .config("spark.sql.streaming.checkpointLocation", CHECKPOINT_PATH) \
    .config("spark.sql.adaptive.enabled", "true") \
    .config("spark.driver.memory", "4g") \
    .getOrCreate()
```
**Giáº£i thÃ­ch tá»«ng config:**

#### `.appName("CryptoPriceStructuredStreaming")`
- TÃªn application (hiá»ƒn thá»‹ trong Spark UI)

#### `.config("spark.jars.packages", "...")`
- **Download Kafka connector dependency**
- `org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.3`
  - `spark-sql-kafka-0-10`: Connector cho Kafka
  - `_2.12`: Scala version 2.12
  - `3.5.3`: Spark version 3.5.3
- Tá»± Ä‘á»™ng download tá»« Maven Central khi start

#### `.config("spark.sql.streaming.checkpointLocation", CHECKPOINT_PATH)`
- Äáº·t checkpoint location máº·c Ä‘á»‹nh
- Náº¿u khÃ´ng set â†’ Má»—i query pháº£i specify riÃªng

#### `.config("spark.sql.adaptive.enabled", "true")`
- Báº­t **Adaptive Query Execution (AQE)**
- Tá»± Ä‘á»™ng tá»‘i Æ°u query plan runtime

#### `.config("spark.driver.memory", "4g")`
- Cáº¥p phÃ¡t 4GB RAM cho driver
- **Táº¡i sao cáº§n 4GB?**
  - Spark Streaming giá»¯ state trong memory
  - Aggregations cáº§n memory
  - Default 1GB thÆ°á»ng khÃ´ng Ä‘á»§

---

### DÃ²ng 41-44: Log Level & Info
```python
spark.sparkContext.setLogLevel("WARN")

print(f"\nâœ“ Spark {spark.version} initialized")
print(f"âœ“ Checkpoint location: {CHECKPOINT_PATH}")
print(f"âœ“ Output location: {OUTPUT_PATH}\n")
```
**Giáº£i thÃ­ch:**
- `setLogLevel("WARN")`: Chá»‰ log WARNING vÃ  ERROR
  - áº¨n INFO logs (quÃ¡ nhiá»u)
- Print thÃ´ng tin initialization

---

## 2. Schema Definition

### DÃ²ng 46-62: Message Schema
```python
# ============================================================================
# SCHEMA DEFINITION
# ============================================================================
# Schema cho JSON message tá»« Kafka
message_schema = StructType([
    StructField("symbol", StringType(), True),
    StructField("event_time", LongType(), True),
    StructField("price", DoubleType(), True),
    StructField("open", DoubleType(), True),
    StructField("high", DoubleType(), True),
    StructField("low", DoubleType(), True),
    StructField("volume", DoubleType(), True),
    StructField("quote_volume", DoubleType(), True),
    StructField("number_trades", IntegerType(), True),
    StructField("price_change", DoubleType(), True),
    StructField("price_change_percent", DoubleType(), True),
    StructField("timestamp", StringType(), True)
])
```
**Giáº£i thÃ­ch:**

#### StructType & StructField
- `StructType`: Container cho schema
- `StructField(name, dataType, nullable)`:
  - `name`: TÃªn field
  - `dataType`: Kiá»ƒu dá»¯ liá»‡u
  - `nullable`: True = Cho phÃ©p NULL

#### Táº¡i sao cáº§n schema?
- **from_json()** cáº§n schema Ä‘á»ƒ parse JSON
- KhÃ´ng cÃ³ schema â†’ KhÃ´ng parse Ä‘Æ°á»£c
- **VÃ­ dá»¥:**
  ```python
  # Kafka message (bytes):
  b'{"symbol":"BTCUSDT","price":42000.0,...}'
  
  # Parse vá»›i schema:
  Row(symbol='BTCUSDT', price=42000.0, ...)
  ```

#### Schema Fields
| Field | Type | Nullable | Source |
|-------|------|----------|--------|
| `symbol` | String | True | Producer |
| `event_time` | Long | True | Binance API (ms) |
| `price` | Double | True | Current price |
| `open` | Double | True | 24h open |
| `high` | Double | True | 24h high |
| `low` | Double | True | 24h low |
| `volume` | Double | True | 24h volume (BTC/ETH) |
| `quote_volume` | Double | True | 24h volume (USDT) |
| `number_trades` | Int | True | 24h trades count |
| `price_change` | Double | True | 24h change ($) |
| `price_change_percent` | Double | True | 24h change (%) |
| `timestamp` | String | True | Producer timestamp (ISO) |

---

## 3. Read Stream from Kafka

### DÃ²ng 64-67: Step 1 Header
```python
# ============================================================================
# STEP 1: READ STREAM FROM KAFKA
# ============================================================================
print("STEP 1: Reading stream from Kafka...")
```
**Giáº£i thÃ­ch:** Báº¯t Ä‘áº§u Step 1.

---

### DÃ²ng 69-78: Kafka Stream Reader
```python
kafkaDF = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", KAFKA_BOOTSTRAP_SERVERS) \
    .option("subscribe", KAFKA_TOPIC) \
    .option("startingOffsets", "earliest") \
    .option("failOnDataLoss", "false") \
    .option("maxOffsetsPerTrigger", 1000) \
    .option("kafka.session.timeout.ms", "30000") \
    .option("kafka.request.timeout.ms", "40000") \
    .load()
```
**Giáº£i thÃ­ch tá»«ng option:**

#### `.format("kafka")`
- DÃ¹ng Kafka source connector

#### `.option("kafka.bootstrap.servers", ...)`
- Kafka broker address: `localhost:9092`

#### `.option("subscribe", KAFKA_TOPIC)`
- Subscribe topic `crypto-prices`

#### `.option("startingOffsets", "earliest")`
- Äá»c tá»« Ä‘áº§u topic (offset Ä‘áº§u tiÃªn)
- **Alternatives:**
  - `"latest"`: Chá»‰ Ä‘á»c messages má»›i
  - `{"crypto-prices": {"0": 100}}`: Tá»« offset cá»¥ thá»ƒ

#### `.option("failOnDataLoss", "false")`
- **KhÃ´ng fail** náº¿u data bá»‹ máº¥t (Kafka retention)
- `true` â†’ Exception náº¿u offset khÃ´ng tá»“n táº¡i
- **Use case:** Kafka cÃ³ retention 7 ngÃ y, checkpoint > 7 ngÃ y â†’ Data loss

#### `.option("maxOffsetsPerTrigger", 1000)`
- **Rate limiting:** Tá»‘i Ä‘a 1000 messages/trigger
- TrÃ¡nh overload náº¿u cÃ³ backlog lá»›n
- VÃ­ dá»¥: CÃ³ 10,000 messages â†’ Process 10 batches (1000/batch)

#### `.option("kafka.session.timeout.ms", "30000")`
- **Session timeout:** 30 giÃ¢y
- Kafka broker sáº½ Ä‘á»£i 30s trÆ°á»›c khi coi consumer cháº¿t
- Default 10s (quÃ¡ ngáº¯n cho slow network)

#### `.option("kafka.request.timeout.ms", "40000")`
- **Request timeout:** 40 giÃ¢y
- Timeout cho má»—i Kafka request
- Pháº£i > session.timeout.ms

---

### DÃ²ng 80-81: Info
```python
print(f"âœ“ Connected to Kafka: {KAFKA_BOOTSTRAP_SERVERS}")
print(f"âœ“ Subscribed to topic: {KAFKA_TOPIC}\n")
```
**Giáº£i thÃ­ch:** Print success message.

---

### Kafka DataFrame Schema
```
kafkaDF schema:
â”œâ”€â”€ key: binary (null)
â”œâ”€â”€ value: binary (JSON message)
â”œâ”€â”€ topic: string (crypto-prices)
â”œâ”€â”€ partition: int (0)
â”œâ”€â”€ offset: long (0, 1, 2, ...)
â”œâ”€â”€ timestamp: timestamp (Kafka timestamp)
â””â”€â”€ timestampType: int (0=CreateTime, 1=LogAppendTime)
```

---

## 4. Parse JSON Data

### DÃ²ng 83-86: Step 2 Header
```python
# ============================================================================
# STEP 2: PARSE JSON DATA
# ============================================================================
print("STEP 2: Parsing JSON messages...")
```
**Giáº£i thÃ­ch:** Báº¯t Ä‘áº§u Step 2 - Parse JSON.

---

### DÃ²ng 88-92: JSON Parsing
```python
parsedDF = kafkaDF.select(
    from_json(col("value").cast("string"), message_schema).alias("data"),
    col("timestamp").alias("kafka_timestamp")
).select("data.*", "kafka_timestamp")
```
**Giáº£i thÃ­ch tá»«ng bÆ°á»›c:**

#### `.select(from_json(...), col("timestamp"))`
- Select 2 cá»™t:
  1. Parsed JSON (alias "data")
  2. Kafka timestamp (alias "kafka_timestamp")

#### `from_json(col("value").cast("string"), message_schema)`
- **Step 1:** `col("value")` â†’ Láº¥y column value (binary)
- **Step 2:** `.cast("string")` â†’ Chuyá»ƒn binary â†’ string
  - `b'{"symbol":"BTCUSDT",...}'` â†’ `'{"symbol":"BTCUSDT",...}'`
- **Step 3:** `from_json(..., message_schema)` â†’ Parse JSON
  - String â†’ StructType (nested columns)
- **Step 4:** `.alias("data")` â†’ Äáº·t tÃªn column "data"

#### `.select("data.*", "kafka_timestamp")`
- `data.*`: Unpack nested struct
  - `data.symbol`, `data.price`, ... â†’ `symbol`, `price`, ...
- `kafka_timestamp`: Giá»¯ Kafka timestamp

**VÃ­ dá»¥ transformation:**
```
Before:
â”œâ”€â”€ value: b'{"symbol":"BTCUSDT","price":42000.0}'
â””â”€â”€ timestamp: 2025-12-16 10:30:45

After from_json():
â”œâ”€â”€ data: {symbol: "BTCUSDT", price: 42000.0, ...}
â””â”€â”€ kafka_timestamp: 2025-12-16 10:30:45

After select("data.*"):
â”œâ”€â”€ symbol: "BTCUSDT"
â”œâ”€â”€ price: 42000.0
â”œâ”€â”€ ...
â””â”€â”€ kafka_timestamp: 2025-12-16 10:30:45
```

---

### DÃ²ng 94: Info
```python
print("âœ“ JSON parsed successfully\n")
```
**Giáº£i thÃ­ch:** Print success.

---

## 5. Data Transformation

### DÃ²ng 96-99: Step 3 Header
```python
# ============================================================================
# STEP 3: DATA TRANSFORMATION
# ============================================================================
print("STEP 3: Transforming data...")
```

---

### DÃ²ng 101-105: Timestamp Conversion
```python
streamDF = parsedDF \
    .withColumn("event_timestamp", (col("event_time") / 1000).cast("timestamp")) \
    .withColumn("date", to_date(col("event_timestamp"))) \
    .withColumn("hour", hour(col("event_timestamp")))
```
**Giáº£i thÃ­ch tá»«ng transformation:**

#### `.withColumn("event_timestamp", ...)`
- **Input:** `event_time` = 1734134400000 (milliseconds)
- **Transform:**
  1. `col("event_time") / 1000` â†’ 1734134400.0 (seconds)
  2. `.cast("timestamp")` â†’ 2025-12-16 10:30:00 (datetime)
- **Output:** Cá»™t `event_timestamp` kiá»ƒu timestamp

#### `.withColumn("date", to_date(col("event_timestamp")))`
- **Input:** `event_timestamp` = 2025-12-16 10:30:00
- **Transform:** `to_date()` â†’ Extract date
- **Output:** `date` = 2025-12-16 (date only)

#### `.withColumn("hour", hour(col("event_timestamp")))`
- **Input:** `event_timestamp` = 2025-12-16 10:30:00
- **Transform:** `hour()` â†’ Extract hour
- **Output:** `hour` = 10 (integer)

**Táº¡i sao cáº§n 3 cá»™t timestamp?**
- `event_time`: Original (ms) - LÆ°u trá»¯
- `event_timestamp`: Datetime - **Watermarking** vÃ  **Window aggregation**
- `date`: Date only - Partitioning, grouping
- `hour`: Hour - Hourly aggregations (náº¿u cáº§n)

---

### DÃ²ng 107-108: Info
```python
print("âœ“ Timestamps converted")
print("âœ“ Date and hour extracted\n")
```

---

## 6. Watermarking

### DÃ²ng 110-113: Step 4 Header
```python
# ============================================================================
# STEP 4: WATERMARKING (Handle Late Data)
# ============================================================================
print("STEP 4: Applying watermark...")
```

---

### DÃ²ng 115: Watermark
```python
watermarkedDF = streamDF.withWatermark("event_timestamp", "1 hour")
```
**Giáº£i thÃ­ch Watermarking:**

#### Watermark lÃ  gÃ¬?
- **Äá»‹nh nghÄ©a:** Threshold Ä‘á»ƒ drop late data
- **CÃ´ng thá»©c:** `watermark = max_event_time - threshold`
- **VÃ­ dá»¥:**
  ```
  Max event_time seen: 10:30:00
  Threshold: 1 hour
  â†’ Watermark: 09:30:00
  â†’ Drop data cÃ³ event_time < 09:30:00
  ```

#### Táº¡i sao cáº§n Watermarking?
- **Váº¥n Ä‘á»:** Data cÃ³ thá»ƒ Ä‘áº¿n muá»™n (late arrival)
  - Network delay
  - Producer restart
  - Kafka retention
- **KhÃ´ng cÃ³ watermark:**
  - Pháº£i giá»¯ state vÃ´ háº¡n (memory leak)
  - Window khÃ´ng bao giá» Ä‘Ã³ng
- **CÃ³ watermark:**
  - Sau 1 giá» â†’ ÄÃ³ng window, emit result
  - Drop data Ä‘áº¿n muá»™n > 1 giá»

#### Watermark Flow
```
Time 10:00 - Message arrives (event_time: 10:00)
  â†’ max_event_time = 10:00
  â†’ watermark = 10:00 - 1h = 09:00
  â†’ Accept data >= 09:00

Time 10:30 - Message arrives (event_time: 10:30)
  â†’ max_event_time = 10:30
  â†’ watermark = 10:30 - 1h = 09:30
  â†’ Accept data >= 09:30
  â†’ Drop data < 09:30

Time 11:00 - Late message (event_time: 08:50)
  â†’ watermark = 10:00
  â†’ 08:50 < 10:00 â†’ DROPPED
```

---

### DÃ²ng 117: Info
```python
print("âœ“ Watermark: 1 hour (late data tolerance)\n")
```

---

## 7. Window Aggregation

### DÃ²ng 119-122: Step 5 Header
```python
# ============================================================================
# STEP 5: WINDOW AGGREGATION - DAILY
# ============================================================================
print("STEP 5: Daily aggregation...")
```

---

### DÃ²ng 124-127: Group By Window
```python
dailyDF = watermarkedDF \
    .groupBy(
        window(col("event_timestamp"), "1 day"),
        col("symbol")
    ) \
```
**Giáº£i thÃ­ch:**

#### `.groupBy(window(...), col("symbol"))`
- **Group by 2 keys:**
  1. `window(col("event_timestamp"), "1 day")`: Tumbling window 1 ngÃ y
  2. `col("symbol")`: Symbol (BTCUSDT/ETHUSDT)

#### `window(col("event_timestamp"), "1 day")`
- **Tumbling Window:** KhÃ´ng overlap
- **Size:** 1 ngÃ y (24 giá»)
- **VÃ­ dá»¥ windows:**
  ```
  Window 1: 2025-12-16 00:00:00 â†’ 2025-12-17 00:00:00
  Window 2: 2025-12-17 00:00:00 â†’ 2025-12-18 00:00:00
  Window 3: 2025-12-18 00:00:00 â†’ 2025-12-19 00:00:00
  ```

#### Window Types
| Type | Description | Example |
|------|-------------|---------|
| **Tumbling** | KhÃ´ng overlap | [0-1h], [1-2h], [2-3h] |
| **Sliding** | CÃ³ overlap | [0-1h], [0.5-1.5h], [1-2h] |
| **Session** | Dynamic size | Dá»±a vÃ o gaps |

- **File nÃ y dÃ¹ng:** Tumbling window

---

### DÃ²ng 128-138: Aggregations
```python
    .agg(
        first("open").alias("daily_open"),
        max("high").alias("daily_high"),
        min("low").alias("daily_low"),
        last("price").alias("daily_close"),  # Use 'price' (lastPrice from Binance)
        sum("volume").alias("daily_volume"),
        sum("quote_volume").alias("daily_quote_volume"),
        sum("number_trades").alias("total_trades"),
        count("*").alias("tick_count"),
        avg("price").alias("avg_price")
    ) \
```
**Giáº£i thÃ­ch tá»«ng aggregation:**

| Function | Input | Output | Ã nghÄ©a |
|----------|-------|--------|---------|
| `first("open")` | `open` | `daily_open` | GiÃ¡ open **Ä‘áº§u tiÃªn** trong window |
| `max("high")` | `high` | `daily_high` | GiÃ¡ **cao nháº¥t** trong window |
| `min("low")` | `low` | `daily_low` | GiÃ¡ **tháº¥p nháº¥t** trong window |
| `last("price")` | `price` | `daily_close` | GiÃ¡ **cuá»‘i cÃ¹ng** trong window |
| `sum("volume")` | `volume` | `daily_volume` | **Tá»•ng** volume BTC/ETH |
| `sum("quote_volume")` | `quote_volume` | `daily_quote_volume` | **Tá»•ng** volume USDT |
| `sum("number_trades")` | `number_trades` | `total_trades` | **Tá»•ng** sá»‘ trades |
| `count("*")` | All rows | `tick_count` | **Sá»‘ messages** trong window |
| `avg("price")` | `price` | `avg_price` | **GiÃ¡ trung bÃ¬nh** |

**ChÃº Ã½:**
- `first("open")`: Láº¥y open cá»§a **message Ä‘áº§u tiÃªn** (theo event_timestamp)
- `last("price")`: Láº¥y price cá»§a **message cuá»‘i cÃ¹ng** (theo event_timestamp)
- **Táº¡i sao dÃ¹ng `price` thay vÃ¬ `close`?**
  - `close` lÃ  close cá»§a 24h window (Binance API)
  - `price` lÃ  current price (lastPrice)
  - Muá»‘n close cá»§a daily window â†’ DÃ¹ng `price` cuá»‘i cÃ¹ng

---

### DÃ²ng 139-151: Select Columns
```python
    .select(
        col("window.start").alias("date"),
        col("symbol"),
        col("daily_open"),
        col("daily_high"),
        col("daily_low"),
        col("daily_close"),
        col("daily_volume"),
        col("daily_quote_volume"),
        col("total_trades"),
        col("tick_count"),
        col("avg_price")
    )
```
**Giáº£i thÃ­ch:**

#### `col("window.start").alias("date")`
- `window` lÃ  StructType vá»›i 2 fields:
  - `window.start`: Timestamp báº¯t Ä‘áº§u window
  - `window.end`: Timestamp káº¿t thÃºc window
- Láº¥y `start` vÃ  Ä‘áº·t alias `date`
- **VÃ­ dá»¥:**
  ```
  window.start: 2025-12-16 00:00:00
  window.end: 2025-12-17 00:00:00
  â†’ date: 2025-12-16 00:00:00
  ```

#### Select 11 cá»™t
- `date`: Window start (timestamp)
- `symbol`: BTCUSDT/ETHUSDT
- `daily_open/high/low/close`: OHLC
- `daily_volume/quote_volume`: Volume
- `total_trades`: Sá»‘ trades
- `tick_count`: Sá»‘ messages
- `avg_price`: GiÃ¡ trung bÃ¬nh

---

### DÃ²ng 153-154: Info
```python
print("âœ“ Window: 1 day")
print("âœ“ Aggregations: OHLC, Volume, Trades\n")
```

---

## 8. Write Streams

### DÃ²ng 156-159: Step 6 Header
```python
# ============================================================================
# STEP 6: WRITE STREAMS
# ============================================================================
print("STEP 6: Starting streaming queries...\n")
```

---

### DÃ²ng 161-170: Query 1 - Daily Parquet
```python
# Query 1: Daily data to Parquet
daily_query = dailyDF.writeStream \
    .outputMode("append") \
    .format("parquet") \
    .option("path", f"{OUTPUT_PATH}/daily") \
    .option("checkpointLocation", f"{CHECKPOINT_PATH}/daily") \
    .partitionBy("symbol") \
    .trigger(processingTime="10 seconds") \
    .start()
```
**Giáº£i thÃ­ch tá»«ng option:**

#### `.outputMode("append")`
- **Append mode:** Chá»‰ ghi rows má»›i
- **Alternatives:**
  - `"complete"`: Ghi láº¡i toÃ n bá»™ result table (cho aggregations)
  - `"update"`: Chá»‰ ghi rows changed
- **Táº¡i sao append?**
  - Window Ä‘Ã³ng â†’ Emit 1 láº§n â†’ KhÃ´ng update
  - Append mode hiá»‡u quáº£ nháº¥t

#### `.format("parquet")`
- Ghi output dáº¡ng Parquet file

#### `.option("path", f"{OUTPUT_PATH}/daily")`
- ÄÆ°á»ng dáº«n: `streaming_output_spark/daily/`

#### `.option("checkpointLocation", f"{CHECKPOINT_PATH}/daily")`
- Checkpoint path: `checkpoint_spark/daily/`
- LÆ°u offset Kafka, state aggregations

#### `.partitionBy("symbol")`
- Partition theo symbol
- Folder structure:
  ```
  streaming_output_spark/daily/
  â”œâ”€â”€ symbol=BTCUSDT/
  â”‚   â”œâ”€â”€ part-00000-xxx.parquet
  â”‚   â””â”€â”€ part-00001-xxx.parquet
  â””â”€â”€ symbol=ETHUSDT/
      â”œâ”€â”€ part-00000-xxx.parquet
      â””â”€â”€ part-00001-xxx.parquet
  ```

#### `.trigger(processingTime="10 seconds")`
- **Micro-batch trigger:** Má»—i 10 giÃ¢y
- Process messages accumulated trong 10s
- **Alternatives:**
  - `trigger(once=True)`: Cháº¡y 1 láº§n rá»“i dá»«ng
  - `trigger(continuous="1 second")`: Continuous mode (experimental)

#### `.start()`
- Start streaming query (async)
- Return `StreamingQuery` object

---

### DÃ²ng 172-175: Info
```python
print("âœ“ Query 1: Daily aggregates â†’ Parquet")
print(f"  Output: {OUTPUT_PATH}/daily")
print(f"  Trigger: 10 seconds")
```

---

### DÃ²ng 177-189: Query 2 - Console Monitoring
```python
# Query 2: Raw stream to Console (monitoring)
console_query = streamDF \
    .select(
        col("symbol"),
        col("price"),
        col("volume"),
        col("price_change_percent"),
        col("event_timestamp")
    ) \
    .writeStream \
    .outputMode("append") \
    .format("console") \
    .option("truncate", "false") \
    .option("numRows", "10") \
    .trigger(processingTime="30 seconds") \
    .start()
```
**Giáº£i thÃ­ch:**

#### Select 5 cá»™t
- `symbol`, `price`, `volume`, `price_change_percent`, `event_timestamp`
- Chá»‰ select columns quan trá»ng (trÃ¡nh spam console)

#### `.format("console")`
- Ghi output ra console (terminal)

#### `.option("truncate", "false")`
- KhÃ´ng cáº¯t text (hiá»ƒn thá»‹ full)
- Default `true`: Cáº¯t strings > 20 chars

#### `.option("numRows", "10")`
- Chá»‰ show 10 rows/batch
- TrÃ¡nh spam console

#### `.trigger(processingTime="30 seconds")`
- Má»—i 30 giÃ¢y (Ã­t hÆ¡n daily query)
- TrÃ¡nh log quÃ¡ nhiá»u

**Output vÃ­ dá»¥:**
```
-------------------------------------------
Batch: 1
-------------------------------------------
+--------+--------+----------+-------------------+-------------------+
|symbol  |price   |volume    |price_change_percent|event_timestamp    |
+--------+--------+----------+-------------------+-------------------+
|BTCUSDT |42000.50|12345.67  |-2.38              |2025-12-16 10:30:00|
|ETHUSDT |3200.00 |45678.90  |1.25               |2025-12-16 10:30:01|
+--------+--------+----------+-------------------+-------------------+
```

---

### DÃ²ng 191-192: Info
```python
print("âœ“ Query 3: Raw data â†’ Console (monitoring)")
print(f"  Trigger: 30 seconds")
```

---

### DÃ²ng 194-200: Query 3 - Memory Table
```python
# Query 4: Real-time stats to Memory (for queries)
stats_query = dailyDF.writeStream \
    .outputMode("complete") \
    .format("memory") \
    .queryName("crypto_daily_stats") \
    .trigger(processingTime="10 seconds") \
    .start()
```
**Giáº£i thÃ­ch:**

#### `.outputMode("complete")`
- **Complete mode:** Ghi láº¡i **toÃ n bá»™** result table
- Cáº§n cho in-memory table (query Ä‘Æ°á»£c full data)

#### `.format("memory")`
- Ghi vÃ o **in-memory table** (khÃ´ng ra file)
- Store trong Spark catalog

#### `.queryName("crypto_daily_stats")`
- TÃªn table: `crypto_daily_stats`
- Query Ä‘Æ°á»£c báº±ng SQL:
  ```python
  spark.sql("SELECT * FROM crypto_daily_stats").show()
  ```

#### Use case
- **Real-time monitoring:** Query data Ä‘ang stream
- **Dashboard:** Power BI, Tableau connect vÃ o Spark
- **Ad-hoc queries:** Debug, explore data

**VÃ­ dá»¥ query:**
```python
# Terminal 2 (while streaming Ä‘ang cháº¡y):
spark.sql("""
  SELECT symbol, date, daily_close, daily_volume
  FROM crypto_daily_stats
  ORDER BY date DESC
  LIMIT 5
""").show()
```

---

### DÃ²ng 202-203: Info
```python
print("âœ“ Query 4: Daily stats â†’ Memory table")
print(f"  Table name: crypto_daily_stats")
```

---

## 9. Monitoring vÃ  Termination

### DÃ²ng 205-218: Monitoring Info
```python
# ============================================================================
# MONITORING
# ============================================================================
print("\n" + "=" * 80)
print("STREAMING QUERIES ACTIVE")
print("=" * 80)
print("\nActive queries:")
for query in spark.streams.active:
    print(f"  - {query.name if query.name else query.id}")

print("\nğŸ“Š Monitor status:")
print("  - Console output will show every 30 seconds")
print("  - Parquet files updated every 10 seconds")
print("  - Check checkpoint/ for progress")
print("  - Check streaming_output_spark/ for results")
```
**Giáº£i thÃ­ch:**

#### `spark.streams.active`
- List táº¥t cáº£ streaming queries Ä‘ang cháº¡y
- Má»—i query cÃ³:
  - `id`: UUID duy nháº¥t
  - `name`: TÃªn (náº¿u set báº±ng `.queryName()`)

#### Monitoring Tips
- **Console:** Show raw data má»—i 30s
- **Parquet:** Write má»—i 10s
- **Checkpoint:** LÆ°u progress (offsets, state)
- **Output:** Káº¿t quáº£ cuá»‘i cÃ¹ng

---

### DÃ²ng 220-221: Query Instructions
```python
print("\nğŸ’¡ To query in-memory stats, open another terminal:")
print("  spark.sql('SELECT * FROM crypto_daily_stats').show()")
```

---

### DÃ²ng 223-225: Stop Instructions
```python
print("\nPress Ctrl+C to stop all queries\n")
print("=" * 80)
```

---

### DÃ²ng 227-232: Wait for Termination
```python
# ============================================================================
# WAIT FOR TERMINATION
# ============================================================================
try:
    # Wait for all queries
    spark.streams.awaitAnyTermination()
```
**Giáº£i thÃ­ch:**

#### `spark.streams.awaitAnyTermination()`
- **Block main thread** cho Ä‘áº¿n khi 1 query terminate
- Náº¿u khÃ´ng cÃ³ â†’ Program exit ngay (queries sáº½ stop)
- **Táº¡i sao cáº§n?**
  - Streaming queries cháº¡y async (background threads)
  - Main thread pháº£i chá» Ä‘á»ƒ keep program alive

---

### DÃ²ng 234-243: Graceful Shutdown
```python
except KeyboardInterrupt:
    print("\n\nâ¹ Stopping all streaming queries...")
    
    # Stop all queries gracefully
    for query in spark.streams.active:
        print(f"  Stopping: {query.name if query.name else query.id}")
        query.stop()
    
    print("\nâœ“ All queries stopped")
    print("âœ“ Checkpoints saved")
```
**Giáº£i thÃ­ch:**

#### `except KeyboardInterrupt`
- Báº¯t Ctrl+C

#### `query.stop()`
- **Graceful stop:**
  1. Finish current micro-batch
  2. Save checkpoint
  3. Close resources
- **KhÃ´ng dÃ¹ng `kill -9`** (máº¥t checkpoint!)

#### Táº¡i sao quan trá»ng?
- **Checkpoint saved:** Resume Ä‘Æ°á»£c tá»« Ä‘Ãºng offset
- **No data loss:** Finish processing batch hiá»‡n táº¡i
- **Clean state:** KhÃ´ng corrupt files

---

### DÃ²ng 245-258: Final Statistics
```python
    # Show final statistics
    print("\n" + "=" * 80)
    print("FINAL STATISTICS")
    print("=" * 80)
    
    try:
        daily_stats = spark.sql("SELECT * FROM crypto_daily_stats")
        print("\nDaily aggregates:")
        daily_stats.show(10, truncate=False)
    except:
        print("No daily stats available yet")
    
    print("\n" + "=" * 80)
    
    spark.stop()
    print("\nâœ“ Spark session closed")
```
**Giáº£i thÃ­ch:**

#### Query Final Stats
- Láº¥y data tá»« in-memory table
- Show 10 rows cuá»‘i
- Náº¿u chÆ°a cÃ³ data (window chÆ°a Ä‘Ã³ng) â†’ Print warning

#### `spark.stop()`
- Dá»«ng Spark session
- Giáº£i phÃ³ng resources (memory, threads)

---

---

# TÃ³m táº¯t Tá»•ng quan

## ğŸ¯ Má»¥c Ä‘Ã­ch File
File `spark_streaming_consumer.py` lÃ  **Speed Layer Consumer (Production)** trong Lambda Architecture - Xá»­ lÃ½ real-time stream tá»« Kafka báº±ng Spark Structured Streaming vá»›i watermarking vÃ  window aggregation.

---

## ğŸ“Š Workflow (6 Steps)

### **1. Read Stream from Kafka**
- Connect to Kafka broker (`localhost:9092`)
- Subscribe topic `crypto-prices`
- Read from earliest offset
- Rate limiting: 1000 messages/trigger

### **2. Parse JSON Data**
- Convert Kafka binary â†’ JSON string
- Parse vá»›i predefined schema
- Unpack nested struct â†’ flat columns

### **3. Data Transformation**
- Convert Unix timestamp (ms) â†’ Datetime
- Extract date vÃ  hour
- Prepare for windowing

### **4. Watermarking**
- Apply 1-hour watermark
- Drop late data > 1 hour
- Enable window closing

### **5. Window Aggregation**
- Tumbling window: 1 day
- Group by: (window, symbol)
- Aggregations: OHLC, Volume, Trades

### **6. Write Streams**
- **Query 1:** Daily â†’ Parquet (append, 10s trigger)
- **Query 2:** Raw â†’ Console (append, 30s trigger)
- **Query 3:** Daily â†’ Memory (complete, 10s trigger)

---

## ğŸ”‘ Äiá»ƒm Quan Trá»ng

### **1. Structured Streaming vs Batch**
| Aspect | Batch | Streaming |
|--------|-------|-----------|
| Processing | Full dataset at once | Incremental micro-batches |
| Latency | Minutes to hours | Seconds |
| State | Stateless | Stateful (aggregations) |
| Fault Tolerance | Rerun entire job | Checkpoint + replay |

### **2. Watermarking**
```
Purpose: Handle late data vÃ  close windows
Threshold: 1 hour
Formula: watermark = max_event_time - 1h
Effect: Drop data cÃ³ event_time < watermark
```

**Táº¡i sao cáº§n watermark?**
- KhÃ´ng cÃ³ â†’ Window khÃ´ng bao giá» Ä‘Ã³ng (memory leak)
- CÃ³ â†’ Window Ä‘Ã³ng sau 1h, emit result

### **3. Window Types**
File nÃ y dÃ¹ng **Tumbling Window:**
- Size: 1 day (24h)
- No overlap
- Windows: [00:00-24:00], [24:00-48:00], ...

**Khi nÃ o window Ä‘Ã³ng?**
- Watermark vÆ°á»£t qua `window.end`
- **VÃ­ dá»¥:**
  ```
  Window: 2025-12-16 00:00 â†’ 2025-12-17 00:00
  Window.end: 2025-12-17 00:00
  Watermark: 2025-12-17 01:00 (max_event_time: 2025-12-17 02:00 - 1h)
  â†’ Window Ä‘Ã³ng, emit result
  ```

### **4. Output Modes**
| Mode | Description | Use Case |
|------|-------------|----------|
| **Append** | Chá»‰ ghi rows má»›i | Daily Parquet (window Ä‘Ã³ng â†’ emit 1 láº§n) |
| **Update** | Ghi rows changed | Real-time updates |
| **Complete** | Ghi láº¡i toÃ n bá»™ | Memory table (query full data) |

### **5. Checkpoint**
**LÆ°u gÃ¬?**
- Kafka offsets (Ä‘Ã£ process Ä‘áº¿n Ä‘Ã¢u)
- Aggregation state (window states)
- Metadata (query config)

**Táº¡i sao quan trá»ng?**
- **Exactly-once semantics:** KhÃ´ng duplicate/loss data
- **Fault tolerance:** Restart tá»« checkpoint (khÃ´ng reprocess)
- **State recovery:** Giá»¯ aggregations khi restart

**CÃ¡ch hoáº¡t Ä‘á»™ng:**
```
Batch 1: Process offset 0-99 â†’ Save checkpoint (offset=100)
Batch 2: Process offset 100-199 â†’ Save checkpoint (offset=200)
[CRASH]
Restart: Read checkpoint â†’ Resume from offset 200
```

---

## ğŸ“ Output Structure

### **1. Parquet Output**
```
streaming_output_spark/daily/
â”œâ”€â”€ symbol=BTCUSDT/
â”‚   â”œâ”€â”€ part-00000-xxx.parquet
â”‚   â”œâ”€â”€ part-00001-xxx.parquet
â”‚   â””â”€â”€ _spark_metadata/
â””â”€â”€ symbol=ETHUSDT/
    â”œâ”€â”€ part-00000-xxx.parquet
    â””â”€â”€ _spark_metadata/
```

### **2. Checkpoint**
```
checkpoint_spark/daily/
â”œâ”€â”€ commits/
â”‚   â”œâ”€â”€ 0
â”‚   â”œâ”€â”€ 1
â”‚   â””â”€â”€ 2
â”œâ”€â”€ offsets/
â”‚   â”œâ”€â”€ 0
â”‚   â”œâ”€â”€ 1
â”‚   â””â”€â”€ 2
â”œâ”€â”€ metadata
â””â”€â”€ state/
    â””â”€â”€ 0/
        â””â”€â”€ 0/
```

### **3. Schema Output**
| Column | Type | Example |
|--------|------|---------|
| `date` | timestamp | 2025-12-16 00:00:00 |
| `symbol` | string | BTCUSDT |
| `daily_open` | double | 43000.0 |
| `daily_high` | double | 43500.0 |
| `daily_low` | double | 41800.0 |
| `daily_close` | double | 42000.0 |
| `daily_volume` | double | 12345.67 |
| `daily_quote_volume` | double | 520000000.0 |
| `total_trades` | long | 123456 |
| `tick_count` | long | 1008 |
| `avg_price` | double | 42250.0 |

---

## ğŸ’¡ Use Cases

### **Khi nÃ o cháº¡y file nÃ y?**
1. âœ… **Production 24/7:** Continuous real-time processing
2. âœ… Sau khi start Kafka vÃ  Producer
3. âœ… Muá»‘n exactly-once semantics
4. âœ… Cáº§n fault tolerance (checkpoint recovery)

### **Khi nÃ o KHÃ”NG cháº¡y?**
- âŒ **Demo nhanh:** DÃ¹ng `kafka_batch_reader.py` thay tháº¿
  - LÃ½ do: Window 1 ngÃ y cáº§n Ä‘á»£i 24h Ä‘á»ƒ Ä‘Ã³ng
- âŒ Kafka chÆ°a start
- âŒ Producer chÆ°a gá»­i data (sáº½ Ä‘á»£i mÃ£i)
- âŒ KhÃ´ng muá»‘n Ä‘á»£i lÃ¢u (window closure)

---

## ğŸš€ CÃ¡ch Sá»­ Dá»¥ng

### **1. Start Infrastructure**
```bash
cd week6_streaming
docker-compose up -d
```

### **2. Start Producer**
```bash
python websocket_producer.py
# Äá»ƒ cháº¡y Ã­t nháº¥t 24 giá»
```

### **3. Start Consumer (Terminal 2)**
```bash
python spark_streaming_consumer.py
# Äá»£i 24-25 giá» Ä‘á»ƒ window Ä‘Ã³ng
```

### **4. Expected Output**
```
================================================================================
SPARK STRUCTURED STREAMING - Crypto Price Analysis
================================================================================

âœ“ Spark 3.5.3 initialized
âœ“ Checkpoint location: checkpoint_spark
âœ“ Output location: streaming_output_spark

...

================================================================================
STREAMING QUERIES ACTIVE
================================================================================

Active queries:
  - None
  - None
  - crypto_daily_stats

ğŸ“Š Monitor status:
  - Console output will show every 30 seconds
  - Parquet files updated every 10 seconds
```

### **5. Monitor Console Output (Má»—i 30s)**
```
-------------------------------------------
Batch: 5
-------------------------------------------
+--------+--------+----------+-------------------+-------------------+
|symbol  |price   |volume    |price_change_percent|event_timestamp    |
+--------+--------+----------+-------------------+-------------------+
|BTCUSDT |42000.50|12345.67  |-2.38              |2025-12-16 10:30:00|
|ETHUSDT |3200.00 |45678.90  |1.25               |2025-12-16 10:30:01|
+--------+--------+----------+-------------------+-------------------+
```

### **6. Query Memory Table (Terminal 3)**
```python
from pyspark.sql import SparkSession
spark = SparkSession.builder.getOrCreate()

# Query in-memory stats
spark.sql("""
  SELECT symbol, date, daily_close, daily_volume
  FROM crypto_daily_stats
  ORDER BY date DESC
""").show(truncate=False)
```

### **7. Stop Gracefully**
```bash
# Press Ctrl+C in Terminal 2
â¹ Stopping all streaming queries...
  Stopping: None
  Stopping: None
  Stopping: crypto_daily_stats

âœ“ All queries stopped
âœ“ Checkpoints saved
```

---

## ğŸ”§ Troubleshooting

### **1. No Output Files**
**Triá»‡u chá»©ng:** `streaming_output_spark/daily/` trá»‘ng  
**NguyÃªn nhÃ¢n:** Window chÆ°a Ä‘Ã³ng (chÆ°a Ä‘á»§ 24h + 1h watermark)  
**Giáº£i phÃ¡p:**
- Äá»£i Ã­t nháº¥t 25 giá»
- Hoáº·c dÃ¹ng `kafka_batch_reader.py` (instant output)

### **2. Kafka Connection Timeout**
**Error:** `TimeoutException: Failed to get records for crypto-prices`  
**Giáº£i phÃ¡p:**
```bash
# Check Kafka running
docker ps | grep kafka

# Check topic exists
docker exec -it kafka_container kafka-topics --list --bootstrap-server localhost:9092

# Increase timeout
.option("kafka.session.timeout.ms", "60000")
```

### **3. Out of Memory**
**Error:** `java.lang.OutOfMemoryError: Java heap space`  
**Giáº£i phÃ¡p:**
```python
# TÄƒng driver memory
.config("spark.driver.memory", "8g")

# Hoáº·c giáº£m maxOffsetsPerTrigger
.option("maxOffsetsPerTrigger", 500)
```

### **4. Checkpoint Corruption**
**Error:** `IllegalStateException: Checkpoint directory corrupted`  
**Giáº£i phÃ¡p:**
```bash
# XÃ³a checkpoint vÃ  restart tá»« Ä‘áº§u
rm -rf checkpoint_spark
python spark_streaming_consumer.py
```

### **5. Late Data Dropped**
**Triá»‡u chá»©ng:** Thiáº¿u rows trong output  
**NguyÃªn nhÃ¢n:** Watermark drop data > 1h  
**Giáº£i phÃ¡p:**
```python
# TÄƒng watermark threshold
.withWatermark("event_timestamp", "2 hours")
```

---

## ğŸ“ˆ Performance

### **Throughput**
- **Input:** 2 msg/s tá»« Producer
- **Trigger:** 10s (process 20 messages/batch)
- **Window:** 1 day (accumulate 172,800 messages)

### **Latency**
- **Micro-batch:** 10s (trigger interval)
- **Window closure:** 24h (window size) + 1h (watermark) = **25 giá»**
- **End-to-end:** Kafka â†’ Spark â†’ Parquet ~ 10-20s

### **Resource Usage**
- **Memory:** 4GB driver + 2GB executor
- **CPU:** 2-4 cores (parallel processing)
- **Disk:** ~100 KB/day/symbol (Parquet compressed)

---

## ğŸ“ Key Technologies

- **Spark Structured Streaming:** Real-time processing framework
- **Kafka Consumer:** Spark Kafka connector
- **Watermarking:** Late data handling
- **Tumbling Windows:** Non-overlapping time windows
- **Checkpoint:** Fault tolerance + exactly-once
- **Micro-batching:** Batches every 10s
- **Parquet:** Columnar storage format

---

## ğŸ”— Integration

### **Lambda Architecture Flow**
```
Binance API
  â†“
websocket_producer.py (Speed Layer Producer)
  â†“ (Kafka: crypto-prices)
spark_streaming_consumer.py (Speed Layer Consumer)
  â†“ (Parquet: streaming_output_spark/daily/)
week6_merge.py (Serving Layer)
  â†“ (Merge Batch + Speed)
prophet_train.py (ML Layer)
```

### **Alternative Flow (Demo)**
```
websocket_producer.py
  â†“ (Kafka: crypto-prices)
kafka_batch_reader.py (Batch mode - Instant output)
  â†“ (Parquet: streaming_output_spark_BATCH/)
week6_merge.py
```

---

## âš ï¸ Production vs Demo

| Aspect | Production (File nÃ y) | Demo (kafka_batch_reader.py) |
|--------|----------------------|------------------------------|
| **Processing** | Streaming (continuous) | Batch (one-time) |
| **Window** | 1 day (24h) | Batch entire topic |
| **Watermark** | 1 hour | N/A |
| **Output Time** | 25 hours | 1-2 seconds |
| **Use Case** | 24/7 real-time | Quick demo (5-10 min) |
| **Checkpoint** | Yes (recovery) | No |

**Recommendation cho demo:**
- âœ… DÃ¹ng `kafka_batch_reader.py` Ä‘á»ƒ show káº¿t quáº£ nhanh
- âœ… Giáº£i thÃ­ch file nÃ y lÃ  production version (24h window)
- âœ… Show code vÃ  architecture (khÃ´ng cáº§n cháº¡y tháº­t)

---

**TÃ¡c giáº£:** ÄoÃ n Tháº¿ TÃ­n  
**MSSV:** 4551190056  
**File:** `week6_streaming/spark_streaming_consumer.py`  
**Lines:** 263 dÃ²ng code  
**Má»¥c Ä‘Ã­ch:** Speed Layer Consumer (Production) - Real-time processing vá»›i Spark Structured Streaming

---
