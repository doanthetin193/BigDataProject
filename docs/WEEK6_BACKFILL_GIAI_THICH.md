# Gi·∫£i th√≠ch chi ti·∫øt: week6_backfill.py

**File:** `scripts/lambda_batch/week6_backfill.py`  
**Ch·ª©c nƒÉng:** Backfill missing dates t·ª´ Binance API cho Batch Layer  
**T√°c gi·∫£:** ƒêo√†n Th·∫ø T√≠n  
**Ng√†y:** Week 6 - Lambda Architecture

---

## üìã M·ª•c l·ª•c
1. [Import v√† kh·ªüi t·∫°o](#1-import-v√†-kh·ªüi-t·∫°o)
2. [Step 1: Detect Last Date](#2-step-1-detect-last-date)
3. [Step 2: Fetch t·ª´ Binance API](#3-step-2-fetch-t·ª´-binance-api)
4. [Step 3: Clean Data](#4-step-3-clean-data)
5. [Step 4: Aggregate Daily OHLC](#5-step-4-aggregate-daily-ohlc)
6. [Step 5: Forward Fill Missing Dates](#6-step-5-forward-fill-missing-dates)
7. [Step 6: Prepare Backfill Data](#7-step-6-prepare-backfill-data)
8. [Step 7: Merge & Recalculate MA](#8-step-7-merge--recalculate-ma)
9. [Step 8: Extract Prophet Input](#9-step-8-extract-prophet-input)
10. [T√≥m t·∫Øt](#t√≥m-t·∫Øt-t·ªïng-quan)

---

## 1. Import v√† Kh·ªüi t·∫°o

### D√≤ng 1-5: Docstring
```python
"""
Week 6 - Batch Layer (Lambda Architecture)
Backfill missing dates from Binance API
"""
```
**Gi·∫£i th√≠ch:** Header m√¥ t·∫£ m·ª•c ƒë√≠ch c·ªßa file - backfill gaps (kho·∫£ng tr·ªëng) trong d·ªØ li·ªáu b·∫±ng c√°ch l·∫•y t·ª´ Binance API.

---

### D√≤ng 6-17: Import PySpark
```python
from pyspark.sql import SparkSession
from pyspark.sql import functions as F
from pyspark.sql.functions import (
    col, from_unixtime, to_date, year, month, first, last, max, min, 
    sum, count, avg, lit, sequence, explode, lag, coalesce
)
from pyspark.sql.window import Window
from pyspark.sql.types import LongType, DoubleType
```
**Gi·∫£i th√≠ch:**
- `SparkSession`: ƒêi·ªÉm v√†o ch√≠nh c·ªßa Spark, d√πng ƒë·ªÉ t·∫°o DataFrame
- `functions as F`: Alias cho c√°c h√†m Spark SQL
- `col, from_unixtime, to_date...`: C√°c h√†m x·ª≠ l√Ω d·ªØ li·ªáu:
  - `col()`: Truy c·∫≠p c·ªôt
  - `from_unixtime()`: Chuy·ªÉn Unix timestamp ‚Üí datetime
  - `to_date()`: Extract date t·ª´ datetime
  - `year(), month()`: Extract year/month
  - `max(), min(), sum(), count(), avg()`: Aggregation functions
  - `sequence()`: T·∫°o d√£y s·ªë/ng√†y
  - `explode()`: Chuy·ªÉn array ‚Üí rows
  - `lag()`: L·∫•y gi√° tr·ªã row tr∆∞·ªõc ƒë√≥ (window function)
  - `coalesce()`: L·∫•y gi√° tr·ªã non-null ƒë·∫ßu ti√™n
- `Window`: D√πng cho window functions (x·ª≠ l√Ω theo c·ª≠a s·ªï d·ªØ li·ªáu)
- `LongType, DoubleType`: Ki·ªÉu d·ªØ li·ªáu Spark

---

### D√≤ng 18-21: Import Python Libraries
```python
import requests
import pandas as pd
from datetime import datetime, timedelta
import time
import os
```
**Gi·∫£i th√≠ch:**
- `requests`: Call HTTP API (Binance)
- `pandas`: X·ª≠ l√Ω data d·∫°ng DataFrame (Python, kh√¥ng ph·∫£i Spark)
- `datetime, timedelta`: X·ª≠ l√Ω ng√†y th√°ng
- `time`: Sleep/delay gi·ªØa c√°c API calls
- `os`: X·ª≠ l√Ω ƒë∆∞·ªùng d·∫´n file

---

### D√≤ng 23-24: Base Directory
```python
base_dir = r"D:\BigDataProject"
```
**Gi·∫£i th√≠ch:** 
- `r"..."`: Raw string (kh√¥ng escape `\`)
- ƒê∆∞·ªùng d·∫´n g·ªëc c·ªßa project

---

### D√≤ng 26-30: Kh·ªüi t·∫°o Spark
```python
spark = SparkSession.builder \
    .appName("Week6_Backfill_BatchLayer") \
    .config("spark.sql.adaptive.enabled", "true") \
    .getOrCreate()
```
**Gi·∫£i th√≠ch:**
- `SparkSession.builder`: Builder pattern
- `.appName(...)`: ƒê·∫∑t t√™n application (hi·ªÉn th·ªã trong Spark UI)
- `.config("spark.sql.adaptive.enabled", "true")`: B·∫≠t Adaptive Query Execution (AQE)
  - AQE t·ª± ƒë·ªông t·ªëi ∆∞u query plan khi runtime
  - C·∫£i thi·ªán performance cho join, aggregate
- `.getOrCreate()`: L·∫•y session hi·ªán t·∫°i ho·∫∑c t·∫°o m·ªõi n·∫øu ch∆∞a c√≥

---

### D√≤ng 32-36: Header
```python
print("=" * 80)
print("WEEK 6 - BATCH LAYER (Lambda Architecture)")
print("Backfill missing dates from Binance API")
print("=" * 80)
```
**Gi·∫£i th√≠ch:** In header ƒë·∫πp cho console output.

---

## 2. Step 1: Detect Last Date

### D√≤ng 38-41: B·∫Øt ƒë·∫ßu Step 1
```python
print("\n[STEP 1] Detecting last date in existing data...")

daily_filled_path = os.path.join(base_dir, "data_analysis", "daily_filled")
```
**Gi·∫£i th√≠ch:**
- `os.path.join()`: N·ªëi ƒë∆∞·ªùng d·∫´n m·ªôt c√°ch an to√†n (t·ª± ƒë·ªông x·ª≠ l√Ω `/` ho·∫∑c `\`)
- `daily_filled_path`: `D:\BigDataProject\data_analysis\daily_filled`

---

### D√≤ng 43-55: Detect Last Date t·ª´ Daily Filled
```python
try:
    df_existing = spark.read.parquet(daily_filled_path)
    last_date_existing = df_existing.agg(F.max("date")).first()[0]
    data_source = "daily_filled"
except:
    try:
        prophet_path = os.path.join(base_dir, "data_analysis", "prophet_input")
        df_existing = spark.read.parquet(prophet_path)
        last_date_existing = df_existing.agg(F.max("ds")).first()[0]
        data_source = "prophet_input"
    except:
        print("  [WARN] No existing data found!")
        spark.stop()
        exit(1)
```
**Gi·∫£i th√≠ch:**
- **Try 1:** ƒê·ªçc t·ª´ `daily_filled/`
  - `spark.read.parquet()`: ƒê·ªçc Parquet file
  - `.agg(F.max("date"))`: L·∫•y ng√†y l·ªõn nh·∫•t (latest)
  - `.first()[0]`: L·∫•y gi√° tr·ªã ƒë·∫ßu ti√™n (row ƒë·∫ßu, c·ªôt ƒë·∫ßu)
- **Try 2:** N·∫øu kh√¥ng c√≥ daily_filled, th·ª≠ ƒë·ªçc `prophet_input/`
  - T∆∞∆°ng t·ª± nh∆∞ng c·ªôt t√™n `ds` (kh√¥ng ph·∫£i `date`)
- **Except:** N·∫øu c·∫£ 2 kh√¥ng c√≥ ‚Üí C·∫£nh b√°o v√† tho√°t
  - `spark.stop()`: D·ª´ng Spark session
  - `exit(1)`: Tho√°t v·ªõi m√£ l·ªói 1

---

### D√≤ng 57-64: T√≠nh Gap
```python
print(f"  [OK] Last date found in {data_source}: {last_date_existing}")

today = datetime.now().strftime('%Y-%m-%d')
fetch_start_date = (datetime.strptime(str(last_date_existing), '%Y-%m-%d') + timedelta(days=1)).strftime('%Y-%m-%d')
gap_days = (datetime.strptime(today, '%Y-%m-%d') - datetime.strptime(fetch_start_date, '%Y-%m-%d')).days

print(f"  [DATE] Today: {today}")
print(f"  [DATA] Gap: {gap_days} days")
```
**Gi·∫£i th√≠ch:**
- `datetime.now().strftime('%Y-%m-%d')`: Ng√†y h√¥m nay (format YYYY-MM-DD)
- `fetch_start_date`: Ng√†y sau last_date_existing
  - `datetime.strptime()`: Parse string ‚Üí datetime object
  - `+ timedelta(days=1)`: C·ªông 1 ng√†y
  - `.strftime()`: Format l·∫°i th√†nh string
- `gap_days`: S·ªë ng√†y c·∫ßn backfill
  - T√≠nh: today - fetch_start_date

---

### D√≤ng 66-76: Ki·ªÉm tra c·∫ßn Backfill kh√¥ng
```python
if gap_days <= 0:
    print("\n[OK] Data is already up to date! No backfill needed.")
    print("  You can start streaming for real-time data:")
    print("    cd week6_streaming")
    print("    docker-compose up -d")
    print("    python websocket_producer.py")
    print("    python spark_streaming_consumer.py")
    spark.stop()
    exit(0)

print(f"\n  [TARGET] Will backfill: {fetch_start_date} -> {today} ({gap_days} days)")
```
**Gi·∫£i th√≠ch:**
- N·∫øu `gap_days <= 0`: D·ªØ li·ªáu ƒë√£ up-to-date
  - In h∆∞·ªõng d·∫´n ch·∫°y Speed Layer
  - Tho√°t v·ªõi m√£ 0 (success)
- N·∫øu c√≥ gap: In th√¥ng tin s·∫Ω backfill

---

## 3. Step 2: Fetch t·ª´ Binance API

### D√≤ng 78-81: B·∫Øt ƒë·∫ßu Step 2
```python
print("\n[STEP 2] Fetching data from Binance API...")

def fetch_binance_klines(symbol, start_date, end_date, interval='1m'):
    """Fetch klines from Binance API with pagination"""
```
**Gi·∫£i th√≠ch:** ƒê·ªãnh nghƒ©a h√†m fetch d·ªØ li·ªáu t·ª´ Binance API.

---

### D√≤ng 82-84: Chuy·ªÉn ƒë·ªïi Timestamp
```python
    start_ts = int(datetime.strptime(start_date, '%Y-%m-%d').timestamp() * 1000)
    end_ts = int(datetime.strptime(end_date, '%Y-%m-%d').timestamp() * 1000) + 86399999
```
**Gi·∫£i th√≠ch:**
- Binance API d√πng Unix timestamp (milliseconds)
- `datetime.strptime()`: Parse string ‚Üí datetime
- `.timestamp()`: Chuy·ªÉn ‚Üí Unix timestamp (seconds)
- `* 1000`: Chuy·ªÉn seconds ‚Üí milliseconds
- `+ 86399999`: Th√™m 23:59:59.999 (cu·ªëi ng√†y)
  - 86399 gi√¢y = 23h 59m 59s
  - 999 ms = 0.999s

---

### D√≤ng 86-88: Kh·ªüi t·∫°o Variables
```python
    all_data = []
    current_start = start_ts
```
**Gi·∫£i th√≠ch:**
- `all_data`: List ch·ª©a t·∫•t c·∫£ klines
- `current_start`: Timestamp b·∫Øt ƒë·∫ßu (ƒë·ªÉ pagination)

---

### D√≤ng 90-100: Loop Pagination
```python
    while current_start < end_ts:
        url = f"https://api.binance.com/api/v3/klines"
        params = {
            'symbol': symbol,
            'interval': interval,
            'startTime': current_start,
            'endTime': end_ts,
            'limit': 1000
        }
        
        max_retries = 3
```
**Gi·∫£i th√≠ch:**
- `while current_start < end_ts`: L·∫∑p cho ƒë·∫øn khi l·∫•y h·∫øt data
- **T·∫°i sao c·∫ßn loop?** Binance API limit 1000 rows/request ‚Üí C·∫ßn nhi·ªÅu request
- `url`: Endpoint Binance Klines API
- `params`: Query parameters:
  - `symbol`: BTCUSDT ho·∫∑c ETHUSDT
  - `interval`: '1m' (1 ph√∫t)
  - `startTime`: Timestamp b·∫Øt ƒë·∫ßu
  - `endTime`: Timestamp k·∫øt th√∫c
  - `limit`: 1000 rows/request (max)
- `max_retries = 3`: Th·ª≠ l·∫°i t·ªëi ƒëa 3 l·∫ßn n·∫øu l·ªói

---

### D√≤ng 101-120: Retry Logic
```python
        for retry in range(max_retries):
            try:
                response = requests.get(url, params=params, timeout=30)
                response.raise_for_status()
                data = response.json()
                
                if not data:
                    break
                
                all_data.extend(data)
                current_start = int(data[-1][0]) + 60000
                time.sleep(0.1)
                break
            except Exception as e:
                if retry < max_retries - 1:
                    print(f"    Retry after error: {e}")
                    time.sleep(2)
                else:
                    print(f"    [WARN] Failed after {max_retries} retries")
                    break
```
**Gi·∫£i th√≠ch:**
- `for retry in range(max_retries)`: Loop 3 l·∫ßn
- **Try block:**
  - `requests.get()`: Call HTTP GET
    - `timeout=30`: Timeout 30 gi√¢y
  - `raise_for_status()`: Raise exception n·∫øu HTTP error (4xx, 5xx)
  - `.json()`: Parse JSON response
  - `if not data: break`: N·∫øu kh√¥ng c√≥ data ‚Üí Tho√°t loop
  - `all_data.extend(data)`: Th√™m data v√†o list
  - `current_start = int(data[-1][0]) + 60000`: Update start time
    - `data[-1][0]`: Timestamp c·ªßa row cu·ªëi
    - `+ 60000`: Th√™m 1 ph√∫t (60,000 ms)
  - `time.sleep(0.1)`: ƒê·ª£i 0.1s tr√°nh rate limit
  - `break`: Tho√°t retry loop (th√†nh c√¥ng)
- **Except block:**
  - N·∫øu retry < 2: Print error, ƒë·ª£i 2s, th·ª≠ l·∫°i
  - N·∫øu retry = 2 (l·∫ßn cu·ªëi): Print warning, tho√°t

---

### D√≤ng 122: Return
```python
    return all_data
```
**Gi·∫£i th√≠ch:** Tr·∫£ v·ªÅ list ch·ª©a t·∫•t c·∫£ klines.

---

### D√≤ng 124-142: Fetch cho BTC v√† ETH
```python
all_klines = []
for symbol in ["BTCUSDT", "ETHUSDT"]:
    print(f"\n  Fetching {symbol}...")
    klines = fetch_binance_klines(symbol, fetch_start_date, today)
    
    if klines:
        for k in klines:
            all_klines.append({
                'timestamp': int(k[0]) // 1000,
                'open': float(k[1]),
                'high': float(k[2]),
                'low': float(k[3]),
                'close': float(k[4]),
                'volume': float(k[5]),
                'symbol': symbol
            })
        print(f"    [OK] Fetched {len(klines):,} rows")
    else:
        print(f"    [WARN] No data fetched")
```
**Gi·∫£i th√≠ch:**
- Loop qua 2 symbols: BTCUSDT, ETHUSDT
- Call `fetch_binance_klines()` cho m·ªói symbol
- **Parse klines:**
  - Binance API tr·∫£ v·ªÅ array: `[timestamp, open, high, low, close, volume, ...]`
  - `k[0]`: Timestamp (ms) ‚Üí Chia 1000 ‚Üí seconds
  - `k[1]`: Open price
  - `k[2]`: High price
  - `k[3]`: Low price
  - `k[4]`: Close price
  - `k[5]`: Volume
- Chuy·ªÉn th√†nh dictionary v√† append v√†o `all_klines`

---

### D√≤ng 144-149: Ki·ªÉm tra Data
```python
if not all_klines:
    print("\n[ERROR] No data fetched. Exiting.")
    spark.stop()
    exit(1)

print(f"\n  Total rows fetched: {len(all_klines):,}")
```
**Gi·∫£i th√≠ch:**
- N·∫øu kh√¥ng fetch ƒë∆∞·ª£c data n√†o ‚Üí Tho√°t
- In t·ªïng s·ªë rows fetched

---

## 4. Step 3: Clean Data

### D√≤ng 151-156: T·∫°o DataFrame
```python
print("\n[STEP 3] Cleaning data...")

df_raw = spark.createDataFrame(all_klines)
df_raw = df_raw.withColumn("datetime", from_unixtime(col("timestamp")))
df_raw = df_raw.withColumn("date", to_date("datetime"))
```
**Gi·∫£i th√≠ch:**
- `spark.createDataFrame()`: Chuy·ªÉn Python list ‚Üí Spark DataFrame
- `.withColumn("datetime", ...)`: Th√™m c·ªôt m·ªõi
  - `from_unixtime(col("timestamp"))`: Unix timestamp ‚Üí datetime string
- `.withColumn("date", ...)`: Extract date t·ª´ datetime
  - `to_date("datetime")`: '2025-12-16 10:30:00' ‚Üí '2025-12-16'

---

### D√≤ng 158-161: Remove Duplicates
```python
df_clean = df_raw.dropDuplicates(["symbol", "timestamp"])
clean_rows = df_clean.count()
print(f"  [OK] After deduplication: {clean_rows:,} rows")
```
**Gi·∫£i th√≠ch:**
- `dropDuplicates(["symbol", "timestamp"])`: X√≥a duplicate d·ª±a tr√™n (symbol, timestamp)
  - V√≠ d·ª•: N·∫øu c√≥ 2 rows v·ªõi (BTCUSDT, 1734134400) ‚Üí Gi·ªØ 1, x√≥a 1
- `.count()`: ƒê·∫øm s·ªë rows
- `{clean_rows:,}`: Format s·ªë v·ªõi d·∫•u ph·∫©y (1,000)

---

## 5. Step 4: Aggregate Daily OHLC

### D√≤ng 163-165: B·∫Øt ƒë·∫ßu Step 4
```python
print("\n[STEP 4] Aggregating to daily OHLC...")

minmax = df_clean.groupBy("symbol", "date").agg(
```
**Gi·∫£i th√≠ch:** T√≠nh min/max timestamp cho m·ªói ng√†y (ƒë·ªÉ l·∫•y open/close).

---

### D√≤ng 166-169: Min/Max Timestamp
```python
    F.min("timestamp").alias("min_ts"),
    F.max("timestamp").alias("max_ts")
)
```
**Gi·∫£i th√≠ch:**
- `groupBy("symbol", "date")`: Nh√≥m theo symbol v√† ng√†y
- `F.min("timestamp")`: Timestamp nh·ªè nh·∫•t (ƒë·∫ßu ng√†y)
- `F.max("timestamp")`: Timestamp l·ªõn nh·∫•t (cu·ªëi ng√†y)
- `.alias()`: ƒê·∫∑t t√™n c·ªôt m·ªõi

---

### D√≤ng 171-174: Daily Open
```python
opens = df_clean.join(minmax, on=["symbol", "date"]) \
    .filter(col("timestamp") == col("min_ts")) \
    .select("symbol", "date", col("open").alias("daily_open"))
```
**Gi·∫£i th√≠ch:**
- **Join:** df_clean v·ªõi minmax
- **Filter:** Ch·ªâ l·∫•y rows c√≥ timestamp = min_ts (ƒë·∫ßu ng√†y)
- **Select:** L·∫•y gi√° `open` ‚Üí ƒê·∫∑t t√™n `daily_open`
- **K·∫øt qu·∫£:** DataFrame v·ªõi (symbol, date, daily_open)

---

### D√≤ng 176-179: Daily Close
```python
closes = df_clean.join(minmax, on=["symbol", "date"]) \
    .filter(col("timestamp") == col("max_ts")) \
    .select("symbol", "date", col("close").alias("daily_close"))
```
**Gi·∫£i th√≠ch:** T∆∞∆°ng t·ª± `opens` nh∆∞ng l·∫•y `close` ·ªü timestamp cu·ªëi ng√†y.

---

### D√≤ng 181-186: Daily High/Low/Volume
```python
basic = df_clean.groupBy("symbol", "date").agg(
    F.max("high").alias("daily_high"),
    F.min("low").alias("daily_low"),
    F.sum("volume").alias("daily_volume")
)
```
**Gi·∫£i th√≠ch:**
- `F.max("high")`: Gi√° cao nh·∫•t trong ng√†y
- `F.min("low")`: Gi√° th·∫•p nh·∫•t trong ng√†y
- `F.sum("volume")`: T·ªïng volume trong ng√†y

---

### D√≤ng 188-191: Join All
```python
df_daily = basic.join(opens, ["symbol", "date"], "left") \
                .join(closes, ["symbol", "date"], "left") \
                .orderBy("symbol", "date")
```
**Gi·∫£i th√≠ch:**
- Join 3 DataFrames: `basic`, `opens`, `closes`
- Left join: Gi·ªØ t·∫•t c·∫£ rows t·ª´ `basic`
- `orderBy()`: S·∫Øp x·∫øp theo symbol, date

---

### D√≤ng 193-199: Print Statistics
```python
daily_count = df_daily.count()
print(f"  [OK] Daily aggregation: {daily_count} rows")
df_daily.groupBy("symbol").agg(
    count("*").alias("days"),
    min("date").alias("first_date"),
    max("date").alias("last_date")
).show(truncate=False)
```
**Gi·∫£i th√≠ch:**
- In s·ªë rows
- Group by symbol v√† show statistics
- `.show(truncate=False)`: Hi·ªÉn th·ªã full text (kh√¥ng c·∫Øt)

---

## 6. Step 5: Forward Fill Missing Dates

### D√≤ng 201-209: T·∫°o Date Range
```python
print("\n[STEP 5] Forward filling missing dates...")

date_range_df = spark.sql(f"""
    SELECT explode(sequence(
        to_date('{fetch_start_date}'),
        to_date('{today}'),
        interval 1 day
    )) as date
""")
```
**Gi·∫£i th√≠ch:**
- **Spark SQL:** T·∫°o DataFrame v·ªõi t·∫•t c·∫£ ng√†y t·ª´ start ‚Üí end
- `sequence(start, end, interval 1 day)`: T·∫°o array ng√†y
  - V√≠ d·ª•: ['2025-12-15', '2025-12-16']
- `explode()`: Chuy·ªÉn array ‚Üí rows
  - ['2025-12-15', '2025-12-16'] ‚Üí 2 rows

---

### D√≤ng 211-213: Loop qua Symbols
```python
df_filled_list = []

for symbol in ["BTCUSDT", "ETHUSDT"]:
```
**Gi·∫£i th√≠ch:** X·ª≠ l√Ω t·ª´ng symbol ri√™ng.

---

### D√≤ng 214-217: Create Complete Date Range
```python
    df_symbol = df_daily.filter(col("symbol") == symbol)
    df_complete = date_range_df.crossJoin(df_symbol.select("symbol").distinct())
    df_with_gaps = df_complete.join(df_symbol, ["symbol", "date"], "left")
```
**Gi·∫£i th√≠ch:**
- `df_symbol`: Ch·ªâ l·∫•y 1 symbol
- `df_complete`: CrossJoin date_range v·ªõi symbol
  - V√≠ d·ª•: 2 ng√†y √ó 1 symbol = 2 rows
- `df_with_gaps`: Left join v·ªõi data th·∫≠t
  - Rows kh√¥ng c√≥ data ‚Üí NULL

---

### D√≤ng 219: Window Specification
```python
    window_spec = Window.partitionBy("symbol").orderBy("date").rowsBetween(Window.unboundedPreceding, 0)
```
**Gi·∫£i th√≠ch:**
- **Window Function:** X·ª≠ l√Ω theo c·ª≠a s·ªï d·ªØ li·ªáu
- `partitionBy("symbol")`: Ph√¢n v√πng theo symbol (m·ªói symbol t√≠nh ri√™ng)
- `orderBy("date")`: S·∫Øp x·∫øp theo ng√†y
- `rowsBetween(unboundedPreceding, 0)`: C·ª≠a s·ªï t·ª´ ƒë·∫ßu ƒë·∫øn row hi·ªán t·∫°i
  - D√πng cho forward fill: L·∫•y gi√° tr·ªã g·∫ßn nh·∫•t tr∆∞·ªõc ƒë√≥

---

### D√≤ng 221-227: Forward Fill
```python
    for col_name in ["daily_open", "daily_high", "daily_low", "daily_close", "daily_volume"]:
        df_with_gaps = df_with_gaps.withColumn(
            col_name,
            F.last(col(col_name), ignorenulls=True).over(window_spec)
        )
    
    df_filled_list.append(df_with_gaps)
```
**Gi·∫£i th√≠ch:**
- Loop qua 5 c·ªôt OHLCV
- `F.last(..., ignorenulls=True).over(window_spec)`:
  - L·∫•y gi√° tr·ªã **cu·ªëi c√πng non-null** trong c·ª≠a s·ªï
  - V√≠ d·ª•: [100, NULL, NULL] ‚Üí [100, 100, 100]
  - ƒê√¢y l√† **forward fill**: ƒêi·ªÅn gi√° tr·ªã tr∆∞·ªõc ƒë√≥ v√†o NULL
- Append v√†o list

---

### D√≤ng 229-234: Union All Symbols
```python
df_filled = df_filled_list[0]
if len(df_filled_list) > 1:
    df_filled = df_filled.union(df_filled_list[1])

df_filled = df_filled.filter(col("daily_close").isNotNull())
filled_count = df_filled.count()
print(f"  [OK] After forward fill: {filled_count} rows")
```
**Gi·∫£i th√≠ch:**
- Union 2 DataFrames (BTCUSDT + ETHUSDT)
- Filter b·ªè rows kh√¥ng c√≥ data (daily_close NULL)
  - V√¨ c√≥ th·ªÉ c√≥ ng√†y ch∆∞a c√≥ data g·ªëc ‚Üí Kh√¥ng th·ªÉ forward fill
- In s·ªë rows

---

## 7. Step 6: Prepare Backfill Data

### D√≤ng 236-246: Select Columns
```python
print("\n[STEP 6] Preparing daily OHLCV data...")

df_backfill = df_filled.select("symbol", "date", "daily_open", "daily_high", "daily_low", "daily_close", "daily_volume")

print(f"  [OK] Backfill data prepared: {df_backfill.count()} rows")
df_backfill.groupBy("symbol").agg(
    count("*").alias("rows"),
    min("date").alias("first"),
    max("date").alias("last")
).show(truncate=False)
```
**Gi·∫£i th√≠ch:**
- Select 7 c·ªôt c·∫ßn thi·∫øt (symbol, date, OHLCV)
- In statistics

---

## 8. Step 7: Merge & Recalculate MA

### D√≤ng 248-256: Read Existing Data
```python
print("\n[STEP 7] Merging with existing daily_filled...")

try:
    df_old_filled = spark.read.parquet(daily_filled_path)
    old_count = df_old_filled.count()
    print(f"  [FOUND] Found existing daily_filled: {old_count:,} rows")
    
    # Select only OHLCV columns (drop old MA7/MA30)
    df_old_filled = df_old_filled.select("symbol", "date", "daily_open", "daily_high", "daily_low", "daily_close", "daily_volume")
```
**Gi·∫£i th√≠ch:**
- Try ƒë·ªçc daily_filled c≈©
- **Quan tr·ªçng:** Ch·ªâ select OHLCV, **b·ªè MA7/MA30 c≈©**
  - V√¨ s·∫Ω t√≠nh l·∫°i MA sau khi merge

---

### D√≤ng 258-264: Union & Deduplicate
```python
    # Union and remove duplicates
    df_merged = df_old_filled.union(df_backfill).dropDuplicates(["symbol", "date"]).orderBy("symbol", "date")
    merged_count = df_merged.count()
    print(f"  [MERGE] After merge: {merged_count:,} rows (added {merged_count - old_count} new)")
except Exception as e:
    print(f"  [INFO] No existing daily_filled, creating new ({type(e).__name__})")
    df_merged = df_backfill
```
**Gi·∫£i th√≠ch:**
- **Union:** K·∫øt h·ª£p old + new data
- **dropDuplicates:** X√≥a duplicate theo (symbol, date)
  - N·∫øu tr√πng ‚Üí Gi·ªØ row ƒë·∫ßu ti√™n (old data)
- **Except:** N·∫øu kh√¥ng c√≥ old data ‚Üí D√πng backfill data

---

### D√≤ng 266-272: Recalculate MA7/MA30
```python
print("\n  [CALC] Recalculating MA7/MA30 for entire dataset...")

window_ma7 = Window.partitionBy("symbol").orderBy("date").rowsBetween(-6, 0)
window_ma30 = Window.partitionBy("symbol").orderBy("date").rowsBetween(-29, 0)

df_merged = df_merged \
    .withColumn("ma7", avg("daily_close").over(window_ma7)) \
```
**Gi·∫£i th√≠ch:**
- **Window MA7:** 7 ng√†y g·∫ßn nh·∫•t
  - `rowsBetween(-6, 0)`: 6 rows tr∆∞·ªõc + row hi·ªán t·∫°i = 7 rows
- **Window MA30:** 30 ng√†y g·∫ßn nh·∫•t
  - `rowsBetween(-29, 0)`: 29 rows tr∆∞·ªõc + row hi·ªán t·∫°i = 30 rows
- `avg("daily_close").over(window_ma7)`: T√≠nh average trong c·ª≠a s·ªï

---

### D√≤ng 273-275: Add Year/Month
```python
    .withColumn("ma30", avg("daily_close").over(window_ma30)) \
    .withColumn("year", year("date")) \
    .withColumn("month", month("date"))
```
**Gi·∫£i th√≠ch:**
- Th√™m c·ªôt `year`, `month` ƒë·ªÉ partition khi save Parquet

---

### D√≤ng 277-281: Save Daily Filled
```python
df_merged.write.mode("overwrite").partitionBy("symbol", "year", "month").parquet(daily_filled_path)

print(f"  [OK] Saved daily_filled: {df_merged.count():,} rows")
print(f"  [SAVE] Path: {daily_filled_path}")
```
**Gi·∫£i th√≠ch:**
- `.write.mode("overwrite")`: Ghi ƒë√® file c≈©
- `.partitionBy("symbol", "year", "month")`: T·∫°o folders:
  - `symbol=BTCUSDT/year=2025/month=12/part-xxx.parquet`
- `.parquet()`: Save d·∫°ng Parquet format

---

## 9. Step 8: Extract Prophet Input

### D√≤ng 283-294: Extract Prophet Input
```python
print("\n[STEP 8] Extracting prophet_input from daily_filled...")

prophet_path = os.path.join(base_dir, "data_analysis", "prophet_input")

# Extract minimal schema: (ds, y, symbol) - NO MA columns
df_prophet = df_merged.select(
    col("date").alias("ds"),
    col("daily_close").alias("y"),
    "symbol"
)

# Save (overwrite - always sync with daily_filled)
df_prophet.write.mode("overwrite").partitionBy("symbol").parquet(prophet_path)
```
**Gi·∫£i th√≠ch:**
- **Prophet c·∫ßn 3 c·ªôt:**
  - `ds`: Date (datetime)
  - `y`: Target variable (gi√° close)
  - `symbol`: Partition key
- **Kh√¥ng include MA7/MA30** ·ªü ƒë√¢y
  - S·∫Ω join t·ª´ daily_filled khi train Prophet
- `partitionBy("symbol")`: 2 partitions (BTCUSDT, ETHUSDT)

---

### D√≤ng 296-298: Print Info
```python
print(f"  [OK] Prophet input extracted: {df_prophet.count():,} rows")
print(f"  [INFO] Schema: (ds, y, symbol) - MA7/MA30 will be joined from daily_filled")
print(f"  [SAVE] Path: {prophet_path}")
```
**Gi·∫£i th√≠ch:** In th√¥ng tin output.

---

## 10. Summary & Next Steps

### D√≤ng 300-308: Final Statistics
```python
print("\n" + "=" * 80)
print("[OK] BACKFILL COMPLETE (BATCH LAYER)")
print("=" * 80)

print("\n[DATA] Final Statistics:")
df_merged.groupBy("symbol").agg(
    count("*").alias("total_days"),
    min("date").alias("first_date"),
    max("date").alias("last_date")
).show(truncate=False)
```
**Gi·∫£i th√≠ch:** In statistics t·ªïng k·∫øt.

---

### D√≤ng 310-314: Output Paths
```python
print("\n[DATA] Output:")
print(f"  [OK] data_analysis/daily_filled/   (Backfilled data with MA7/MA30)")
print(f"  [OK] data_analysis/prophet_input/  (Ready for Prophet)")

print("\n[TARGET] Next Steps:")
```
**Gi·∫£i th√≠ch:** Li·ªát k√™ output files.

---

### D√≤ng 315-324: Next Steps
```python
print("  1. Start Streaming (Speed Layer):")
print("     cd week6_streaming")
print("     docker-compose up -d")
print("     python websocket_producer.py  # Terminal 1")
print("     python spark_streaming_consumer.py  # Terminal 2")
print("")
print("  2. Let streaming run to collect real-time data")
print("")
print("  3. Merge batch + streaming data:")
print("     python week6_merge.py")
```
**Gi·∫£i th√≠ch:** H∆∞·ªõng d·∫´n b∆∞·ªõc ti·∫øp theo.

---

### D√≤ng 326-327: Cleanup
```python
spark.stop()
print("\n[DONE] Week 6 Backfill process finished!")
```
**Gi·∫£i th√≠ch:**
- `spark.stop()`: D·ª´ng Spark session (gi·∫£i ph√≥ng t√†i nguy√™n)
- In message ho√†n th√†nh

---

---

# T√≥m t·∫Øt T·ªïng quan

## üéØ M·ª•c ƒë√≠ch File
File `week6_backfill.py` th·ª±c hi·ªán **Batch Layer backfill** trong Lambda Architecture - ƒêi·ªÅn gaps (kho·∫£ng tr·ªëng) trong d·ªØ li·ªáu b·∫±ng c√°ch l·∫•y historical data t·ª´ Binance API.

---

## üìä Workflow (8 Steps)

### **1. Detect Last Date**
- ƒê·ªçc `daily_filled/` ho·∫∑c `prophet_input/`
- T√¨m ng√†y cu·ªëi c√πng c√≥ data
- T√≠nh gap: H√¥m nay - Last date

### **2. Fetch t·ª´ Binance API**
- Call Binance Klines API cho BTC v√† ETH
- L·∫•y 1-minute candles t·ª´ last_date+1 ƒë·∫øn h√¥m nay
- Pagination: 1000 rows/request
- Retry logic: 3 l·∫ßn n·∫øu timeout

### **3. Clean Data**
- Chuy·ªÉn Unix timestamp ‚Üí Date
- Remove duplicates (symbol, timestamp)

### **4. Aggregate Daily OHLC**
- Group by (symbol, date)
- Daily Open: Gi√° ƒë·∫ßu ng√†y (min timestamp)
- Daily High: Max high
- Daily Low: Min low
- Daily Close: Gi√° cu·ªëi ng√†y (max timestamp)
- Daily Volume: Sum volume

### **5. Forward Fill Missing Dates**
- T·∫°o complete date range (t·∫•t c·∫£ ng√†y trong kho·∫£ng)
- Forward fill NULL values (ƒëi·ªÅn gi√° tr·ªã g·∫ßn nh·∫•t tr∆∞·ªõc ƒë√≥)
- X·ª≠ l√Ω gaps do Binance API thi·∫øu data

### **6. Prepare Backfill Data**
- Select columns: symbol, date, OHLCV
- Ready to merge

### **7. Merge & Recalculate MA**
- Union old data + new backfill data
- Deduplicate (gi·ªØ old data n·∫øu tr√πng)
- **Recalculate MA7/MA30** cho to√†n b·ªô dataset
  - MA7: Average 7 ng√†y
  - MA30: Average 30 ng√†y
- Save ‚Üí `data_analysis/daily_filled/`

### **8. Extract Prophet Input**
- Select (date‚Üíds, daily_close‚Üíy, symbol)
- **Kh√¥ng include MA** (s·∫Ω join t·ª´ daily_filled)
- Save ‚Üí `data_analysis/prophet_input/`

---

## üîë ƒêi·ªÉm Quan Tr·ªçng

### **1. T·∫°i sao Recalculate MA?**
- MA ph·ª• thu·ªôc v√†o **to√†n b·ªô dataset**
- Khi th√™m data m·ªõi ‚Üí MA c·ªßa ng√†y c≈© thay ƒë·ªïi
- **Ph·∫£i t√≠nh l·∫°i to√†n b·ªô** ƒë·ªÉ ƒë·∫£m b·∫£o ch√≠nh x√°c

### **2. Forward Fill**
- ƒêi·ªÅn gaps do Binance API thi·∫øu data (ngh·ªâ l·ªÖ, downtime)
- D√πng gi√° tr·ªã ng√†y tr∆∞·ªõc ƒë√≥
- Tr√°nh NULL trong dataset

### **3. Deduplication Priority**
- Old data > New backfill data
- Tr√°nh ghi ƒë√® data ƒë√£ clean/verified

### **4. Partition Strategy**
- `partitionBy("symbol", "year", "month")` 
- T·ªëi ∆∞u query performance
- D·ªÖ d√†ng qu·∫£n l√Ω/delete theo th·ªùi gian

---

## üìÅ Input/Output

| Stage | Input | Output |
|-------|-------|--------|
| **Detect** | `daily_filled/` ho·∫∑c `prophet_input/` | Last date |
| **Fetch** | Binance API | Raw 1-min klines |
| **Clean** | Raw klines | Deduplicated DataFrame |
| **Aggregate** | 1-min data | Daily OHLCV |
| **Forward Fill** | Daily OHLCV | Complete date range |
| **Merge** | Old + New data | `daily_filled/` with MA |
| **Extract** | `daily_filled/` | `prophet_input/` |

---

## üí° Use Cases

### **Khi n√†o ch·∫°y file n√†y?**
1. ‚úÖ L·∫ßn ƒë·∫ßu setup project (no data)
2. ‚úÖ Sau khi pause project v√†i ng√†y (c√≥ gap)
3. ‚úÖ Tr∆∞·ªõc khi train Prophet (c·∫ßn data m·ªõi nh·∫•t)
4. ‚úÖ Sau khi fix data corruption (rebuild)

### **Khi n√†o KH√îNG ch·∫°y?**
- ‚ùå Data ƒë√£ up-to-date (gap ‚â§ 0 days)
- ‚ùå Binance API ƒëang down/rate limit
- ‚ùå ƒêang c√≥ Speed Layer running (ch·ªù merge)

---

## üöÄ Next Steps (Sau khi Backfill)

1. **Start Speed Layer:**
   ```bash
   cd week6_streaming
   docker-compose up -d
   python websocket_producer.py
   python kafka_batch_reader.py
   ```

2. **Merge Batch + Speed:**
   ```bash
   python scripts/lambda_batch/week6_merge.py
   ```

3. **Train Prophet:**
   ```bash
   python scripts/ml_models/prophet_train.py
   ```

---

## üéì Key Technologies

- **PySpark:** DataFrame API, SparkSQL
- **Window Functions:** MA calculation, Forward fill
- **Binance API:** Historical klines endpoint
- **Parquet:** Columnar storage, Partitioning
- **Lambda Architecture:** Batch Layer component

---

**T√°c gi·∫£:** ƒêo√†n Th·∫ø T√≠n  
**MSSV:** 4551190056  
**File:** `scripts/lambda_batch/week6_backfill.py`  
**Lines:** 327 d√≤ng code  
**M·ª•c ƒë√≠ch:** Backfill missing dates cho Batch Layer trong Lambda Architecture

---
