# Giáº£i thÃ­ch chi tiáº¿t: kafka_batch_reader.py

**File:** `week6_streaming/kafka_batch_reader.py`  
**Chá»©c nÄƒng:** Speed Layer Demo - Batch mode reader Ä‘á»ƒ test nhanh Kafka â†’ Aggregation â†’ Parquet  
**TÃ¡c giáº£:** ÄoÃ n Tháº¿ TÃ­n  
**NgÃ y:** Week 6 - Lambda Architecture

---

## ğŸ“‹ Má»¥c lá»¥c
1. [Import vÃ  Configuration](#1-import-vÃ -configuration)
2. [Spark Session Setup](#2-spark-session-setup)
3. [Schema Definition](#3-schema-definition)
4. [Read Batch from Kafka](#4-read-batch-from-kafka)
5. [Parse JSON Data](#5-parse-json-data)
6. [Data Transformation](#6-data-transformation)
7. [Daily Aggregation](#7-daily-aggregation)
8. [Save to Parquet](#8-save-to-parquet)
9. [TÃ³m táº¯t](#tÃ³m-táº¯t-tá»•ng-quan)

---

## 1. Import vÃ  Configuration

### DÃ²ng 1-6: Docstring
```python
"""
kafka_batch_reader.py - Doc du lieu tu Kafka bang batch mode

Muc dich: Chung minh Speed Layer hoat dong (doc tu Kafka -> aggregation -> Parquet)
Su dung: python kafka_batch_reader.py
"""
```
**Giáº£i thÃ­ch:**
- **Batch mode:** Äá»c táº¥t cáº£ messages tá»« Kafka **1 láº§n** (khÃ´ng continuous stream)
- **Má»¥c Ä‘Ã­ch:** Demo nhanh Speed Layer (khÃ´ng cáº§n Ä‘á»£i 24h nhÆ° streaming)
- **Use case:** Presentation, testing, quick results

---

### DÃ²ng 7-10: Import
```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
import os
```
**Giáº£i thÃ­ch:**
- `SparkSession`: Entry point Spark
- `functions`: CÃ¡c hÃ m SQL (col, from_json, to_date, ...)
- `types`: Data types (StringType, LongType, DoubleType, ...)
- `os`: Xá»­ lÃ½ Ä‘Æ°á»ng dáº«n file

---

### DÃ²ng 12-14: Banner
```python
print("="*80)
print("KAFKA BATCH READER - Speed Layer Test")
print("="*80)
```
**Giáº£i thÃ­ch:** In header cho console.

---

## 2. Spark Session Setup

### DÃ²ng 16-19: Spark Session
```python
# Spark Session
spark = SparkSession.builder \
    .appName("KafkaBatchReader") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.3") \
    .getOrCreate()
```
**Giáº£i thÃ­ch:**

#### `.appName("KafkaBatchReader")`
- TÃªn application
- Hiá»ƒn thá»‹ trong Spark UI

#### `.config("spark.jars.packages", "...")`
- Download Kafka connector dependency
- `org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.3`
  - TÆ°Æ¡ng tá»± nhÆ° `spark_streaming_consumer.py`
  - Cáº§n Ä‘á»ƒ Ä‘á»c tá»« Kafka

#### `.getOrCreate()`
- Láº¥y session hiá»‡n cÃ³ hoáº·c táº¡o má»›i

---

### DÃ²ng 21-22: Log Level
```python
spark.sparkContext.setLogLevel("WARN")
print("\nâœ“ Spark initialized\n")
```
**Giáº£i thÃ­ch:**
- `WARN`: Chá»‰ log WARNING vÃ  ERROR
- áº¨n INFO logs (giáº£m noise)

---

## 3. Schema Definition

### DÃ²ng 24-38: Message Schema
```python
# Schema
message_schema = StructType([
    StructField("symbol", StringType(), True),
    StructField("event_time", LongType(), True),
    StructField("open", DoubleType(), True),
    StructField("high", DoubleType(), True),
    StructField("low", DoubleType(), True),
    StructField("price", DoubleType(), True),
    StructField("volume", DoubleType(), True),
    StructField("quote_volume", DoubleType(), True),
    StructField("number_trades", IntegerType(), True),
    StructField("price_change", DoubleType(), True),
    StructField("price_change_percent", DoubleType(), True),
    StructField("timestamp", StringType(), True)
])
```
**Giáº£i thÃ­ch:**

### Schema giá»‘ng vá»›i `spark_streaming_consumer.py`
- **Táº¡i sao?** CÃ¹ng Ä‘á»c tá»« Kafka topic `crypto-prices`
- **Producer** (`websocket_producer.py`) gá»­i JSON vá»›i schema nÃ y
- **Consumer** (cáº£ streaming vÃ  batch) dÃ¹ng cÃ¹ng schema Ä‘á»ƒ parse

### So sÃ¡nh vá»›i Streaming Consumer
| Aspect | Batch Reader (File nÃ y) | Streaming Consumer |
|--------|------------------------|-------------------|
| Schema | Giá»‘ng há»‡t | Giá»‘ng há»‡t |
| Kafka Topic | crypto-prices | crypto-prices |
| Read Mode | **Batch (1 láº§n)** | **Streaming (continuous)** |

---

## 4. Read Batch from Kafka

### DÃ²ng 40-48: Batch Read
```python
# Doc BATCH tu Kafka (khong phai streaming)
print("Reading ALL messages from Kafka (batch mode)...")
df = spark.read \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "localhost:9092") \
    .option("subscribe", "crypto-prices") \
    .option("startingOffsets", "earliest") \
    .option("endingOffsets", "latest") \
    .load()
```
**Giáº£i thÃ­ch:**

### `spark.read` vs `spark.readStream`
| API | Mode | Behavior |
|-----|------|----------|
| `spark.read` | **Batch** | Äá»c táº¥t cáº£ data 1 láº§n, return DataFrame |
| `spark.readStream` | **Streaming** | Äá»c continuous, return StreamingDataFrame |

### Äiá»ƒm khÃ¡c biá»‡t chÃ­nh
```python
# Batch (File nÃ y):
df = spark.read.format("kafka")...

# Streaming (spark_streaming_consumer.py):
df = spark.readStream.format("kafka")...
```

---

### DÃ²ng 43-47: Kafka Options

#### `.format("kafka")`
- DÃ¹ng Kafka source connector

#### `.option("kafka.bootstrap.servers", "localhost:9092")`
- Kafka broker address

#### `.option("subscribe", "crypto-prices")`
- Subscribe topic `crypto-prices`

#### `.option("startingOffsets", "earliest")`
- **Báº¯t Ä‘áº§u tá»« offset Ä‘áº§u tiÃªn** trong topic
- Äá»c táº¥t cáº£ messages tá»« Ä‘áº§u Ä‘áº¿n cuá»‘i
- **Alternatives:**
  - `"latest"`: Chá»‰ Ä‘á»c messages má»›i (khÃ´ng Ä‘á»c gÃ¬ náº¿u topic Ä‘Ã£ cÃ³ data)
  - `{"crypto-prices": {"0": 100}}`: Tá»« offset 100

#### `.option("endingOffsets", "latest")`
- **Káº¿t thÃºc á»Ÿ offset cuá»‘i cÃ¹ng** hiá»‡n táº¡i
- **Táº¡i sao cáº§n?**
  - Batch mode cáº§n biáº¿t Ä‘iá»ƒm dá»«ng
  - Streaming khÃ´ng cÃ³ `endingOffsets` (continuous)

---

### Kafka Offset Flow
```
Topic: crypto-prices
â”œâ”€â”€ Offset 0: {"symbol": "BTCUSDT", "price": 42000.0, ...}
â”œâ”€â”€ Offset 1: {"symbol": "ETHUSDT", "price": 3200.0, ...}
â”œâ”€â”€ Offset 2: {"symbol": "BTCUSDT", "price": 42010.0, ...}
...
â””â”€â”€ Offset 1007: {"symbol": "ETHUSDT", "price": 3210.0, ...}

Batch Read:
  startingOffsets = "earliest" â†’ Báº¯t Ä‘áº§u tá»« 0
  endingOffsets = "latest" â†’ Káº¿t thÃºc á»Ÿ 1007
  â†’ Äá»c 1,008 messages
```

---

### DÃ²ng 50-51: Info
```python
print(f"âœ“ Read from Kafka topic: crypto-prices")
print(f"âœ“ Total messages: {df.count()}\n")
```
**Giáº£i thÃ­ch:**
- `df.count()`: Äáº¿m sá»‘ messages Ä‘á»c Ä‘Æ°á»£c
- **Action:** Trigger Spark job (Ä‘á»c Kafka)

**VÃ­ dá»¥ output:**
```
âœ“ Read from Kafka topic: crypto-prices
âœ“ Total messages: 1,008
```

---

## 5. Parse JSON Data

### DÃ²ng 53-57: JSON Parsing
```python
# Parse JSON
parsed_df = df.select(
    from_json(col("value").cast("string"), message_schema).alias("data"),
    col("timestamp").alias("kafka_timestamp")
).select("data.*", "kafka_timestamp")
```
**Giáº£i thÃ­ch:**

### Logic giá»‘ng `spark_streaming_consumer.py`
- **Step 1:** Láº¥y column `value` (binary)
- **Step 2:** Cast sang string
- **Step 3:** Parse JSON vá»›i schema
- **Step 4:** Unpack struct â†’ flat columns

### Transformation
```
Input (Kafka DataFrame):
â”œâ”€â”€ key: null
â”œâ”€â”€ value: b'{"symbol":"BTCUSDT","price":42000.0,...}'
â”œâ”€â”€ topic: "crypto-prices"
â”œâ”€â”€ partition: 0
â”œâ”€â”€ offset: 0
â””â”€â”€ timestamp: 2025-12-16 10:30:00

After parsing:
â”œâ”€â”€ symbol: "BTCUSDT"
â”œâ”€â”€ event_time: 1734134400000
â”œâ”€â”€ price: 42000.0
â”œâ”€â”€ open: 43000.0
â”œâ”€â”€ high: 43500.0
â”œâ”€â”€ low: 41800.0
â”œâ”€â”€ volume: 12345.67
â”œâ”€â”€ quote_volume: 520000000.0
â”œâ”€â”€ number_trades: 123456
â”œâ”€â”€ price_change: -1000.0
â”œâ”€â”€ price_change_percent: -2.38
â”œâ”€â”€ timestamp: "2025-12-16T10:30:45.123456"
â””â”€â”€ kafka_timestamp: 2025-12-16 10:30:00
```

---

## 6. Data Transformation

### DÃ²ng 59-63: Timestamp Conversion
```python
# Transform
transformed_df = parsed_df \
    .withColumn("event_timestamp", (col("event_time") / 1000).cast("timestamp")) \
    .withColumn("date", to_date(col("event_timestamp"))) \
    .withColumn("hour", hour(col("event_timestamp")))
```
**Giáº£i thÃ­ch:**

### Logic giá»‘ng `spark_streaming_consumer.py`

#### `.withColumn("event_timestamp", ...)`
- Convert milliseconds â†’ timestamp
- `1734134400000 / 1000` â†’ `1734134400` (seconds)
- `.cast("timestamp")` â†’ `2025-12-16 10:30:00`

#### `.withColumn("date", ...)`
- Extract date tá»« timestamp
- `2025-12-16 10:30:00` â†’ `2025-12-16`

#### `.withColumn("hour", ...)`
- Extract hour
- `2025-12-16 10:30:00` â†’ `10`

---

### DÃ²ng 65-66: Sample Data
```python
print("Sample data:")
transformed_df.select("symbol", "price", "volume", "event_timestamp").show(10, False)
```
**Giáº£i thÃ­ch:**
- Show 10 rows máº«u
- `.show(10, False)`: 10 rows, khÃ´ng truncate

**VÃ­ dá»¥ output:**
```
Sample data:
+--------+--------+----------+-------------------+
|symbol  |price   |volume    |event_timestamp    |
+--------+--------+----------+-------------------+
|BTCUSDT |42000.50|12345.67  |2025-12-16 10:30:00|
|ETHUSDT |3200.00 |45678.90  |2025-12-16 10:30:01|
|BTCUSDT |42010.00|12346.78  |2025-12-16 10:30:02|
|ETHUSDT |3201.50 |45680.12  |2025-12-16 10:30:03|
...
```

---

## 7. Daily Aggregation

### DÃ²ng 68-82: Aggregation
```python
# Daily Aggregation (khong can watermark cho batch)
daily_df = transformed_df \
    .groupBy("date", "symbol") \
    .agg(
        first("open").alias("daily_open"),
        max("high").alias("daily_high"),
        min("low").alias("daily_low"),
        last("price").alias("daily_close"),
        sum("volume").alias("daily_volume"),
        sum("quote_volume").alias("daily_quote_volume"),
        sum("number_trades").alias("total_trades"),
        count("*").alias("tick_count"),
        avg("price").alias("avg_price")
    ) \
    .orderBy("date", "symbol")
```
**Giáº£i thÃ­ch:**

### KhÃ¡c biá»‡t vá»›i Streaming Consumer

| Aspect | Batch Reader (File nÃ y) | Streaming Consumer |
|--------|------------------------|-------------------|
| **Group By** | `groupBy("date", "symbol")` | `groupBy(window(...), "symbol")` |
| **Watermark** | **KHÃ”NG Cáº¦N** | Cáº§n (1 hour) |
| **Window** | **KHÃ”NG Cáº¦N** | Tumbling 1 day |
| **Processing** | 1 láº§n | Continuous micro-batches |

---

### Táº¡i sao khÃ´ng cáº§n Watermark?
- **Batch mode:** Äá»c táº¥t cáº£ data sáºµn cÃ³ (khÃ´ng cÃ³ late data)
- **Streaming mode:** Data Ä‘áº¿n liÃªn tá»¥c â†’ Cáº§n watermark Ä‘á»ƒ drop late data

### Táº¡i sao khÃ´ng dÃ¹ng window()?
- **Batch:** DÃ¹ng `groupBy("date")` Ä‘Æ¡n giáº£n
  - `date` Ä‘Ã£ extract tá»« timestamp
  - Group theo date tháº³ng luÃ´n
- **Streaming:** DÃ¹ng `window(col("event_timestamp"), "1 day")`
  - Cáº§n window object Ä‘á»ƒ quáº£n lÃ½ state
  - Window cÃ³ `start` vÃ  `end` timestamps

---

### Aggregation Functions

#### `first("open")`
- GiÃ¡ `open` cá»§a message **Ä‘áº§u tiÃªn** trong ngÃ y
- **Sáº¯p xáº¿p:** Theo thá»© tá»± Spark read (offset order)
- **Assumption:** Producer gá»­i theo thá»© tá»± thá»i gian

#### `max("high")`
- GiÃ¡ cao nháº¥t trong ngÃ y

#### `min("low")`
- GiÃ¡ tháº¥p nháº¥t trong ngÃ y

#### `last("price")`
- GiÃ¡ `price` cá»§a message **cuá»‘i cÃ¹ng** trong ngÃ y
- ÄÃ¢y lÃ  **daily close price**

#### `sum("volume")`
- Tá»•ng volume BTC/ETH trong ngÃ y

#### `sum("quote_volume")`
- Tá»•ng volume USDT trong ngÃ y

#### `sum("number_trades")`
- Tá»•ng sá»‘ trades trong ngÃ y

#### `count("*")`
- Sá»‘ messages (ticks) trong ngÃ y
- **VÃ­ dá»¥:** Producer cháº¡y 1h, gá»­i 2 msg/s â†’ 7,200 ticks

#### `avg("price")`
- GiÃ¡ trung bÃ¬nh trong ngÃ y

---

### DÃ²ng 84-85: Show Results
```python
print("\nDaily aggregation:")
daily_df.show(20, False)
```
**Giáº£i thÃ­ch:**
- Show 20 rows káº¿t quáº£
- `False`: KhÃ´ng truncate

**VÃ­ dá»¥ output:**
```
Daily aggregation:
+----------+--------+-----------+-----------+-----------+------------+-------------+-------------------+------------+----------+-----------+
|date      |symbol  |daily_open |daily_high |daily_low  |daily_close |daily_volume |daily_quote_volume |total_trades|tick_count|avg_price  |
+----------+--------+-----------+-----------+-----------+------------+-------------+-------------------+------------+----------+-----------+
|2025-12-16|BTCUSDT |43000.0    |43500.0    |41800.0    |42000.0     |12345.67     |520000000.0        |123456      |504       |42250.5    |
|2025-12-16|ETHUSDT |3150.0     |3220.0     |3100.0     |3200.0      |45678.90     |145000000.0        |89012       |504       |3160.8     |
+----------+--------+-----------+-----------+-----------+------------+-------------+-------------------+------------+----------+-----------+
```

---

## 8. Save to Parquet

### DÃ²ng 87-89: Output Path
```python
# Luu Parquet
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
OUTPUT_PATH = os.path.join(BASE_DIR, "streaming_output_spark_BATCH")
```
**Giáº£i thÃ­ch:**

#### `os.path.dirname(os.path.abspath(__file__))`
- `__file__`: Path cá»§a file hiá»‡n táº¡i
  - `/week6_streaming/kafka_batch_reader.py`
- `os.path.abspath()`: Convert â†’ absolute path
  - `D:\BigDataProject\week6_streaming\kafka_batch_reader.py`
- `os.path.dirname()`: Láº¥y thÆ° má»¥c cha
  - `D:\BigDataProject\week6_streaming\`

#### Output Path
- `streaming_output_spark_BATCH`
- **KhÃ¡c vá»›i streaming consumer:** `streaming_output_spark` (khÃ´ng cÃ³ `_BATCH`)
- **Táº¡i sao tÃ¡ch riÃªng?**
  - TrÃ¡nh conflict vá»›i streaming output
  - Dá»… phÃ¢n biá»‡t batch vs streaming results

---

### DÃ²ng 91-94: Write Parquet
```python
daily_df.write \
    .mode("overwrite") \
    .partitionBy("symbol") \
    .parquet(OUTPUT_PATH)
```
**Giáº£i thÃ­ch:**

#### `.write`
- Batch write API (khÃ´ng pháº£i `.writeStream`)

#### `.mode("overwrite")`
- **Overwrite mode:** XÃ³a data cÅ©, ghi má»›i
- **Alternatives:**
  - `"append"`: ThÃªm vÃ o cuá»‘i
  - `"ignore"`: Skip náº¿u Ä‘Ã£ tá»“n táº¡i
  - `"error"`: Raise exception náº¿u tá»“n táº¡i

#### `.partitionBy("symbol")`
- Partition theo symbol
- Folder structure:
  ```
  streaming_output_spark_BATCH/
  â”œâ”€â”€ symbol=BTCUSDT/
  â”‚   â””â”€â”€ part-00000-xxx.parquet
  â””â”€â”€ symbol=ETHUSDT/
      â””â”€â”€ part-00000-xxx.parquet
  ```

#### `.parquet(OUTPUT_PATH)`
- Save dáº¡ng Parquet format
- Path: `week6_streaming/streaming_output_spark_BATCH/`

---

### DÃ²ng 96-99: Success Message
```python
print(f"\nâœ“ Saved to: {OUTPUT_PATH}")
print(f"âœ“ Total rows: {daily_df.count()}")
print(f"\n{'='*80}")
print("SUCCESS! Speed Layer data processed from Kafka")
```
**Giáº£i thÃ­ch:**
- Print output path
- Print sá»‘ rows saved
- Success banner

**VÃ­ dá»¥ output:**
```
âœ“ Saved to: D:\BigDataProject\week6_streaming\streaming_output_spark_BATCH
âœ“ Total rows: 2

================================================================================
SUCCESS! Speed Layer data processed from Kafka
================================================================================
```

---

### DÃ²ng 101-103: Cleanup
```python
print(f"{'='*80}\n")

spark.stop()
```
**Giáº£i thÃ­ch:**
- `spark.stop()`: Dá»«ng Spark session
- Giáº£i phÃ³ng resources (memory, threads)

---

---

# TÃ³m táº¯t Tá»•ng quan

## ğŸ¯ Má»¥c Ä‘Ã­ch File
File `kafka_batch_reader.py` lÃ  **Speed Layer Demo Reader** - Äá»c táº¥t cáº£ messages tá»« Kafka báº±ng batch mode (1 láº§n) Ä‘á»ƒ test nhanh vÃ  demo káº¿t quáº£ mÃ  khÃ´ng cáº§n Ä‘á»£i 24h nhÆ° streaming consumer.

---

## ğŸ“Š Workflow (5 Steps)

### **1. Read Batch from Kafka**
- Batch mode: `spark.read` (khÃ´ng pháº£i `spark.readStream`)
- Äá»c tá»« `earliest` â†’ `latest` offsets
- 1 láº§n xong (khÃ´ng continuous)

### **2. Parse JSON Data**
- Convert binary â†’ string â†’ JSON
- Parse vá»›i predefined schema
- Unpack nested struct

### **3. Transform Data**
- Convert Unix timestamp â†’ Datetime
- Extract date vÃ  hour

### **4. Aggregate Daily**
- Group by (date, symbol)
- **KHÃ”NG cáº§n watermark** (batch mode)
- **KHÃ”NG dÃ¹ng window()** (dÃ¹ng date tháº³ng)
- Aggregations: OHLC, Volume, Trades

### **5. Save Parquet**
- Write mode: Overwrite
- Partition by symbol
- Output: `streaming_output_spark_BATCH/`

---

## ğŸ”‘ Äiá»ƒm Quan Trá»ng

### **1. Batch vs Streaming Mode**

| Aspect | Batch Mode (File nÃ y) | Streaming Mode |
|--------|----------------------|----------------|
| **API** | `spark.read` | `spark.readStream` |
| **Read** | 1 láº§n (earliest â†’ latest) | Continuous |
| **Offsets** | `startingOffsets` + `endingOffsets` | `startingOffsets` only |
| **Processing** | 1 job | Micro-batches (10s) |
| **Watermark** | **KHÃ”NG Cáº¦N** | Cáº§n (1h) |
| **Window** | **KHÃ”NG Cáº¦N** | Tumbling 1 day |
| **Output** | `.write` | `.writeStream` |
| **Time to Result** | **1-2 giÃ¢y** | **25 giá»** |
| **Use Case** | Demo, Testing | Production 24/7 |

---

### **2. Khi nÃ o dÃ¹ng Batch Reader?**

âœ… **Demo cho giáº£ng viÃªn** (5-10 phÃºt)
- Cháº¡y Producer 10 phÃºt
- Cháº¡y Batch Reader â†’ Káº¿t quáº£ ngay
- Show Parquet output

âœ… **Testing Speed Layer logic**
- Verify aggregation logic Ä‘Ãºng
- Check schema matching
- Debug nhanh

âœ… **One-time processing**
- Process backlog messages
- Reprocess data vá»›i logic má»›i

---

### **3. Khi nÃ o dÃ¹ng Streaming Consumer?**

âœ… **Production 24/7**
- Real-time continuous processing
- Exactly-once semantics
- Fault tolerance vá»›i checkpoint

âŒ **KHÃ”NG dÃ¹ng cho demo**
- Window 1 ngÃ y cáº§n Ä‘á»£i 24h
- Watermark 1h thÃªm delay
- Tá»•ng: 25 giá» Ä‘á»ƒ cÃ³ káº¿t quáº£

---

### **4. Aggregation Logic**

#### Group By
```python
# Batch (Ä‘Æ¡n giáº£n):
.groupBy("date", "symbol")

# Streaming (phá»©c táº¡p):
.groupBy(window(col("event_timestamp"), "1 day"), col("symbol"))
```

#### Táº¡i sao khÃ¡c?
- **Batch:** Data Ä‘Ã£ cÃ³ sáºµn, group theo `date` column
- **Streaming:** Cáº§n window object Ä‘á»ƒ quáº£n lÃ½ state

#### Output giá»‘ng nhau
- Cáº£ 2 Ä‘á»u cho OHLC daily
- Schema giá»‘ng há»‡t nhau

---

## ğŸ“ Output Structure

### **Folder Structure**
```
week6_streaming/
â”œâ”€â”€ streaming_output_spark/          # Streaming consumer output
â”‚   â””â”€â”€ daily/
â”‚       â”œâ”€â”€ symbol=BTCUSDT/
â”‚       â””â”€â”€ symbol=ETHUSDT/
â””â”€â”€ streaming_output_spark_BATCH/    # Batch reader output (File nÃ y)
    â”œâ”€â”€ symbol=BTCUSDT/
    â”‚   â””â”€â”€ part-00000-xxx.parquet
    â””â”€â”€ symbol=ETHUSDT/
        â””â”€â”€ part-00000-xxx.parquet
```

### **Schema Output**
| Column | Type | Example | Source |
|--------|------|---------|--------|
| `date` | date | 2025-12-16 | Extract tá»« event_timestamp |
| `symbol` | string | BTCUSDT | Producer |
| `daily_open` | double | 43000.0 | first("open") |
| `daily_high` | double | 43500.0 | max("high") |
| `daily_low` | double | 41800.0 | min("low") |
| `daily_close` | double | 42000.0 | last("price") |
| `daily_volume` | double | 12345.67 | sum("volume") |
| `daily_quote_volume` | double | 520000000.0 | sum("quote_volume") |
| `total_trades` | long | 123456 | sum("number_trades") |
| `tick_count` | long | 504 | count(*) |
| `avg_price` | double | 42250.5 | avg("price") |

---

## ğŸ’¡ Use Cases

### **1. Quick Demo (5-10 phÃºt)**
```bash
# Step 1: Start Kafka
cd week6_streaming
docker-compose up -d

# Step 2: Run Producer (10 phÃºt)
python websocket_producer.py
# Ctrl+C sau 10 phÃºt (~1,200 messages)

# Step 3: Run Batch Reader
python kafka_batch_reader.py
# âœ“ Káº¿t quáº£ ngay láº­p tá»©c!

# Step 4: Check output
ls streaming_output_spark_BATCH/
```

### **2. Test Aggregation Logic**
```bash
# Run producer vá»›i test data
python websocket_producer.py
# Ctrl+C sau 1 phÃºt

# Run batch reader vÃ  verify
python kafka_batch_reader.py
# Check console output
```

### **3. Reprocess Messages**
```bash
# Kafka cÃ³ 10,000 messages
# Muá»‘n reprocess vá»›i logic má»›i

# Edit kafka_batch_reader.py (update aggregation)
# Run láº¡i
python kafka_batch_reader.py
# Process all 10,000 messages
```

---

## ğŸš€ CÃ¡ch Sá»­ Dá»¥ng

### **Prerequisites**
```bash
# Kafka pháº£i running
docker ps | grep kafka

# Topic pháº£i cÃ³ data
# (Cháº¡y websocket_producer.py trÆ°á»›c)
```

---

### **Run Command**
```bash
cd week6_streaming
python kafka_batch_reader.py
```

---

### **Expected Output**
```
================================================================================
KAFKA BATCH READER - Speed Layer Test
================================================================================

âœ“ Spark initialized

Reading ALL messages from Kafka (batch mode)...
âœ“ Read from Kafka topic: crypto-prices
âœ“ Total messages: 1,008

Sample data:
+--------+--------+----------+-------------------+
|symbol  |price   |volume    |event_timestamp    |
+--------+--------+----------+-------------------+
|BTCUSDT |42000.50|12345.67  |2025-12-16 10:30:00|
|ETHUSDT |3200.00 |45678.90  |2025-12-16 10:30:01|
|BTCUSDT |42010.00|12346.78  |2025-12-16 10:30:02|
|ETHUSDT |3201.50 |45680.12  |2025-12-16 10:30:03|
...
+--------+--------+----------+-------------------+

Daily aggregation:
+----------+--------+-----------+-----------+-----------+------------+-------------+-------------------+------------+----------+-----------+
|date      |symbol  |daily_open |daily_high |daily_low  |daily_close |daily_volume |daily_quote_volume |total_trades|tick_count|avg_price  |
+----------+--------+-----------+-----------+-----------+------------+-------------+-------------------+------------+----------+-----------+
|2025-12-16|BTCUSDT |43000.0    |43500.0    |41800.0    |42000.0     |12345.67     |520000000.0        |123456      |504       |42250.5    |
|2025-12-16|ETHUSDT |3150.0     |3220.0     |3100.0     |3200.0      |45678.90     |145000000.0        |89012       |504       |3160.8     |
+----------+--------+-----------+-----------+-----------+------------+-------------+-------------------+------------+----------+-----------+

âœ“ Saved to: D:\BigDataProject\week6_streaming\streaming_output_spark_BATCH
âœ“ Total rows: 2

================================================================================
SUCCESS! Speed Layer data processed from Kafka
================================================================================
```

---

### **Verify Output**
```bash
# Check Parquet files
ls streaming_output_spark_BATCH/

# Output:
# symbol=BTCUSDT/
# symbol=ETHUSDT/

# Read Parquet vá»›i PySpark
from pyspark.sql import SparkSession
spark = SparkSession.builder.getOrCreate()
df = spark.read.parquet("streaming_output_spark_BATCH")
df.show()
```

---

## ğŸ”§ Troubleshooting

### **1. No Messages Read**
**Triá»‡u chá»©ng:** `Total messages: 0`  
**NguyÃªn nhÃ¢n:**
- Kafka topic trá»‘ng (chÆ°a cháº¡y producer)
- Topic khÃ´ng tá»“n táº¡i

**Giáº£i phÃ¡p:**
```bash
# Check topic exists
docker exec -it kafka_container kafka-topics --list --bootstrap-server localhost:9092

# Check topic cÃ³ data
docker exec -it kafka_container kafka-console-consumer \
  --bootstrap-server localhost:9092 \
  --topic crypto-prices \
  --from-beginning \
  --max-messages 10

# Run producer trÆ°á»›c
python websocket_producer.py
# Ctrl+C sau vÃ i phÃºt
```

---

### **2. Kafka Connection Failed**
**Error:** `Failed to get records for crypto-prices`  
**Giáº£i phÃ¡p:**
```bash
# Check Kafka running
docker ps | grep kafka

# Start if not running
docker-compose up -d

# Check port 9092
netstat -an | grep 9092
```

---

### **3. JSON Parse Error**
**Error:** `Could not parse JSON`  
**NguyÃªn nhÃ¢n:** Schema khÃ´ng khá»›p vá»›i data  
**Giáº£i phÃ¡p:**
```python
# Check sample message tá»« Kafka
df.select(col("value").cast("string")).show(1, False)

# Verify schema match
# Update message_schema náº¿u cáº§n
```

---

### **4. Empty Output**
**Triá»‡u chá»©ng:** `Total rows: 0` sau aggregation  
**NguyÃªn nhÃ¢n:**
- All rows filtered out (NULL values)
- Aggregation logic sai

**Giáº£i phÃ¡p:**
```python
# Check intermediate results
print(f"Parsed rows: {parsed_df.count()}")
print(f"Transformed rows: {transformed_df.count()}")
print(f"Daily rows: {daily_df.count()}")

# Show nulls
transformed_df.filter(col("date").isNull()).show()
```

---

### **5. File Already Exists**
**Error:** `Path already exists`  
**NguyÃªn nhÃ¢n:** Output folder Ä‘Ã£ tá»“n táº¡i  
**Giáº£i phÃ¡p:**
```bash
# XÃ³a output cÅ©
rm -rf streaming_output_spark_BATCH

# Hoáº·c dÃ¹ng append mode
.mode("append")

# Hoáº·c ignore mode
.mode("ignore")
```

---

## ğŸ“ˆ Performance

### **Processing Time**
| Stage | Time | Note |
|-------|------|------|
| Read Kafka | 1-2s | 1,000 messages |
| Parse JSON | 0.5s | Schema apply |
| Aggregation | 0.5s | Group by 2 symbols |
| Write Parquet | 0.5s | 2 rows |
| **Total** | **2-3s** | Very fast! |

### **Throughput**
- **Input:** 1,008 messages (504 BTC + 504 ETH)
- **Output:** 2 rows (1 BTC + 1 ETH daily)
- **Compression:** 504:1 ratio

### **Resource Usage**
- **Memory:** 1-2 GB (default Spark driver)
- **CPU:** 1-2 cores
- **Disk:** ~10 KB output (Parquet compressed)

---

## ğŸ“ Key Technologies

- **Spark Batch Read:** `spark.read.format("kafka")`
- **Kafka Consumer:** Read from earliest to latest offsets
- **JSON Parsing:** `from_json()` with schema
- **Group By Aggregation:** Daily OHLC
- **Parquet:** Columnar storage with partitioning

---

## ğŸ”— Integration

### **Lambda Architecture Flow**
```
Binance API
  â†“
websocket_producer.py
  â†“ (Kafka: crypto-prices)
kafka_batch_reader.py (DEMO - Instant)
  â†“ (Parquet: streaming_output_spark_BATCH/)
week6_merge.py
  â†“ (Merge Batch + Speed)
prophet_train.py
```

### **Alternative: Streaming Flow**
```
websocket_producer.py
  â†“ (Kafka: crypto-prices)
spark_streaming_consumer.py (PRODUCTION - 25h)
  â†“ (Parquet: streaming_output_spark/)
week6_merge.py
```

---

## âš™ï¸ Demo Strategy

### **Giáº£ng viÃªn há»i: "Speed Layer tháº­t sá»± hoáº¡t Ä‘á»™ng khÃ´ng?"**

**Tráº£ lá»i:**
> "Dáº¡ cÃ³ áº¡! Em cháº¡y demo cho tháº§y xem ngay:
> 1. Producer Ä‘Ã£ gá»­i 1,008 messages vÃ o Kafka
> 2. Batch Reader Ä‘á»c táº¥t cáº£ messages trong 2 giÃ¢y
> 3. Káº¿t quáº£: 2 rows daily OHLC Ä‘Ã£ save vÃ o Parquet
> 4. Schema giá»‘ng há»‡t Batch Layer (ready to merge)
> 
> Production thÃ¬ dÃ¹ng spark_streaming_consumer.py vá»›i:
> - Continuous streaming (micro-batches 10s)
> - Watermark 1h (handle late data)
> - Window 1 day (tumbling window)
> - Checkpoint (fault tolerance)
> - Cáº§n Ä‘á»£i 25h Ä‘á»ƒ window Ä‘Ã³ng
> 
> NÃªn em dÃ¹ng batch reader Ä‘á»ƒ demo nhanh cho tháº§y!"

---

## ğŸ“Š Comparison Table

| Feature | Batch Reader | Streaming Consumer |
|---------|-------------|-------------------|
| **Read API** | `spark.read` | `spark.readStream` |
| **Processing** | 1 láº§n | Continuous |
| **Offsets** | earliest â†’ latest | earliest â†’ ongoing |
| **Watermark** | âŒ KhÃ´ng cáº§n | âœ… 1 hour |
| **Window** | âŒ KhÃ´ng cáº§n | âœ… 1 day tumbling |
| **Write API** | `.write` | `.writeStream` |
| **Output Mode** | overwrite/append | append/complete |
| **Time** | **2-3 giÃ¢y** | **25 giá»** |
| **Checkpoint** | âŒ KhÃ´ng cÃ³ | âœ… Fault tolerance |
| **State** | âŒ Stateless | âœ… Stateful |
| **Exactly-once** | âŒ No | âœ… Yes |
| **Use Case** | Demo, Testing | Production 24/7 |

---

**TÃ¡c giáº£:** ÄoÃ n Tháº¿ TÃ­n  
**MSSV:** 4551190056  
**File:** `week6_streaming/kafka_batch_reader.py`  
**Lines:** 104 dÃ²ng code  
**Má»¥c Ä‘Ã­ch:** Speed Layer Demo Reader - Batch mode processing cho presentation vÃ  testing

---
