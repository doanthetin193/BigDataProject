# PhÃ¢n tÃ­ch vÃ  Dá»± Ä‘oÃ¡n GiÃ¡ Tiá»n MÃ£ HÃ³a sá»­ dá»¥ng Apache Spark

**Äá» tÃ i:** PhÃ¢n tÃ­ch vÃ  dá»± Ä‘oÃ¡n xu hÆ°á»›ng giÃ¡ tiá»n mÃ£ hÃ³a (BTC vÃ  ETH) vá»›i Lambda Architecture  
**Sinh viÃªn:** ÄoÃ n Tháº¿ TÃ­n  
**MSSV:** 4551190056  
**Lá»›p:** KTPM45

---

## ğŸ“‹ Tá»•ng quan

Dá»± Ã¡n xÃ¢y dá»±ng há»‡ thá»‘ng phÃ¢n tÃ­ch dá»¯ liá»‡u lá»›n vÃ  dá»± Ä‘oÃ¡n giÃ¡ tiá»n mÃ£ hÃ³a (Bitcoin, Ethereum) sá»­ dá»¥ng **Lambda Architecture** vá»›i Apache Spark, Kafka, vÃ  Facebook Prophet.

### Äáº·c Ä‘iá»ƒm ná»•i báº­t:
- âœ… **Quy mÃ´ dá»¯ liá»‡u:** 50+ triá»‡u dÃ²ng (tick-level 1 phÃºt, 2012-2025)
- âœ… **Lambda Architecture:** Batch Layer + Speed Layer + Serving Layer
- âœ… **Real-time Streaming:** Kafka + Spark Structured Streaming + WebSocket
- âœ… **Machine Learning:** Facebook Prophet vá»›i MAPE < 4%
- âœ… **Dá»¯ liá»‡u sáº¡ch:** 8,140 ngÃ y sau xá»­ lÃ½, backfill gaps tá»± Ä‘á»™ng

---

## ğŸ—ï¸ Kiáº¿n trÃºc há»‡ thá»‘ng (Lambda Architecture)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                        DATA SOURCES                              â”‚
â”‚  - Historical CSV (50M+ rows)                                    â”‚
â”‚  - Binance WebSocket API (Real-time)                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
        â†“                                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚   BATCH LAYER     â”‚                   â”‚    SPEED LAYER        â”‚
â”‚                   â”‚                   â”‚                       â”‚
â”‚ â€¢ Preprocessing   â”‚                   â”‚ â€¢ WebSocket Producer  â”‚
â”‚ â€¢ Backfill Gaps   â”‚                   â”‚ â€¢ Kafka (1 partition) â”‚
â”‚ â€¢ Daily Aggregate â”‚                   â”‚ â€¢ Spark Streaming     â”‚
â”‚ â€¢ Output: 8,140   â”‚                   â”‚ â€¢ Daily Aggregate     â”‚
â”‚   rows Parquet    â”‚                   â”‚ â€¢ Output: Parquet     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
        â”‚                                           â”‚
        â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚   SERVING LAYER       â”‚
                  â”‚                       â”‚
                  â”‚ â€¢ Merge Batch + Speed â”‚
                  â”‚ â€¢ Deduplication       â”‚
                  â”‚ â€¢ Prophet Input       â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                              â†“
                  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                  â”‚   MACHINE LEARNING    â”‚
                  â”‚                       â”‚
                  â”‚ â€¢ Facebook Prophet    â”‚
                  â”‚ â€¢ MAPE: BTC 3.36%     â”‚
                  â”‚        ETH 3.90%      â”‚
                  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ Cáº¥u trÃºc thÆ° má»¥c

```
BigDataProject/
â”‚
â”œâ”€â”€ data/                          # Dá»¯ liá»‡u thÃ´
â”‚   â”œâ”€â”€ btc/BTCUSDT_1min_2012-2025.csv  (28M rows)
â”‚   â””â”€â”€ eth/ETHUSDT_1min_2017-2025.csv  (24M rows)
â”‚
â”œâ”€â”€ data_parquet/                  # Dá»¯ liá»‡u Parquet (partitioned by year)
â”‚   â”œâ”€â”€ btc_clean/                 # BTC Ä‘Ã£ lÃ m sáº¡ch
â”‚   â””â”€â”€ eth_clean/                 # ETH Ä‘Ã£ lÃ m sáº¡ch
â”‚
â”œâ”€â”€ data_analysis/                 # Output phÃ¢n tÃ­ch
â”‚   â”œâ”€â”€ daily_filled/              # Batch Layer output (8,140 rows)
â”‚   â”œâ”€â”€ prophet_input/             # Input cho Prophet (merged)
â”‚   â”œâ”€â”€ prophet_forecasts/         # Káº¿t quáº£ dá»± Ä‘oÃ¡n (Parquet)
â”‚   â”œâ”€â”€ prophet_results/           # Actual vs Predicted (CSV)
â”‚   â”œâ”€â”€ prophet_metrics/           # MAPE, MAE, RMSE
â”‚   â””â”€â”€ prophet_visualizations/    # Biá»ƒu Ä‘á»“ HTML tÆ°Æ¡ng tÃ¡c
â”‚
â”œâ”€â”€ week6_streaming/               # Speed Layer
â”‚   â”œâ”€â”€ docker-compose.yml         # Kafka + Zookeeper setup
â”‚   â”œâ”€â”€ websocket_producer.py      # Binance WebSocket â†’ Kafka
â”‚   â”œâ”€â”€ spark_streaming_consumer.py # Kafka â†’ Spark Streaming (PRODUCTION)
â”‚   â”‚                              #   â€¢ Watermark: 1h
â”‚   â”‚                              #   â€¢ Window: 1 day tumbling
â”‚   â”‚                              #   â€¢ Output: Chá»‰ khi window close
â”‚   â”œâ”€â”€ kafka_batch_reader.py      # Kafka â†’ Batch read (DEMO)
â”‚   â”‚                              #   â€¢ Batch mode: Äá»c táº¥t cáº£ messages
â”‚   â”‚                              #   â€¢ Aggregate: Daily OHLCV
â”‚   â”‚                              #   â€¢ Output: Ngay láº­p tá»©c
â”‚   â”œâ”€â”€ checkpoint_spark/          # Checkpoint metadata (git ignored)
â”‚   â””â”€â”€ streaming_output_spark_BATCH/ # Speed Layer output (git ignored)
â”‚
â”œâ”€â”€ scripts/                       # Source code
â”‚   â”œâ”€â”€ preprocessing/             # 4 scripts tiá»n xá»­ lÃ½
â”‚   â”‚   â”œâ”€â”€ convert_to_parquet.py
â”‚   â”‚   â”œâ”€â”€ clean_parquet.py
â”‚   â”‚   â”œâ”€â”€ preprocess_step1.py
â”‚   â”‚   â””â”€â”€ preprocess_step2.py
â”‚   â”‚
â”‚   â”œâ”€â”€ lambda_batch/              # Batch Layer
â”‚   â”‚   â”œâ”€â”€ week6_backfill.py      # Backfill gaps < 30 ngÃ y
â”‚   â”‚   â”œâ”€â”€ week6_backfill_batch.py # Backfill gaps lá»›n
â”‚   â”‚   â””â”€â”€ week6_merge.py         # Merge Batch + Speed
â”‚   â”‚
â”‚   â””â”€â”€ ml_models/                 # Machine Learning
â”‚       â””â”€â”€ prophet_train.py       # Train & forecast Prophet
â”‚
â”œâ”€â”€ docs/                          # TÃ i liá»‡u (sáº½ táº¡o láº¡i)
â””â”€â”€ README.md                      # File nÃ y
```

---

## ğŸš€ HÆ°á»›ng dáº«n cháº¡y

### **1. Chuáº©n bá»‹ mÃ´i trÆ°á»ng**

```bash
# CÃ i Ä‘áº·t dependencies
pip install pyspark pandas numpy matplotlib prophet requests websocket-client

# CÃ i Docker Desktop (cho Kafka)
# Download: https://www.docker.com/products/docker-desktop
```

### **2. Preprocessing (Batch Layer - BÆ°á»›c 1)**

```bash
# BÆ°á»›c 1: Chuyá»ƒn CSV â†’ Parquet
cd D:\BigDataProject
python scripts/preprocessing/convert_to_parquet.py

# BÆ°á»›c 2: LÃ m sáº¡ch dá»¯ liá»‡u
python scripts/preprocessing/clean_parquet.py

# BÆ°á»›c 3: Táº¡o daily aggregates
python scripts/preprocessing/preprocess_step1.py

# BÆ°á»›c 4: Äiá»n gaps vÃ  tÃ­nh MA
python scripts/preprocessing/preprocess_step2.py
```

**Output:** `data_analysis/daily_filled/` (~7,980 rows)

### **3. Backfill Gaps (Batch Layer - BÆ°á»›c 2)**

```bash
# Backfill gaps nhá» (<30 ngÃ y)
python scripts/lambda_batch/week6_backfill.py

# Backfill gap lá»›n (79 ngÃ y, Nov-Dec 2024)
python scripts/lambda_batch/week6_backfill_batch.py
```

**Output:** `data_analysis/daily_filled/` (8,140 rows - HOÃ€N CHá»ˆNH)

### **4. Speed Layer (Real-time Streaming)**

#### **QUAN TRá»ŒNG - 2 CÃ¡ch cháº¡y Speed Layer:**

**A. Production Mode** (Spark Streaming Consumer):
```bash
cd week6_streaming
docker-compose up -d
python websocket_producer.py      # Terminal 1
python spark_streaming_consumer.py # Terminal 2
```
- âœ… **Watermark:** 1 giá»
- âœ… **Window:** 1 ngÃ y (tumbling)
- âš ï¸ **LÆ°u Ã½:** Pháº£i Ä‘á»£i Ä‘áº¿n 00:00 ngÃ y hÃ´m sau + 1h watermark â†’ Window má»›i close â†’ Má»›i cÃ³ output file
- **Use case:** Production (cháº¡y liÃªn tá»¥c 24/7)

**B. Demo Mode** (Kafka Batch Reader - **KHUYáº¾N NGHá»Š CHO DEMO**):
```bash
cd week6_streaming
docker-compose up -d

# Cháº¡y Producer 1-2 phÃºt (láº¥y ~1000-2000 messages)
python websocket_producer.py
# Ctrl+C sau 1-2 phÃºt

# Cháº¡y Batch Reader (output ngay láº­p tá»©c)
python kafka_batch_reader.py
```
- âœ… **Batch Mode:** Äá»c táº¥t cáº£ messages tá»« Kafka
- âœ… **Aggregate:** Daily OHLCV
- âœ… **Output ngay:** KhÃ´ng cáº§n Ä‘á»£i window close
- **Use case:** Demo nhanh, testing, POC

**Giáº£i thÃ­ch cho giáº£ng viÃªn:**
> "Em Ä‘Ã£ implement Spark Streaming Consumer production vá»›i 1-day window (file `spark_streaming_consumer.py`), nhÆ°ng vÃ¬ window 1 ngÃ y nÃªn pháº£i Ä‘á»£i lÃ¢u má»›i cÃ³ output. Äá»ƒ demo nhanh, em viáº¿t thÃªm Kafka Batch Reader (`kafka_batch_reader.py`) Ä‘á»c batch mode tá»« Kafka vÃ  aggregate ngay. Cáº£ 2 file Ä‘á»u chá»©ng minh Kafka + Spark hoáº¡t Ä‘á»™ng tá»‘t."

**Setup Kafka:**
```bash
# BÆ°á»›c 1: Khá»Ÿi Ä‘á»™ng Kafka
cd week6_streaming
docker-compose up -d

# Äá»£i 15s Ä‘á»ƒ Kafka khá»Ÿi Ä‘á»™ng
# Verify: docker ps (2 containers running)

# BÆ°á»›c 2: Táº¡o topic (1 partition)
docker exec kafka kafka-topics --create --topic crypto-prices \
  --partitions 1 --replication-factor 1 --bootstrap-server localhost:9092

# BÆ°á»›c 3: Cháº¡y Producer (1-2 phÃºt)
python websocket_producer.py
# Ctrl+C sau khi tháº¥y ~1000-2000 messages

# BÆ°á»›c 4: Cháº¡y Batch Reader (demo nhanh)
python kafka_batch_reader.py
```

**Output:** `streaming_output_spark_BATCH/` (2 rows: BTCUSDT, ETHUSDT - daily aggregate)

### **5. Serving Layer (Merge Batch + Speed)**

```bash
# Merge dá»¯ liá»‡u tá»« Batch + Speed Layer
python scripts/lambda_batch/week6_merge.py
```

**Output:** 
- `data_analysis/daily_filled/` (8,140 rows - overwrite with cache)
- `data_analysis/prophet_input/` (8,140 rows - ready for ML)

### **6. Machine Learning (Prophet)**

```bash
# Train vÃ  forecast
python scripts/ml_models/prophet_train.py
```

**Output:**
- `prophet_forecasts/` - Dá»± Ä‘oÃ¡n 30 ngÃ y (Parquet)
- `prophet_metrics/metrics.csv` - MAPE, MAE, RMSE
- `prophet_results/` - Actual vs Predicted CSV
- `prophet_visualizations/` - Biá»ƒu Ä‘á»“ HTML tÆ°Æ¡ng tÃ¡c

**Backup tá»± Ä‘á»™ng:**
- Má»—i láº§n cháº¡y láº¡i â†’ backup sang `*_backup/`
- Äáº£m báº£o khÃ´ng máº¥t káº¿t quáº£ cÅ©

---

## ğŸ“Š Káº¿t quáº£ Ä‘áº¡t Ä‘Æ°á»£c

### **1. Batch Layer**
- âœ… **Dá»¯ liá»‡u gá»‘c:** 50M+ rows (CSV 1-phÃºt)
- âœ… **Sau preprocessing:** 7,980 rows daily
- âœ… **Sau backfill:** 8,140 rows (100% complete tá»« 2017-2025)
- âœ… **Features:** OHLCV + MA7 + MA30

### **2. Speed Layer**
- âœ… **Kafka:** 1 partition, 1,008 messages
- âœ… **WebSocket Producer:** Binance real-time API
- âœ… **Spark Streaming Consumer:** Micro-batch 10s
- âœ… **Output:** 2 rows daily aggregate (BTCUSDT, ETHUSDT)

### **3. Serving Layer**
- âœ… **Merge:** Batch (8,140) + Speed (2) = 8,140 rows
- âœ… **Deduplication:** Batch priority, bá» Speed trÃ¹ng ngÃ y
- âœ… **Cache fix:** TrÃ¡nh conflict khi ghi Parquet
- âœ… **Prophet input:** OHLCV + MA7 + MA30

### **4. Machine Learning**
| Coin | MAPE | MAE | RMSE |
|------|------|-----|------|
| **BTCUSDT** | **3.36%** | $2,994 | $3,682 |
| **ETHUSDT** | **3.90%** | $120.43 | $145.82 |

**Káº¿t luáº­n:** MAPE < 4% â†’ MÃ´ hÃ¬nh dá»± Ä‘oÃ¡n ráº¥t tá»‘t!

---

## ğŸ› ï¸ CÃ´ng nghá»‡ sá»­ dá»¥ng

### **Big Data**
- **Apache Spark 3.5.3:** PySpark, SparkSQL, Structured Streaming
- **Apache Kafka:** Message broker (Confluent 7.5.0)
- **Parquet:** Columnar storage format
- **Docker:** Container cho Kafka + Zookeeper

### **Machine Learning**
- **Facebook Prophet:** Time series forecasting
- **Pandas:** Data manipulation
- **NumPy:** Numerical computation

### **Visualization**
- **Matplotlib:** Static plots
- **Plotly:** Interactive HTML charts

### **Real-time Data**
- **WebSocket:** Binance Ticker API
- **Requests:** HTTP API calls

---

## ğŸ“ˆ Demo Workflow

### **Quick Demo (5-10 phÃºt) - KHUYáº¾N NGHá»Š**

```bash
# 1. Start Kafka
cd week6_streaming
docker-compose up -d
# Äá»£i 15s

# 2. Producer - Láº¥y real-time data (1-2 phÃºt)
python websocket_producer.py
# Ctrl+C sau ~1000-2000 messages

# 3. Batch Reader - Aggregate ngay
python kafka_batch_reader.py

# 4. Verify output
cd ..
python -c "from pyspark.sql import SparkSession; spark = SparkSession.builder.appName('Demo').getOrCreate(); df = spark.read.parquet('week6_streaming/streaming_output_spark_BATCH/'); print(f'Speed Layer output: {df.count()} rows'); df.show(); spark.stop()"

# 5. Merge Batch + Speed
python scripts/lambda_batch/week6_merge.py

# 6. Prophet Forecast
python scripts/ml_models/prophet_train.py

# 7. Xem káº¿t quáº£
# Má»Ÿ: data_analysis/prophet_visualizations/BTCUSDT_forecast_interactive.html
```

**Giáº£i thÃ­ch Demo:**
1. âœ… **Batch Layer:** 8,140 rows historical data (Ä‘Ã£ cÃ³ sáºµn)
2. âœ… **Speed Layer:** Real-time tá»« Kafka (demo vá»›i batch reader)
3. âœ… **Serving Layer:** Merge + deduplication
4. âœ… **ML:** Prophet forecast vá»›i MAPE < 4%
5. âœ… **Visualization:** Interactive HTML chart

**Káº¿t quáº£:** Biá»ƒu Ä‘á»“ dá»± Ä‘oÃ¡n giÃ¡ BTC 30 ngÃ y tá»›i vá»›i confidence interval!

---

### **Full Scenario: Dá»± Ä‘oÃ¡n giÃ¡ BTC hÃ´m nay tá»« Ä‘áº§u**

```bash
# 1. Preprocessing (náº¿u chÆ°a cÃ³ data_parquet)
python scripts/preprocessing/convert_to_parquet.py
python scripts/preprocessing/clean_parquet.py
python scripts/preprocessing/preprocess_step1.py
python scripts/preprocessing/preprocess_step2.py

# 2. Backfill gaps (náº¿u cÃ³ gap)
python scripts/lambda_batch/week6_backfill.py

# 3. Speed Layer
cd week6_streaming
docker-compose up -d
python websocket_producer.py  # 1-2 phÃºt
python kafka_batch_reader.py
cd ..

# 4. Merge
python scripts/lambda_batch/week6_merge.py

# 5. Prophet
python scripts/ml_models/prophet_train.py

# 6. View results
# Má»Ÿ: data_analysis/prophet_visualizations/BTCUSDT_forecast_interactive.html
```

---

## ğŸ¯ So sÃ¡nh vá»›i Äá» cÆ°Æ¡ng

| YÃªu cáº§u | Äá» cÆ°Æ¡ng gá»‘c | Thá»±c hiá»‡n |
|---------|--------------|-----------|
| **Quy mÃ´ dá»¯ liá»‡u** | 50-100M rows | âœ… 50M+ rows |
| **Streaming** | Poll API Ä‘Æ¡n giáº£n | âœ… **Kafka + WebSocket** ğŸŒŸ |
| **Architecture** | KhÃ´ng rÃµ | âœ… **Lambda Architecture** ğŸŒŸ |
| **ML Model** | Prophet | âœ… Prophet MAPE < 4% |
| **Visualization** | Matplotlib | âœ… Matplotlib + Plotly HTML |

**Äiá»ƒm cá»™ng lá»›n:**
- ğŸŒŸ **Kafka + Docker:** Production-ready streaming
- ğŸŒŸ **Lambda Architecture:** Batch + Speed + Serving
- ğŸŒŸ **MAPE < 4%:** Dá»± Ä‘oÃ¡n ráº¥t chÃ­nh xÃ¡c

---

## ğŸ“ TÃ i liá»‡u tham kháº£o

1. Apache Spark Documentation: https://spark.apache.org/docs/latest/
2. Facebook Prophet: https://facebook.github.io/prophet/
3. Confluent Kafka: https://docs.confluent.io/
4. Binance WebSocket API: https://binance-docs.github.io/apidocs/spot/en/
5. Lambda Architecture: http://lambda-architecture.net/

---

## ğŸ‘¨â€ğŸ’» TÃ¡c giáº£

**ÄoÃ n Tháº¿ TÃ­n**  
MSSV: 4551190056  
Lá»›p: KTPM45  
Email: [ThÃªm email náº¿u cáº§n]

---

## ğŸ“… Timeline thá»±c hiá»‡n

- **Tuáº§n 1-2:** Thu tháº­p vÃ  preprocessing dá»¯ liá»‡u (50M rows)
- **Tuáº§n 3:** Batch Layer (daily aggregates, backfill)
- **Tuáº§n 4:** Machine Learning (Prophet training, MAPE < 4%)
- **Tuáº§n 5:** Tá»• chá»©c code, documentation
- **Tuáº§n 6:** Speed Layer (Kafka + Spark Streaming) â­
- **Tuáº§n 7-8:** HoÃ n thiá»‡n bÃ¡o cÃ¡o vÃ  demo

---

## ğŸ”§ Troubleshooting

### **1. Kafka Consumer Timeout**
**Lá»—i:** `TimeoutException: Timeout waiting for data from partition`

**NguyÃªn nhÃ¢n:** Topic cÃ³ nhiá»u partition, consumer chá»‰ Ä‘á»c partition 0

**Giáº£i phÃ¡p:**
```bash
# XÃ³a topic cÅ©
docker exec kafka kafka-topics --delete --topic crypto-prices --bootstrap-server localhost:9092

# Táº¡o láº¡i vá»›i 1 partition
docker exec kafka kafka-topics --create --topic crypto-prices --partitions 1 --replication-factor 1 --bootstrap-server localhost:9092
```

### **2. Streaming Consumer khÃ´ng táº¡o file**
**NguyÃªn nhÃ¢n:** Window 1 ngÃ y chÆ°a close (pháº£i Ä‘á»£i Ä‘áº¿n 00:00 + 1h watermark)

**Giáº£i phÃ¡p:** DÃ¹ng Kafka Batch Reader thay vÃ¬ Streaming Consumer cho demo
```bash
python kafka_batch_reader.py  # Output ngay láº­p tá»©c
```

### **3. Binance API Timeout**
**Lá»—i:** `ConnectionTimeout` khi cháº¡y backfill

**Giáº£i phÃ¡p:** 
- Thá»­ láº¡i sau vÃ i phÃºt (rate limit)
- Hoáº·c bá» qua backfill, dÃ¹ng data hiá»‡n cÃ³ (14/12) + Speed Layer realtime (16/12)

### **4. Week6_merge.py lá»—i self-reference**
**Lá»—i:** `AnalysisException: Cannot overwrite a path that is also being read`

**Giáº£i phÃ¡p:** ÄÃ£ fix báº±ng `.cache()` trÆ°á»›c khi ghi (line 67-70 trong week6_merge.py)

### **5. Docker Desktop khÃ´ng start**
**Giáº£i phÃ¡p:**
- Restart Docker Desktop
- Hoáº·c dÃ¹ng WSL2 backend
- Check port 9092 khÃ´ng bá»‹ chiáº¿m

---

## ğŸ“ Ghi chÃº

Dá»± Ã¡n nÃ y lÃ  Ä‘á»“ Ã¡n cÃ¡ nhÃ¢n mÃ´n Big Data Analytics, minh há»a quy trÃ¬nh phÃ¢n tÃ­ch dá»¯ liá»‡u lá»›n tá»« thu tháº­p, xá»­ lÃ½, Ä‘áº¿n dá»± Ä‘oÃ¡n vá»›i cÃ´ng nghá»‡ production-ready (Kafka, Spark Streaming, Lambda Architecture).

**License:** Educational use only.
