# WEEK 6 - SPARK STREAMING CONSUMER - GIáº¢I THÃCH CHI TIáº¾T

## ğŸ“Œ Tá»”NG QUAN

**File:** `week6_streaming/spark_streaming_consumer.py` (308 dÃ²ng)

**Vai trÃ²:** Speed Layer Consumer - Nháº­n real-time data tá»« Kafka, xá»­ lÃ½ streaming vá»›i PySpark

**CÃ´ng nghá»‡:**

- Apache Spark Structured Streaming
- Kafka Consumer
- Window Aggregation (Daily)
- Watermarking (xá»­ lÃ½ late data)
- Checkpoint (fault tolerance)

**Input:** Kafka topic `crypto-prices` (tá»« websocket_producer.py)

**Output:**

- Parquet files: `streaming_output_spark/daily/`
- Console monitoring
- In-memory table: `crypto_daily_stats`

---

## ğŸ”§ Cáº¤U TRÃšC FILE

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ PHáº¦N 1: Import & Configuration     â”‚
â”‚ PHáº¦N 2: Spark Session              â”‚
â”‚ PHáº¦N 3: Schema Definition          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ STEP 1: Read Stream from Kafka     â”‚
â”‚ STEP 2: Parse JSON Data            â”‚
â”‚ STEP 3: Data Transformation        â”‚
â”‚ STEP 4: Watermarking               â”‚
â”‚ STEP 5: Daily Aggregation          â”‚
â”‚ STEP 6: Write Streams (3 queries)  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ PHáº¦N 8: Monitoring                 â”‚
â”‚ PHáº¦N 9: Graceful Shutdown          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“– GIáº¢I THÃCH CHI TIáº¾T

### **PHáº¦N 1: IMPORT & CONFIGURATION (DÃ²ng 1-23)**

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import *
from pyspark.sql.types import *
import os
```

**Import:**

- `SparkSession`: Táº¡o Spark application
- `functions`: CÃ¡c hÃ m nhÆ° `from_json`, `window`, `col`, `to_date`, `hour`
- `types`: Äá»‹nh nghÄ©a schema (StructType, StructField, StringType, DoubleType...)
- `os`: Xá»­ lÃ½ Ä‘Æ°á»ng dáº«n file (khÃ´ng dÃ¹ng trong code nÃ y)

```python
KAFKA_BOOTSTRAP_SERVERS = "localhost:9092"
KAFKA_TOPIC = "crypto-prices"
OUTPUT_PATH = "streaming_output_spark"
CHECKPOINT_PATH = "checkpoint_spark"
```

**Configuration:**

- **Kafka Server:** `localhost:9092` - nÆ¡i Kafka Ä‘ang cháº¡y
- **Topic:** `crypto-prices` - producer gá»­i data vÃ o topic nÃ y
- **Output Path:** ThÆ° má»¥c lÆ°u káº¿t quáº£ Parquet
- **Checkpoint Path:** LÆ°u tiáº¿n trÃ¬nh xá»­ lÃ½ (fault tolerance)

**Táº¡i sao cáº§n Checkpoint?**

```
Scenario:
1. Spark Ä‘ang xá»­ lÃ½ window 2025-12-10
2. Crash giá»¯a chá»«ng
3. Restart láº¡i â†’ Ä‘á»c checkpoint â†’ tiáº¿p tá»¥c tá»« window 2025-12-10
4. KhÃ´ng bá»‹ máº¥t data, khÃ´ng xá»­ lÃ½ trÃ¹ng
```

---

### **PHáº¦N 2: SPARK SESSION (DÃ²ng 24-42)**

```python
spark = SparkSession.builder \
    .appName("CryptoPriceStructuredStreaming") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.3") \
    .config("spark.sql.streaming.checkpointLocation", CHECKPOINT_PATH) \
    .config("spark.sql.adaptive.enabled", "true") \
    .config("spark.driver.memory", "4g") \
    .getOrCreate()
```

**Tá»«ng config:**

**1. `.appName("CryptoPriceStructuredStreaming")`**

- TÃªn application hiá»ƒn thá»‹ trong Spark UI

**2. `.config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.3")`**

- **QUAN TRá»ŒNG NHáº¤T!**
- Táº£i thÆ° viá»‡n Kafka Connector tá»« Maven
- Spark version 3.5.3, Scala 2.12
- KhÃ´ng cÃ³ config nÃ y â†’ khÃ´ng káº¿t ná»‘i Ä‘Æ°á»£c Kafka

**3. `.config("spark.sql.streaming.checkpointLocation", CHECKPOINT_PATH)`**

- Checkpoint location máº·c Ä‘á»‹nh
- Má»—i query cÃ³ thá»ƒ override báº±ng checkpoint riÃªng

**4. `.config("spark.sql.adaptive.enabled", "true")`**

- Adaptive Query Execution (AQE)
- Spark tá»± Ä‘á»™ng tá»‘i Æ°u query execution plan
- Cáº£i thiá»‡n performance

**5. `.config("spark.driver.memory", "4g")`**

- Driver process dÃ¹ng 4GB RAM
- Äá»§ cho streaming nhá»/vá»«a (2 symbols)

```python
spark.sparkContext.setLogLevel("WARN")
```

- Chá»‰ hiá»ƒn thá»‹ log level WARN/ERROR
- KhÃ´ng spam mÃ n hÃ¬nh vá»›i INFO/DEBUG logs

---

### **PHáº¦N 3: SCHEMA DEFINITION (DÃ²ng 43-60)**

```python
message_schema = StructType([
    StructField("symbol", StringType(), True),
    StructField("event_time", LongType(), True),
    StructField("price", DoubleType(), True),
    StructField("open", DoubleType(), True),
    StructField("high", DoubleType(), True),
    StructField("low", DoubleType(), True),
    StructField("volume", DoubleType(), True),
    StructField("quote_volume", DoubleType(), True),
    StructField("number_trades", IntegerType(), True),
    StructField("price_change", DoubleType(), True),
    StructField("price_change_percent", DoubleType(), True),
    StructField("timestamp", StringType(), True)
])
```

**Táº¡i sao cáº§n schema?**

Kafka message lÃ  **JSON string**:

```json
{
  "symbol": "BTCUSDT",
  "event_time": 1733856000123,
  "price": 42156.78,
  ...
}
```

Spark cáº§n **parse JSON** â†’ DataFrame vá»›i datatypes chÃ­nh xÃ¡c:

- `symbol`: String
- `event_time`: Long (milliseconds)
- `price`: Double
- `number_trades`: Integer

**Mapping vá»›i Producer:**

Producer (`websocket_producer.py`) gá»­i:

```python
message = {
    "symbol": ticker["symbol"],
    "event_time": ticker["closeTime"],  # Long
    "price": float(ticker["lastPrice"]),  # Double
    ...
}
```

Consumer parse theo schema â†’ DataFrame vá»›i Ä‘Ãºng datatype.

---

### **STEP 1: READ STREAM FROM KAFKA (DÃ²ng 61-77)**

```python
kafkaDF = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", KAFKA_BOOTSTRAP_SERVERS) \
    .option("subscribe", KAFKA_TOPIC) \
    .option("startingOffsets", "earliest") \
    .option("failOnDataLoss", "false") \
    .load()
```

**Tá»«ng option:**

**1. `.readStream` (khÃ´ng pháº£i `.read`)**

- Streaming mode (continuous processing)
- Batch mode dÃ¹ng `.read`

**2. `.format("kafka")`**

- Sá»­ dá»¥ng Kafka connector (tá»« jars.packages)

**3. `.option("kafka.bootstrap.servers", "localhost:9092")`**

- Kafka server address
- CÃ³ thá»ƒ cÃ³ nhiá»u servers: `"server1:9092,server2:9092"`

**4. `.option("subscribe", "crypto-prices")`**

- Subscribe vÃ o topic `crypto-prices`
- CÃ³ thá»ƒ subscribe nhiá»u topics: `"topic1,topic2"`

**5. `.option("startingOffsets", "earliest")`**

- **Láº§n Ä‘áº§u cháº¡y:** Ä‘á»c tá»« message Ä‘áº§u tiÃªn trong topic
- **Láº§n sau:** dÃ¹ng checkpoint â†’ Ä‘á»c tá»« offset cuá»‘i cÃ¹ng Ä‘Ã£ xá»­ lÃ½

```
Timeline:
[Msg1] [Msg2] [Msg3] [Msg4] [Msg5] ... [MsgN]
  â†‘
earliest - Báº¯t Ä‘áº§u tá»« Ä‘Ã¢y

Checkpoint lÆ°u: "ÄÃ£ xá»­ lÃ½ Ä‘áº¿n Msg100"
Restart â†’ Tiáº¿p tá»¥c tá»« Msg101
```

**6. `.option("failOnDataLoss", "false")`**

- Kafka cÃ³ retention time â†’ message cÅ© bá»‹ xÃ³a
- `false`: KhÃ´ng crash khi message bá»‹ máº¥t
- `true`: Crash náº¿u detect data loss (strict mode)

**DataFrame schema sau khi load:**

```
kafkaDF:
â”œâ”€ key: binary
â”œâ”€ value: binary  â† JSON data (bytes)
â”œâ”€ topic: string
â”œâ”€ partition: int
â”œâ”€ offset: long
â”œâ”€ timestamp: timestamp
â””â”€ timestampType: int
```

---

### **STEP 2: PARSE JSON DATA (DÃ²ng 78-86)**

```python
parsedDF = kafkaDF.select(
    from_json(col("value").cast("string"), message_schema).alias("data"),
    col("timestamp").alias("kafka_timestamp")
).select("data.*", "kafka_timestamp")
```

**Breakdown:**

**BÆ°á»›c 1: `.cast("string")`**

```python
col("value").cast("string")
```

- `value` column lÃ  **binary** (bytes)
- Cast sang **string** Ä‘á»ƒ parse JSON

```
Binary: b'{"symbol":"BTCUSDT","price":42156.78,...}'
String: '{"symbol":"BTCUSDT","price":42156.78,...}'
```

**BÆ°á»›c 2: `from_json(..., message_schema)`**

```python
from_json(col("value").cast("string"), message_schema).alias("data")
```

- Parse JSON string theo schema
- Táº¡o **struct column** tÃªn `data`

```
Before:
value (string): '{"symbol":"BTCUSDT","price":42156.78}'

After:
data (struct):
  â”œâ”€ symbol: "BTCUSDT"
  â”œâ”€ price: 42156.78
  â””â”€ ...
```

**BÆ°á»›c 3: `.select("data.*", "kafka_timestamp")`**

```python
.select("data.*", "kafka_timestamp")
```

- `data.*`: Expand struct â†’ separate columns
- Giá»¯ láº¡i `kafka_timestamp` (timestamp Kafka nháº­n message)

**Káº¿t quáº£ cuá»‘i cÃ¹ng:**

```
parsedDF:
â”œâ”€ symbol: string
â”œâ”€ event_time: long
â”œâ”€ price: double
â”œâ”€ open: double
â”œâ”€ high: double
â”œâ”€ low: double
â”œâ”€ volume: double
â”œâ”€ quote_volume: double
â”œâ”€ number_trades: int
â”œâ”€ price_change: double
â”œâ”€ price_change_percent: double
â”œâ”€ timestamp: string
â””â”€ kafka_timestamp: timestamp
```

---

### **STEP 3: DATA TRANSFORMATION (DÃ²ng 87-99)**

```python
streamDF = parsedDF \
    .withColumn("event_timestamp", (col("event_time") / 1000).cast("timestamp")) \
    .withColumn("date", to_date(col("event_timestamp"))) \
    .withColumn("hour", hour(col("event_timestamp")))
```

**Transformation 1: Convert milliseconds â†’ timestamp**

```python
.withColumn("event_timestamp", (col("event_time") / 1000).cast("timestamp"))
```

Producer gá»­i `event_time` dáº¡ng **milliseconds**:

```
event_time: 1733856000123  (milliseconds)
```

Convert sang **timestamp**:

```
Step 1: 1733856000123 / 1000 = 1733856000.123 (seconds)
Step 2: cast("timestamp") â†’ 2025-12-10 12:00:00.123
```

**Transformation 2: Extract date**

```python
.withColumn("date", to_date(col("event_timestamp")))
```

```
event_timestamp: 2025-12-10 12:34:56
         â†“
date: 2025-12-10
```

**Transformation 3: Extract hour**

```python
.withColumn("hour", hour(col("event_timestamp")))
```

```
event_timestamp: 2025-12-10 12:34:56
         â†“
hour: 12
```

**Táº¡i sao cáº§n convert timestamp?**

Window aggregation cáº§n **timestamp datatype**:

```python
window(col("event_timestamp"), "1 day")  # âœ… OK
window(col("event_time"), "1 day")        # âŒ FAIL (Long type)
```

---

### **STEP 4: WATERMARKING (DÃ²ng 100-106)**

```python
watermarkedDF = streamDF.withWatermark("event_timestamp", "1 hour")
```

**Watermark lÃ  gÃ¬?**

Streaming data **khÃ´ng Ä‘á»“ng bá»™** - cÃ³ thá»ƒ Ä‘áº¿n muá»™n:

```
Timeline thá»±c táº¿:
12:00:00 - Event A xáº£y ra
12:00:01 - Event B xáº£y ra
12:00:02 - Event C xáº£y ra

Data Ä‘áº¿n Consumer:
12:00:01 - Nháº­n Event A âœ…
12:00:02 - Nháº­n Event C âœ…
12:00:05 - Nháº­n Event B âŒ (Late 4 giÃ¢y!)
```

**Watermark = "Chá» late data bao lÃ¢u?"**

```python
.withWatermark("event_timestamp", "1 hour")
```

Ã nghÄ©a:

- Chá» late data tá»‘i Ä‘a **1 giá»**
- Late > 1 giá» â†’ Bá» QUA (quÃ¡ muá»™n)

**VÃ­ dá»¥ thá»±c táº¿:**

```
Current time: 13:05
Watermark: 13:05 - 1 hour = 12:05

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Data nháº­n Ä‘Æ°á»£c:                        â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ event_time=13:04 âœ… OK                 â”‚
â”‚ event_time=13:00 âœ… OK                 â”‚
â”‚ event_time=12:30 âœ… OK (late 35 min)   â”‚
â”‚ event_time=12:05 âœ… OK (late 1 hour)   â”‚
â”‚ event_time=12:04 âŒ Bá» QUA (late > 1h) â”‚
â”‚ event_time=11:00 âŒ Bá» QUA (late > 1h) â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**Watermark áº£nh hÆ°á»Ÿng Ä‘áº¿n Window:**

```
Window: 2025-12-10 00:00 â†’ 2025-12-10 24:00
Watermark: 1 hour

Scenario:
- 2025-12-11 00:30: Váº«n nháº­n data 2025-12-10 23:30 âœ…
- 2025-12-11 01:01: KhÃ´ng nháº­n data 2025-12-10 23:59 âŒ
  â†’ Window 2025-12-10 ÄÃ“NG â†’ ghi file
```

**TÃ³m láº¡i:** Watermark = "deadline" Ä‘á»ƒ window Ä‘Ã³ng láº¡i vÃ  ghi káº¿t quáº£.

---

### **STEP 5: DAILY AGGREGATION (DÃ²ng 107-139)**

```python
dailyDF = watermarkedDF \
    .groupBy(
        window(col("event_timestamp"), "1 day"),
        col("symbol")
    ) \
    .agg(...)
```

**Window Aggregation:**

```python
window(col("event_timestamp"), "1 day")
```

Chia data thÃ nh **cá»­a sá»• 1 ngÃ y**:

```
Window 1: 2025-12-10 00:00:00 â†’ 2025-12-11 00:00:00
Window 2: 2025-12-11 00:00:00 â†’ 2025-12-12 00:00:00
Window 3: 2025-12-12 00:00:00 â†’ 2025-12-13 00:00:00
```

**Group by window + symbol:**

```
Window: 2025-12-10
â”œâ”€ BTCUSDT â†’ Group 1
â””â”€ ETHUSDT â†’ Group 2

Window: 2025-12-11
â”œâ”€ BTCUSDT â†’ Group 3
â””â”€ ETHUSDT â†’ Group 4
```

**Aggregations:**

```python
.agg(
    first("open").alias("daily_open"),
    max("high").alias("daily_high"),
    min("low").alias("daily_low"),
    last("price").alias("daily_close"),
    sum("volume").alias("daily_volume"),
    sum("quote_volume").alias("daily_quote_volume"),
    sum("number_trades").alias("total_trades"),
    count("*").alias("tick_count"),
    avg("price").alias("avg_price")
)
```

**Giáº£i thÃ­ch tá»«ng aggregation:**

| Function               | Ã nghÄ©a                          | VÃ­ dá»¥               |
| ---------------------- | -------------------------------- | ------------------- |
| `first("open")`        | GiÃ¡ **open Ä‘áº§u tiÃªn** trong ngÃ y | 42000 (00:00:01)    |
| `max("high")`          | GiÃ¡ **cao nháº¥t** trong ngÃ y      | 43500 (14:23:45)    |
| `min("low")`           | GiÃ¡ **tháº¥p nháº¥t** trong ngÃ y     | 41800 (09:15:30)    |
| `last("price")`        | GiÃ¡ **cuá»‘i cÃ¹ng** (close)        | 42156.78 (23:59:59) |
| `sum("volume")`        | Tá»•ng volume 24h                  | 1234567.89 BTC      |
| `sum("quote_volume")`  | Tá»•ng quote volume                | 52000000000 USDT    |
| `sum("number_trades")` | Tá»•ng sá»‘ trades                   | 567890 trades       |
| `count("*")`           | Sá»‘ láº§n nháº­n data (ticks)         | 86400 (náº¿u 1s/tick) |
| `avg("price")`         | GiÃ¡ trung bÃ¬nh                   | 42250.33            |

**OHLC Pattern (Open-High-Low-Close):**

```
Day: 2025-12-10
Timeline:
00:00:01 - Open:  42000  â† first("open")
09:15:30 - Low:   41800  â† min("low")
14:23:45 - High:  43500  â† max("high")
23:59:59 - Close: 42156  â† last("price")

         43500 (High)
           â†‘
    42000  |â”€â”€â”
(Open) â”€â”€â”€â”€â”˜  â”‚
               â”‚
    41800  â”€â”€â”€â”€â”˜
    (Low)
               â†“
         42156 (Close)
```

**Output columns:**

```python
.select(
    col("window.start").alias("date"),
    col("symbol"),
    col("daily_open"),
    col("daily_high"),
    ...
)
```

`window.start`: Thá»i Ä‘iá»ƒm báº¯t Ä‘áº§u window (2025-12-10 00:00:00)

**Káº¿t quáº£ máº«u:**

```
date                | symbol   | daily_open | daily_high | daily_low | daily_close | daily_volume
2025-12-10 00:00:00 | BTCUSDT  | 42000.0    | 43500.0    | 41800.0   | 42156.78    | 1234567.89
2025-12-10 00:00:00 | ETHUSDT  | 2200.0     | 2250.0     | 2180.0    | 2235.67     | 45678.90
```

---

### **STEP 6: WRITE STREAMS (DÃ²ng 140-220)**

CÃ³ **3 streaming queries** cháº¡y song song:

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Query 1: Daily â†’ Parquet             â”‚
â”‚ Query 2: Raw â†’ Console (monitoring)  â”‚
â”‚ Query 3: Stats â†’ Memory (SQL)        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

#### **Query 1: Daily Aggregates â†’ Parquet**

```python
daily_query = dailyDF.writeStream \
    .outputMode("append") \
    .format("parquet") \
    .option("path", f"{OUTPUT_PATH}/daily") \
    .option("checkpointLocation", f"{CHECKPOINT_PATH}/daily") \
    .partitionBy("symbol") \
    .trigger(processingTime="10 seconds") \
    .start()
```

**Tá»«ng option:**

**1. `.outputMode("append")`**

3 modes:

- **`append`**: Chá»‰ ghi **data má»›i** (window Ä‘Ã£ Ä‘Ã³ng)
- `complete`: Ghi **toÃ n bá»™** káº¿t quáº£ má»—i batch
- `update`: Chá»‰ ghi rows **thay Ä‘á»•i**

```
append mode:
Batch 1: Window 2025-12-09 Ä‘Ã³ng â†’ ghi
Batch 2: Window 2025-12-10 Ä‘Ã³ng â†’ ghi
Batch 3: (chÆ°a cÃ³ window nÃ o Ä‘Ã³ng) â†’ khÃ´ng ghi gÃ¬
```

**2. `.format("parquet")`**

LÆ°u dáº¡ng **Parquet**:

- Columnar storage (query nhanh)
- Compression (tiáº¿t kiá»‡m disk)
- Schema evolution support

**3. `.option("path", "streaming_output_spark/daily")`**

ThÆ° má»¥c output:

```
streaming_output_spark/
â””â”€ daily/
   â”œâ”€ symbol=BTCUSDT/
   â”‚  â”œâ”€ part-00000.parquet
   â”‚  â””â”€ part-00001.parquet
   â””â”€ symbol=ETHUSDT/
      â”œâ”€ part-00000.parquet
      â””â”€ part-00001.parquet
```

**4. `.option("checkpointLocation", "checkpoint_spark/daily")`**

Checkpoint lÆ°u:

- Offset Ä‘Ã£ xá»­ lÃ½
- State cá»§a aggregations
- Metadata

```
checkpoint_spark/daily/
â”œâ”€ commits/
â”œâ”€ metadata
â”œâ”€ offsets/
â””â”€ state/
```

**5. `.partitionBy("symbol")`**

Partition theo symbol:

```
symbol=BTCUSDT/ â† BTC data
symbol=ETHUSDT/ â† ETH data
```

Lá»£i Ã­ch:

- Query 1 symbol â†’ chá»‰ scan 1 partition
- Parallel processing

**6. `.trigger(processingTime="10 seconds")`**

**Micro-batch processing:**

```
Timeline:
00:00 - Batch 1: Thu tháº­p data tá»« 23:50 â†’ 00:00 â†’ xá»­ lÃ½
00:10 - Batch 2: Thu tháº­p data tá»« 00:00 â†’ 00:10 â†’ xá»­ lÃ½
00:20 - Batch 3: Thu tháº­p data tá»« 00:10 â†’ 00:20 â†’ xá»­ lÃ½
00:30 - Batch 4: Thu tháº­p data tá»« 00:20 â†’ 00:30 â†’ xá»­ lÃ½
```

**7. `.start()`**

Báº¯t Ä‘áº§u streaming query (cháº¡y background).

---

#### **Query 2: Raw Stream â†’ Console (Monitoring)**

```python
console_query = streamDF \
    .select(
        col("symbol"),
        col("price"),
        col("volume"),
        col("price_change_percent"),
        col("event_timestamp")
    ) \
    .writeStream \
    .outputMode("append") \
    .format("console") \
    .option("truncate", "false") \
    .option("numRows", "10") \
    .trigger(processingTime="30 seconds") \
    .start()
```

**Má»¥c Ä‘Ã­ch:** MONITORING real-time

**1. `.select(...)` chá»‰ vÃ i columns quan trá»ng**

KhÃ´ng cáº§n hiá»ƒn thá»‹ táº¥t cáº£ 15 columns â†’ chá»‰ chá»n:

- `symbol`, `price`, `volume`, `price_change_percent`, `event_timestamp`

**2. `.format("console")`**

Hiá»ƒn thá»‹ lÃªn **terminal/console**.

**3. `.option("truncate", "false")`**

```
truncate=true:  BTCUSDT â†’ BTCUS...
truncate=false: BTCUSDT â†’ BTCUSDT (full text)
```

**4. `.option("numRows", "10")`**

Má»—i batch chá»‰ hiá»ƒn thá»‹ **10 rows Ä‘áº§u**.

**5. `.trigger(processingTime="30 seconds")`**

Hiá»ƒn thá»‹ má»—i **30 giÃ¢y** (khÃ´ng cáº§n quÃ¡ thÆ°á»ng xuyÃªn).

**Output máº«u:**

```
-------------------------------------------
Batch: 5
-------------------------------------------
+----------+---------+---------+--------------------+-------------------+
|symbol    |price    |volume   |price_change_percent|event_timestamp    |
+----------+---------+---------+--------------------+-------------------+
|BTCUSDT   |42156.78 |123.45   |1.23                |2025-12-10 12:00:05|
|ETHUSDT   |2235.67  |456.78   |-0.45               |2025-12-10 12:00:05|
|BTCUSDT   |42160.50 |234.56   |1.24                |2025-12-10 12:00:06|
|ETHUSDT   |2234.00  |567.89   |-0.52               |2025-12-10 12:00:06|
...
+----------+---------+---------+--------------------+-------------------+
```

---

#### **Query 3: Daily Stats â†’ Memory (SQL Queries)**

```python
stats_query = dailyDF.writeStream \
    .outputMode("complete") \
    .format("memory") \
    .queryName("crypto_daily_stats") \
    .trigger(processingTime="10 seconds") \
    .start()
```

**KhÃ¡c biá»‡t:**

**1. `.outputMode("complete")`**

```
complete mode:
Batch 1: Ghi toÃ n bá»™ káº¿t quáº£ (1 row)
Batch 2: Ghi toÃ n bá»™ káº¿t quáº£ (2 rows)
Batch 3: Ghi toÃ n bá»™ káº¿t quáº£ (3 rows)

â†’ Má»—i batch OVERWRITE toÃ n bá»™ table
```

**2. `.format("memory")`**

LÆ°u vÃ o **RAM** (in-memory table), khÃ´ng ghi disk.

**3. `.queryName("crypto_daily_stats")`**

TÃªn table: `crypto_daily_stats`

**CÃ¡ch dÃ¹ng:**

Trong terminal khÃ¡c (hoáº·c notebook):

```python
# Query báº±ng SQL
spark.sql("SELECT * FROM crypto_daily_stats").show()

# Filter by symbol
spark.sql("""
    SELECT symbol, daily_high, daily_low, daily_volume
    FROM crypto_daily_stats
    WHERE symbol='BTCUSDT'
""").show()

# Sort by volume
spark.sql("""
    SELECT * FROM crypto_daily_stats
    ORDER BY daily_volume DESC
""").show()
```

**Use cases:**

- Real-time dashboard
- API queries
- Monitoring alerts

---

### **PHáº¦N 8: MONITORING (DÃ²ng 262-282)**

```python
print("\n" + "=" * 80)
print("STREAMING QUERIES ACTIVE")
print("=" * 80)
print("\nActive queries:")
for query in spark.streams.active:
    print(f"  - {query.name if query.name else query.id}")
```

**`spark.streams.active`:** List táº¥t cáº£ streaming queries Ä‘ang cháº¡y.

**Output:**

```
================================================================================
STREAMING QUERIES ACTIVE
================================================================================

Active queries:
  - crypto_daily_stats
  - <query_id_1>
  - <query_id_2>
  - <query_id_3>
```

```python
print("\nğŸ“Š Monitor status:")
print("  - Console output will show every 30 seconds")
print("  - Parquet files updated every 10 seconds")
print("  - Check checkpoint/ for progress")
print("  - Check streaming_output_spark/ for results")
```

HÆ°á»›ng dáº«n user cÃ¡ch monitor.

```python
print("\nğŸ’¡ To query in-memory stats, open another terminal:")
print("  spark.sql('SELECT * FROM crypto_daily_stats').show()")
```

HÆ°á»›ng dáº«n query in-memory table.

---

### **PHáº¦N 9: GRACEFUL SHUTDOWN (DÃ²ng 283-308)**

```python
try:
    spark.streams.awaitAnyTermination()
except KeyboardInterrupt:
    print("\n\nâ¹ Stopping all streaming queries...")
```

**`awaitAnyTermination()`:**

- Chá» cho Ä‘áº¿n khi **báº¥t ká»³ query nÃ o** terminate
- Block main thread (giá»¯ program cháº¡y)

**`KeyboardInterrupt`:**

- User nháº¥n **Ctrl+C**
- Exception Ä‘Æ°á»£c catch

```python
    for query in spark.streams.active:
        print(f"  Stopping: {query.name if query.name else query.id}")
        query.stop()
```

**Graceful shutdown:**

- Loop qua **táº¥t cáº£ queries**
- Stop tá»«ng query má»™t
- Äáº£m báº£o checkpoint Ä‘Æ°á»£c lÆ°u

```
Timeline:
1. User nháº¥n Ctrl+C
2. Catch KeyboardInterrupt
3. Stop query 1 â†’ checkpoint saved
4. Stop query 2 â†’ checkpoint saved
5. Stop query 3 â†’ checkpoint saved
6. Stop query 4 â†’ checkpoint saved
7. Táº¥t cáº£ queries Ä‘Ã£ stop an toÃ n âœ…
```

**Táº¡i sao cáº§n graceful shutdown?**

```
Sai:
User nháº¥n Ctrl+C â†’ kill -9 process â†’ checkpoint KHÃ”NG LÆ¯U
Next run: báº¯t Ä‘áº§u láº¡i tá»« Ä‘áº§u hoáº·c bá»‹ lá»—i

ÄÃºng:
User nháº¥n Ctrl+C â†’ catch exception â†’ stop queries â†’ save checkpoint
Next run: tiáº¿p tá»¥c tá»« chá»— cÅ© âœ…
```

```python
    try:
        daily_stats = spark.sql("SELECT * FROM crypto_daily_stats")
        print("\nDaily aggregates:")
        daily_stats.show(10, truncate=False)
    except:
        print("No daily stats available yet")
```

Hiá»ƒn thá»‹ **final stats** trÆ°á»›c khi táº¯t:

- Náº¿u cÃ³ data â†’ show 10 rows
- Náº¿u chÆ°a cÃ³ data (vá»«a start) â†’ skip

```python
    spark.stop()
    print("\nâœ“ Spark session closed")
```

**`spark.stop()`:**

- ÄÃ³ng Spark session
- Giáº£i phÃ³ng resources (RAM, CPU, network)
- Cleanup temporary files

---

## ğŸ”„ LUá»’NG Xá»¬ LÃ HOÃ€N CHá»ˆNH

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 1. PRODUCER (websocket_producer.py)                        â”‚
â”‚    - Fetch Binance API má»—i 1 giÃ¢y                          â”‚
â”‚    - Gá»­i JSON vÃ o Kafka topic "crypto-prices"              â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 2. KAFKA TOPIC: "crypto-prices"                            â”‚
â”‚    - LÆ°u messages (buffer)                                 â”‚
â”‚    - Partitions, replication                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 3. SPARK CONSUMER (spark_streaming_consumer.py)            â”‚
â”‚                                                             â”‚
â”‚    Step 1: readStream tá»« Kafka                             â”‚
â”‚    Step 2: Parse JSON â†’ DataFrame                          â”‚
â”‚    Step 3: Transform timestamps                            â”‚
â”‚    Step 4: Watermark (late data tolerance)                 â”‚
â”‚    Step 5: Daily window aggregation                        â”‚
â”‚    Step 6: Write 3 streams:                                â”‚
â”‚            - Daily â†’ Parquet                               â”‚
â”‚            - Raw â†’ Console                                 â”‚
â”‚            - Stats â†’ Memory                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                           â†“
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ 4. OUTPUTS                                                  â”‚
â”‚                                                             â”‚
â”‚    streaming_output_spark/                                 â”‚
â”‚    â”œâ”€ daily/symbol=BTCUSDT/*.parquet                       â”‚
â”‚    â””â”€ daily/symbol=ETHUSDT/*.parquet                       â”‚
â”‚                                                             â”‚
â”‚    checkpoint_spark/                                        â”‚
â”‚    â””â”€ daily/ (progress metadata)                           â”‚
â”‚                                                             â”‚
â”‚    In-Memory Table: crypto_daily_stats                     â”‚
â”‚    Console: Real-time monitoring                           â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## âš¡ KEY CONCEPTS

### **1. Structured Streaming vs DStream**

| Aspect          | Structured Streaming | DStream (Old)   |
| --------------- | -------------------- | --------------- |
| API             | DataFrame/SQL        | RDD             |
| Abstraction     | High-level           | Low-level       |
| Optimization    | Catalyst optimizer   | Manual          |
| Late data       | Watermarking         | Manual handling |
| Fault tolerance | Checkpoint auto      | Manual          |

**â†’ Structured Streaming Ä‘Æ°á»£c khuyáº¿n nghá»‹!**

---

### **2. Micro-batch Processing**

```
Timeline:
â”œâ”€ [Batch 1] â”€ 10s â”€ [Batch 2] â”€ 10s â”€ [Batch 3] â”€ 10s â”€â†’
   â†“                  â†“                  â†“
   Process           Process           Process
   0-10s data        10-20s data       20-30s data
```

**KhÃ´ng pháº£i:**

- Batch processing (xá»­ lÃ½ 1 láº§n/ngÃ y)
- True streaming (xá»­ lÃ½ tá»«ng message)

**LÃ :** Micro-batch (xá»­ lÃ½ má»—i 10 giÃ¢y)

---

### **3. Output Modes**

| Mode       | Behavior                       | Use case             |
| ---------- | ------------------------------ | -------------------- |
| `append`   | Chá»‰ ghi rows má»›i (window Ä‘Ã³ng) | Parquet, logs        |
| `complete` | Ghi toÃ n bá»™ káº¿t quáº£ má»—i batch  | In-memory, dashboard |
| `update`   | Chá»‰ ghi rows thay Ä‘á»•i          | Database updates     |

---

### **4. Watermark Visualization**

```
Current time: 13:00
Watermark: 1 hour
Event time watermark: 13:00 - 1h = 12:00

Timeline:
11:00   11:30   12:00   12:30   13:00   13:30
  âŒ      âŒ      â”‚       âœ…      âœ…      âœ…
  DROP    DROP   â”‚     ACCEPT  ACCEPT  ACCEPT
                 â””â”€ Watermark boundary
```

---

### **5. Checkpoint Recovery**

```
Scenario 1: Normal run
â”œâ”€ Process batch 1 â†’ save checkpoint
â”œâ”€ Process batch 2 â†’ save checkpoint
â””â”€ Process batch 3 â†’ save checkpoint

Scenario 2: Crash at batch 2
â”œâ”€ Process batch 1 â†’ save checkpoint âœ…
â”œâ”€ Process batch 2 â†’ CRASH âŒ
â””â”€ Restart â†’ read checkpoint â†’ resume from batch 2 âœ…
```

---

## ğŸ› TROUBLESHOOTING

### **Lá»—i 1: Kafka connector not found**

```
Error: Failed to find data source: kafka
```

**Giáº£i phÃ¡p:**

```python
.config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.3")
```

Hoáº·c táº£i thá»§ cÃ´ng:

```bash
spark-submit --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.3 spark_streaming_consumer.py
```

---

### **Lá»—i 2: Connection refused to Kafka**

```
Error: Connection to localhost:9092 refused
```

**Kiá»ƒm tra:**

```bash
# Kafka cÃ³ cháº¡y khÃ´ng?
docker ps | grep kafka

# Start Kafka
cd week6_streaming
docker-compose up -d
```

---

### **Lá»—i 3: No data in output**

**NguyÃªn nhÃ¢n:**

- Producer chÆ°a cháº¡y
- Window chÆ°a Ä‘Ã³ng (chá» watermark)

**Kiá»ƒm tra:**

```bash
# Producer cÃ³ cháº¡y khÃ´ng?
python websocket_producer.py

# Console query cÃ³ hiá»‡n data khÃ´ng?
# Náº¿u cÃ³ â†’ chá» window Ä‘Ã³ng
# Náº¿u khÃ´ng â†’ producer lá»—i
```

---

### **Lá»—i 4: Out of memory**

```
Error: java.lang.OutOfMemoryError
```

**Giáº£i phÃ¡p:**

```python
.config("spark.driver.memory", "8g")  # TÄƒng tá»« 4g lÃªn 8g
.config("spark.executor.memory", "8g")
```

---

## ğŸ“Š MONITORING

### **1. Spark UI**

```
URL: http://localhost:4040
```

Tabs:

- **Jobs:** Xem batch processing
- **Stages:** Task execution details
- **Streaming:** Query statistics, processing times
- **SQL:** Query plans

---

### **2. Query Progress**

```python
# In another terminal
from pyspark.sql import SparkSession
spark = SparkSession.builder.getOrCreate()

# Get query by name
query = spark.streams.active[0]

# Last progress
query.lastProgress

# Status
query.status
```

---

### **3. Output Files**

```bash
# Check Parquet files
ls -lh streaming_output_spark/daily/symbol=BTCUSDT/

# Read with Spark
df = spark.read.parquet("streaming_output_spark/daily")
df.show()

# Count rows
df.count()
```

---

### **4. Checkpoint**

```bash
# Checkpoint structure
tree checkpoint_spark/daily/

checkpoint_spark/daily/
â”œâ”€ commits/
â”‚  â”œâ”€ 0
â”‚  â”œâ”€ 1
â”‚  â””â”€ 2
â”œâ”€ metadata
â”œâ”€ offsets/
â”‚  â”œâ”€ 0
â”‚  â””â”€ 1
â””â”€ state/
```

---

## ğŸ¯ TÃ“M Táº®T

**File:** `spark_streaming_consumer.py` - Speed Layer Consumer

**Chá»©c nÄƒng:**

1. Äá»c real-time data tá»« Kafka topic `crypto-prices`
2. Parse JSON â†’ DataFrame
3. Transform timestamps, extract date/hour
4. Watermarking (late data tolerance: 1 hour)
5. Window aggregation (Daily)
6. Ghi 3 streams: Parquet (daily), Console, Memory

**Key concepts:**

- Structured Streaming (high-level API)
- Micro-batch processing (10s trigger)
- Watermark (handle late data)
- Checkpoint (fault tolerance)
- Output modes (append, complete)

**Integration:**

```
Producer â†’ Kafka â†’ Consumer (file nÃ y) â†’ Parquet/Memory
                                        â†“
                            Serving Layer (week6_merge.py)
```

**Next:** TÃ¬m hiá»ƒu `week6_merge.py` - Serving Layer (merge Batch + Speed)
