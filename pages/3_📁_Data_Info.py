"""
================================================================================
TRANG DATA INFO - Thá»‘ng kÃª vÃ  ThÃ´ng tin Dataset
================================================================================
"""

import streamlit as st
import pandas as pd
import os
from pyspark.sql import SparkSession
from pyspark.sql.functions import min, max, count

st.set_page_config(page_title="Data Info", page_icon="ğŸ“", layout="wide")

st.title("ğŸ“ ThÃ´ng tin Dataset")
st.markdown("---")

# Initialize Spark (with error handling)
@st.cache_resource
def get_spark():
    try:
        spark = SparkSession.builder \
            .appName("StreamlitDashboard") \
            .config("spark.driver.memory", "2g") \
            .getOrCreate()
        spark.sparkContext.setLogLevel("ERROR")
        return spark
    except Exception as e:
        st.error(f"KhÃ´ng thá»ƒ khá»Ÿi táº¡o Spark: {str(e)}")
        return None

spark = get_spark()

if spark is None:
    st.error("âŒ Spark session khÃ´ng kháº£ dá»¥ng. KhÃ´ng thá»ƒ táº£i thÃ´ng tin dá»¯ liá»‡u.")
    st.stop()

# Paths
daily_filled_path = "data_analysis/daily_filled"
daily_raw_path = "data_analysis/daily_raw"
prophet_input_path = "data_analysis/prophet_input"

# Tabs
tab1, tab2, tab3 = st.tabs(["ğŸ“Š Daily Filled", "ğŸ“ˆ Daily Raw", "ğŸ”® Prophet Input"])

# Tab 1: Daily Filled
with tab1:
    st.markdown("### ğŸ“Š Dataset Daily Filled")
    st.markdown("**Dataset hoÃ n chá»‰nh vá»›i MA7/MA30, dÃ¹ng Ä‘á»ƒ train Prophet**")
    
    if os.path.exists(daily_filled_path):
        try:
            with st.spinner("Äang táº£i dá»¯ liá»‡u..."):
                df = spark.read.parquet(daily_filled_path)
                
                # Statistics
                stats = df.groupBy("symbol").agg(
                    min("date").alias("first_date"),
                    max("date").alias("last_date"),
                    count("*").alias("rows")
                ).toPandas()
                
                st.success("âœ… Dá»¯ liá»‡u Ä‘Ã£ táº£i thÃ nh cÃ´ng!")
                
                # Display stats
                col1, col2, col3 = st.columns(3)
                
                with col1:
                    total_rows = stats['rows'].sum()
                    st.metric("Tá»•ng sá»‘ Rows", f"{total_rows:,}")
                
                with col2:
                    symbols_count = len(stats)
                    st.metric("Sá»‘ Symbols", symbols_count)
                
                with col3:
                    # Date range
                    min_date = stats['first_date'].min()
                    max_date = stats['last_date'].max()
                    st.metric("Khoáº£ng thá»i gian", f"{min_date} â†’ {max_date}")
                
                st.markdown("---")
                
                # Stats table
                st.dataframe(
                    stats,
                    use_container_width=True,
                    hide_index=True,
                    column_config={
                        'symbol': 'Symbol',
                        'first_date': 'NgÃ y Ä‘áº§u',
                        'last_date': 'NgÃ y cuá»‘i',
                        'rows': st.column_config.NumberColumn('Sá»‘ Rows', format="%d")
                    }
                )
                
                st.markdown("---")
                
                # Schema
                st.markdown("### ğŸ“‹ Schema")
                schema_df = pd.DataFrame([
                    (field.name, str(field.dataType), field.nullable)
                    for field in df.schema.fields
                ], columns=['TÃªn cá»™t', 'Kiá»ƒu dá»¯ liá»‡u', 'Cho phÃ©p NULL'])
                
                st.dataframe(schema_df, use_container_width=True, hide_index=True)
                
                # Sample data
                st.markdown("### ğŸ” Dá»¯ liá»‡u Máº«u")
                sample = df.limit(10).toPandas()
                st.dataframe(sample, use_container_width=True, hide_index=True)
                
        except Exception as e:
            st.error(f"âŒ Lá»—i khi táº£i daily_filled: {str(e)}")
    else:
        st.warning(f"âš ï¸ KhÃ´ng tÃ¬m tháº¥y: {daily_filled_path}")

# Tab 2: Daily Raw
with tab2:
    st.markdown("### ğŸ“ˆ Dataset Daily Raw")
    st.markdown("**Dá»¯ liá»‡u OHLC hÃ ng ngÃ y Ä‘Ã£ aggregate (trÆ°á»›c khi forward fill vÃ  tÃ­nh MA)**")
    
    if os.path.exists(daily_raw_path):
        try:
            with st.spinner("Äang táº£i dá»¯ liá»‡u..."):
                df = spark.read.parquet(daily_raw_path)
                
                # Statistics
                stats = df.groupBy("symbol").agg(
                    min("date").alias("first_date"),
                    max("date").alias("last_date"),
                    count("*").alias("rows")
                ).toPandas()
                
                st.success("âœ… Dá»¯ liá»‡u Ä‘Ã£ táº£i thÃ nh cÃ´ng!")
                
                # Display stats
                col1, col2 = st.columns(2)
                
                with col1:
                    st.dataframe(
                        stats,
                        use_container_width=True,
                        hide_index=True,
                        column_config={
                            'symbol': 'Symbol',
                            'first_date': 'NgÃ y Ä‘áº§u',
                            'last_date': 'NgÃ y cuá»‘i',
                            'rows': st.column_config.NumberColumn('Sá»‘ Rows', format="%d")
                        }
                    )
                
                with col2:
                    st.info("""
                    **daily_raw vs daily_filled:**
                    - `daily_raw`: Dá»¯ liá»‡u aggregate gá»‘c (cÃ³ thá»ƒ cÃ³ gaps)
                    - `daily_filled`: ÄÃ£ forward-fill vÃ  tÃ­nh MA7/MA30
                    - Sá»‘ rows tÆ°Æ¡ng tá»± (gaps Ä‘Ã£ Ä‘Æ°á»£c Ä‘iá»n)
                    """)
                
                st.markdown("---")
                
                # Schema
                st.markdown("### ğŸ“‹ Schema")
                schema_df = pd.DataFrame([
                    (field.name, str(field.dataType))
                    for field in df.schema.fields
                ], columns=['TÃªn cá»™t', 'Kiá»ƒu dá»¯ liá»‡u'])
                
                st.dataframe(schema_df, use_container_width=True, hide_index=True)
                
        except Exception as e:
            st.error(f"âŒ Lá»—i khi táº£i daily_raw: {str(e)}")
    else:
        st.warning(f"âš ï¸ KhÃ´ng tÃ¬m tháº¥y: {daily_raw_path}")

# Tab 3: Prophet Input
with tab3:
    st.markdown("### ğŸ”® Dataset Prophet Input")
    st.markdown("**Schema tá»‘i giáº£n cho Prophet training (ds, y, symbol)**")
    
    if os.path.exists(prophet_input_path):
        try:
            with st.spinner("Äang táº£i dá»¯ liá»‡u..."):
                df = spark.read.parquet(prophet_input_path)
                
                # Statistics
                stats = df.groupBy("symbol").agg(
                    min("ds").alias("first_date"),
                    max("ds").alias("last_date"),
                    count("*").alias("rows")
                ).toPandas()
                
                st.success("âœ… Dá»¯ liá»‡u Ä‘Ã£ táº£i thÃ nh cÃ´ng!")
                
                st.dataframe(
                    stats,
                    use_container_width=True,
                    hide_index=True,
                    column_config={
                        'symbol': 'Symbol',
                        'first_date': 'NgÃ y Ä‘áº§u',
                        'last_date': 'NgÃ y cuá»‘i',
                        'rows': st.column_config.NumberColumn('Sá»‘ Rows', format="%d")
                    }
                )
                
                st.markdown("---")
                
                st.info("""
                **Prophet Schema:**
                - `ds`: NgÃ y (theo quy Æ°á»›c cá»§a Prophet)
                - `y`: Biáº¿n má»¥c tiÃªu (giÃ¡ daily_close)
                - `symbol`: Partition key
                
                **LÆ°u Ã½:** MA7/MA30 Ä‘Æ°á»£c thÃªm lÃ m regressors trong quÃ¡ trÃ¬nh training (join tá»« daily_filled)
                """)
                
                # Sample data
                st.markdown("### ğŸ” Dá»¯ liá»‡u Máº«u")
                sample = df.limit(10).toPandas()
                st.dataframe(sample, use_container_width=True, hide_index=True)
                
        except Exception as e:
            st.error(f"âŒ Lá»—i khi táº£i prophet_input: {str(e)}")
    else:
        st.warning(f"âš ï¸ KhÃ´ng tÃ¬m tháº¥y: {prophet_input_path}")

# Footer
st.markdown("---")
st.markdown("""
**Data Pipeline:**
1. **CSV** (Kaggle) â†’ `convert_to_parquet.py` â†’ **Parquet** (11.5M rows)
2. **Parquet** â†’ `preprocess_step1.py` â†’ **daily_raw** (~8,000 rows)
3. **daily_raw** â†’ `preprocess_step2.py` â†’ **daily_filled** (+ MA7/MA30)
4. **daily_filled** â†’ extract â†’ **prophet_input** (schema tá»‘i giáº£n)
5. **prophet_input** â†’ `prophet_train.py` â†’ **Forecasts**

ğŸ“Œ **Data Snapshot:** 01/01/2012 â†’ 14/12/2025 (BTC + ETH)
""")
