# ğŸ“˜ WEEK 6 - PHáº¦N 2: BATCH LAYER (week6_backfill.py)

## ğŸ“‘ Má»¥c lá»¥c
1. [Má»¥c Ä‘Ã­ch cá»§a Batch Layer](#1-má»¥c-Ä‘Ã­ch-cá»§a-batch-layer)
2. [Tá»•ng quan file week6_backfill.py](#2-tá»•ng-quan-file-week6_backfillpy)
3. [Giáº£i thÃ­ch chi tiáº¿t tá»«ng pháº§n code](#3-giáº£i-thÃ­ch-chi-tiáº¿t-tá»«ng-pháº§n-code)
4. [Binance API - CÃ¡ch hoáº¡t Ä‘á»™ng](#4-binance-api---cÃ¡ch-hoáº¡t-Ä‘á»™ng)
5. [Xá»­ lÃ½ dá»¯ liá»‡u vá»›i PySpark](#5-xá»­-lÃ½-dá»¯-liá»‡u-vá»›i-pyspark)
6. [Forward Fill - Äiá»n dá»¯ liá»‡u thiáº¿u](#6-forward-fill---Ä‘iá»n-dá»¯-liá»‡u-thiáº¿u)
7. [Moving Average (MA7, MA30)](#7-moving-average-ma7-ma30)
8. [Output vÃ  cáº¥u trÃºc dá»¯ liá»‡u](#8-output-vÃ -cáº¥u-trÃºc-dá»¯-liá»‡u)
9. [CÃ¢u há»i thÆ°á»ng gáº·p](#9-cÃ¢u-há»i-thÆ°á»ng-gáº·p)

---

## 1. Má»¥c Ä‘Ã­ch cá»§a Batch Layer

### 1.1. Váº¥n Ä‘á» cáº§n giáº£i quyáº¿t

Trong Lambda Architecture, Batch Layer cÃ³ nhiá»‡m vá»¥:
- Xá»­ lÃ½ **dá»¯ liá»‡u lá»‹ch sá»­** (historical data)
- **Backfill** (láº¥p Ä‘áº§y) nhá»¯ng ngÃ y bá»‹ thiáº¿u
- Äáº£m báº£o **tÃ­nh chÃ­nh xÃ¡c** cá»§a dá»¯ liá»‡u

### 1.2. TÃ¬nh huá»‘ng thá»±c táº¿

```
VÃ­ dá»¥:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                     â”‚
â”‚  NgÃ y 24/11: Báº¡n cháº¡y project, dá»¯ liá»‡u Ä‘áº¿n 24/11                   â”‚
â”‚                                                                     â”‚
â”‚  NgÃ y 25/11 - 02/12: MÃ¡y tÃ­nh táº¯t, khÃ´ng cháº¡y streaming           â”‚
â”‚                                                                     â”‚
â”‚  NgÃ y 03/12: Báº¡n má»Ÿ láº¡i project                                    â”‚
â”‚              â†’ GAP: 9 ngÃ y thiáº¿u dá»¯ liá»‡u (25/11 - 03/12)           â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.3. Giáº£i phÃ¡p cá»§a Batch Layer

```
week6_backfill.py sáº½:

1. DETECT: PhÃ¡t hiá»‡n ngÃ y cuá»‘i cÃ¹ng cÃ³ dá»¯ liá»‡u (24/11)
2. CALCULATE: TÃ­nh sá»‘ ngÃ y gap (9 ngÃ y)
3. FETCH: Gá»i Binance API láº¥y dá»¯ liá»‡u 25/11 - 03/12
4. PROCESS: Clean, aggregate, compute MA
5. SAVE: LÆ°u vÃ o Parquet Ä‘á»ƒ dÃ¹ng cho forecast
```

---

## 2. Tá»•ng quan file week6_backfill.py

### 2.1. ThÃ´ng tin file

| Thuá»™c tÃ­nh | GiÃ¡ trá»‹ |
|------------|---------|
| TÃªn file | `week6_backfill.py` |
| Vá»‹ trÃ­ | `D:\BigDataProject\week6_backfill.py` |
| Sá»‘ dÃ²ng | ~335 dÃ²ng |
| NgÃ´n ngá»¯ | Python 3.10 |
| Framework | PySpark 3.5.3 |
| API | Binance API v3 |

### 2.2. Cáº¥u trÃºc file

```python
# ============================================================
# week6_backfill.py - Cáº¥u trÃºc tá»•ng quan
# ============================================================

# PHáº¦N 1: Import thÆ° viá»‡n (dÃ²ng 1-35)
from pyspark.sql import SparkSession
from pyspark.sql import functions as F
import requests
import pandas as pd
...

# PHáº¦N 2: Khá»Ÿi táº¡o Spark (dÃ²ng 37-47)
spark = SparkSession.builder...

# PHáº¦N 3: STEP 1 - Detect last date (dÃ²ng 49-95)
# Äá»c dá»¯ liá»‡u hiá»‡n cÃ³, tÃ¬m ngÃ y cuá»‘i

# PHáº¦N 4: STEP 2 - Fetch from Binance (dÃ²ng 97-170)
# Gá»i API láº¥y dá»¯ liá»‡u má»›i

# PHáº¦N 5: STEP 3 - Clean data (dÃ²ng 172-185)
# Loáº¡i bá» duplicate

# PHáº¦N 6: STEP 4 - Daily aggregation (dÃ²ng 187-210)
# Tá»•ng há»£p tá»« 1-minute thÃ nh daily

# PHáº¦N 7: STEP 5 - Forward fill (dÃ²ng 212-260)
# Äiá»n cÃ¡c ngÃ y thiáº¿u

# PHáº¦N 8: STEP 6 - Compute MA (dÃ²ng 262-280)
# TÃ­nh Moving Average

# PHáº¦N 9: STEP 7-8 - Save output (dÃ²ng 282-320)
# LÆ°u káº¿t quáº£

# PHáº¦N 10: Summary (dÃ²ng 322-335)
# Hiá»ƒn thá»‹ tá»•ng káº¿t
```

### 2.3. Luá»“ng xá»­ lÃ½ (Pipeline)

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    BATCH LAYER PIPELINE                             â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                     â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚   â”‚ STEP 1  â”‚    â”‚ STEP 2  â”‚    â”‚ STEP 3  â”‚    â”‚ STEP 4  â”‚         â”‚
â”‚   â”‚ Detect  â”‚â”€â”€â”€â–ºâ”‚ Fetch   â”‚â”€â”€â”€â–ºâ”‚ Clean   â”‚â”€â”€â”€â–ºâ”‚ Daily   â”‚         â”‚
â”‚   â”‚ Last    â”‚    â”‚ Binance â”‚    â”‚ Data    â”‚    â”‚ OHLC    â”‚         â”‚
â”‚   â”‚ Date    â”‚    â”‚ API     â”‚    â”‚         â”‚    â”‚         â”‚         â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚                                                      â”‚              â”‚
â”‚                                                      â–¼              â”‚
â”‚   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”         â”‚
â”‚   â”‚ STEP 8  â”‚    â”‚ STEP 7  â”‚    â”‚ STEP 6  â”‚    â”‚ STEP 5  â”‚         â”‚
â”‚   â”‚ Update  â”‚â—„â”€â”€â”€â”‚ Save    â”‚â—„â”€â”€â”€â”‚ Compute â”‚â—„â”€â”€â”€â”‚ Forward â”‚         â”‚
â”‚   â”‚ Prophet â”‚    â”‚ Parquet â”‚    â”‚ MA7/30  â”‚    â”‚ Fill    â”‚         â”‚
â”‚   â”‚ Input   â”‚    â”‚         â”‚    â”‚         â”‚    â”‚         â”‚         â”‚
â”‚   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜         â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 3. Giáº£i thÃ­ch chi tiáº¿t tá»«ng pháº§n code

### 3.1. Import thÆ° viá»‡n

```python
# ==================================================================
# PHáº¦N 1: IMPORT THÆ¯ VIá»†N
# ==================================================================

# PySpark - Framework xá»­ lÃ½ Big Data
from pyspark.sql import SparkSession          # Táº¡o session lÃ m viá»‡c vá»›i Spark
from pyspark.sql import functions as F        # CÃ¡c hÃ m xá»­ lÃ½ dá»¯ liá»‡u

# CÃ¡c hÃ m cá»¥ thá»ƒ cá»§a PySpark
from pyspark.sql.functions import (
    col,            # Truy cáº­p cá»™t: col("price")
    from_unixtime,  # Convert timestamp unix -> datetime
    to_date,        # Convert datetime -> date only
    year,           # Láº¥y nÄƒm tá»« date
    month,          # Láº¥y thÃ¡ng tá»« date
    dayofmonth,     # Láº¥y ngÃ y tá»« date
    first,          # Láº¥y giÃ¡ trá»‹ Ä‘áº§u tiÃªn (cho Open price)
    last,           # Láº¥y giÃ¡ trá»‹ cuá»‘i cÃ¹ng (cho Close price)
    max,            # GiÃ¡ trá»‹ lá»›n nháº¥t (cho High price)
    min,            # GiÃ¡ trá»‹ nhá» nháº¥t (cho Low price)
    sum,            # Tá»•ng (cho Volume)
    count,          # Äáº¿m sá»‘ lÆ°á»£ng
    avg,            # Trung bÃ¬nh (cho Moving Average)
    when,           # Äiá»u kiá»‡n if-else
    lit,            # Táº¡o cá»™t constant: lit("BTCUSDT")
    expr,           # Viáº¿t SQL expression
    sequence,       # Táº¡o dÃ£y sá»‘/ngÃ y
    explode,        # Má»Ÿ rá»™ng array thÃ nh nhiá»u rows
    lag,            # Láº¥y giÃ¡ trá»‹ row trÆ°á»›c
    coalesce        # Láº¥y giÃ¡ trá»‹ khÃ´ng null Ä‘áº§u tiÃªn
)

from pyspark.sql.window import Window  # Window functions (cho MA, forward fill)
from pyspark.sql.types import LongType, DoubleType  # Kiá»ƒu dá»¯ liá»‡u

# Python standard libraries
import requests      # Gá»i HTTP API
import pandas as pd  # Xá»­ lÃ½ DataFrame (chuyá»ƒn Ä‘á»•i)
from datetime import datetime, timedelta  # Xá»­ lÃ½ ngÃ y thÃ¡ng
import time          # Sleep (rate limiting)
import os            # Thao tÃ¡c file system
```

**Giáº£i thÃ­ch:**

| ThÆ° viá»‡n | Má»¥c Ä‘Ã­ch |
|----------|----------|
| `pyspark` | Xá»­ lÃ½ dá»¯ liá»‡u lá»›n, distributed computing |
| `requests` | Gá»i REST API (Binance) |
| `pandas` | Chuyá»ƒn Ä‘á»•i dá»¯ liá»‡u trung gian |
| `datetime` | Xá»­ lÃ½ ngÃ y thÃ¡ng |

### 3.2. Khá»Ÿi táº¡o Spark Session

```python
# ==================================================================
# PHáº¦N 2: KHá»I Táº O SPARK SESSION
# ==================================================================

spark = SparkSession.builder \
    .appName("Week6_Backfill_BatchLayer") \    # TÃªn á»©ng dá»¥ng (hiá»ƒn thá»‹ trong Spark UI)
    .config("spark.sql.adaptive.enabled", "true") \  # Báº­t Adaptive Query Execution
    .getOrCreate()  # Táº¡o session má»›i hoáº·c láº¥y session cÃ³ sáºµn
```

**Giáº£i thÃ­ch chi tiáº¿t:**

```
SparkSession lÃ  gÃ¬?
â”œâ”€â”€ Entry point Ä‘á»ƒ lÃ m viá»‡c vá»›i Spark
â”œâ”€â”€ Quáº£n lÃ½ context, configuration
â”œâ”€â”€ Cung cáº¥p API Ä‘á»ƒ Ä‘á»c/ghi dá»¯ liá»‡u
â””â”€â”€ Cho phÃ©p cháº¡y SQL queries

appName:
â”œâ”€â”€ TÃªn hiá»ƒn thá»‹ trong Spark Web UI (localhost:4040)
â””â”€â”€ GiÃºp identify á»©ng dá»¥ng khi cÃ³ nhiá»u jobs

spark.sql.adaptive.enabled:
â”œâ”€â”€ Adaptive Query Execution (AQE)
â”œâ”€â”€ Tá»± Ä‘á»™ng tá»‘i Æ°u query plan
â”œâ”€â”€ Äiá»u chá»‰nh sá»‘ partitions
â””â”€â”€ Xá»­ lÃ½ data skew
```

### 3.3. STEP 1: Detect Last Date

```python
# ==================================================================
# STEP 1: DETECT LAST DATE IN EXISTING DATA
# ==================================================================
print("\n[STEP 1] Detecting last date in existing data...")

try:
    # Thá»­ Ä‘á»c tá»« daily_filled trÆ°á»›c (náº¿u Ä‘Ã£ cháº¡y backfill trÆ°á»›c Ä‘Ã³)
    df_existing = spark.read.parquet("data_analysis/daily_filled")
    
    # Láº¥y ngÃ y lá»›n nháº¥t trong dá»¯ liá»‡u
    # agg(max("date")): Aggregate function láº¥y max cá»§a cá»™t "date"
    # collect()[0][0]: Láº¥y giÃ¡ trá»‹ tá»« DataFrame vá» Python
    last_date_existing = df_existing.agg(max("date")).collect()[0][0]
    data_source = "daily_filled"
    
except:
    try:
        # Náº¿u khÃ´ng cÃ³ daily_filled, Ä‘á»c tá»« prophet_input (Week 4)
        df_existing = spark.read.parquet("data_analysis/prophet_input")
        last_date_existing = df_existing.agg(max("ds")).collect()[0][0]
        data_source = "prophet_input"
        
    except:
        # KhÃ´ng cÃ³ dá»¯ liá»‡u nÃ o â†’ yÃªu cáº§u cháº¡y Week 1-5 trÆ°á»›c
        print("  âš ï¸  No existing data found!")
        print("  Please run Week 1-5 pipeline first:")
        print("    python convert_to_parquet.py")
        print("    python clean_parquet.py")
        print("    python preprocess_step1.py")
        print("    python preprocess_step2.py")
        spark.stop()
        exit(1)

print(f"  âœ… Last date found in {data_source}: {last_date_existing}")
```

**Giáº£i thÃ­ch logic:**

```
Táº¡i sao thá»­ Ä‘á»c nhiá»u nguá»“n?

1. daily_filled (Æ°u tiÃªn cao):
   - ÄÃ¢y lÃ  output cá»§a Batch Layer
   - Náº¿u Ä‘Ã£ cháº¡y week6_backfill.py trÆ°á»›c Ä‘Ã³, data á»Ÿ Ä‘Ã¢y má»›i nháº¥t
   
2. prophet_input (backup):
   - ÄÃ¢y lÃ  output cá»§a Week 4
   - Náº¿u láº§n Ä‘áº§u cháº¡y Week 6, chá»‰ cÃ³ data nÃ y

3. KhÃ´ng cÃ³ data:
   - ChÆ°a cháº¡y pipeline Week 1-5
   - Cáº§n cháº¡y pipeline trÆ°á»›c

Luá»“ng xá»­ lÃ½:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                     â”‚
â”‚   Try daily_filled â”€â”€â–º Success? â”€â”€â–º Use it                         â”‚
â”‚         â”‚                                                           â”‚
â”‚         â”‚ Fail                                                      â”‚
â”‚         â–¼                                                           â”‚
â”‚   Try prophet_input â”€â”€â–º Success? â”€â”€â–º Use it                        â”‚
â”‚         â”‚                                                           â”‚
â”‚         â”‚ Fail                                                      â”‚
â”‚         â–¼                                                           â”‚
â”‚   Exit with error message                                          â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.4. TÃ­nh Gap Days

```python
# TÃ­nh sá»‘ ngÃ y gap
today = datetime.now().date()  # NgÃ y hÃ´m nay
gap_days = (today - last_date_existing).days  # Sá»‘ ngÃ y chÃªnh lá»‡ch

print(f"  ğŸ“… Today: {today}")
print(f"  ğŸ“Š Gap: {gap_days} days")

# Náº¿u khÃ´ng cÃ³ gap, khÃ´ng cáº§n backfill
if gap_days <= 0:
    print("\nâœ… Data is already up to date! No backfill needed.")
    print("  You can start streaming for real-time data:")
    print("    cd week6_streaming")
    print("    docker-compose up -d")
    print("    python websocket_producer.py")
    print("    python spark_streaming_consumer.py")
    spark.stop()
    exit(0)

# NgÃ y báº¯t Ä‘áº§u fetch = ngÃ y sau ngÃ y cuá»‘i
fetch_start_date = last_date_existing + timedelta(days=1)
print(f"\n  ğŸ¯ Will backfill: {fetch_start_date} â†’ {today} ({gap_days} days)")
```

**VÃ­ dá»¥ minh há»a:**

```
VÃ­ dá»¥ 1: CÃ³ gap
  last_date_existing = 2025-11-24
  today             = 2025-12-03
  gap_days          = 9
  fetch_start_date  = 2025-11-25
  â†’ Backfill: 25/11 â†’ 03/12

VÃ­ dá»¥ 2: KhÃ´ng cÃ³ gap (cháº¡y cÃ¹ng ngÃ y)
  last_date_existing = 2025-12-03
  today             = 2025-12-03
  gap_days          = 0
  â†’ KhÃ´ng cáº§n backfill, cháº¡y streaming

VÃ­ dá»¥ 3: Data tÆ°Æ¡ng lai (?)
  last_date_existing = 2025-12-05
  today             = 2025-12-03
  gap_days          = -2
  â†’ KhÃ´ng cáº§n backfill (trÆ°á»ng há»£p hiáº¿m)
```

---

## 4. Binance API - CÃ¡ch hoáº¡t Ä‘á»™ng

### 4.1. Binance Klines API

```python
# ==================================================================
# STEP 2: FETCH DATA FROM BINANCE API
# ==================================================================

def fetch_binance_klines(symbol, interval, start_time, end_time):
    """
    Fetch historical klines (candlestick data) from Binance API.
    
    Parameters:
    -----------
    symbol : str
        Trading pair, e.g., "BTCUSDT", "ETHUSDT"
    interval : str
        Candlestick interval: "1m", "5m", "1h", "1d"
    start_time : int
        Start time in milliseconds
    end_time : int
        End time in milliseconds
    
    Returns:
    --------
    list : List of klines (candlesticks)
    """
    
    url = "https://api.binance.com/api/v3/klines"
    all_klines = []
    current_start = start_time
    
    while current_start < end_time:
        # Táº¡o parameters cho request
        params = {
            "symbol": symbol,        # VD: "BTCUSDT"
            "interval": interval,    # VD: "1m" (1 phÃºt)
            "startTime": current_start,  # Thá»i gian báº¯t Ä‘áº§u (ms)
            "endTime": end_time,         # Thá»i gian káº¿t thÃºc (ms)
            "limit": 1000            # Max 1000 records per request
        }
        
        try:
            # Gá»i API
            response = requests.get(url, params=params, timeout=30)
            response.raise_for_status()  # Raise exception náº¿u lá»—i HTTP
            klines = response.json()
            
            if not klines:
                break  # KhÃ´ng cÃ²n data
            
            all_klines.extend(klines)  # ThÃªm vÃ o list káº¿t quáº£
            
            # Cáº­p nháº­t start time cho láº§n gá»i tiáº¿p theo
            # close_time cá»§a kline cuá»‘i + 1ms
            current_start = klines[-1][6] + 1
            
            time.sleep(0.1)  # Rate limiting: trÃ¡nh bá»‹ block
            
        except Exception as e:
            print(f"    Retry after error: {e}")
            time.sleep(5)  # Chá» 5s rá»“i thá»­ láº¡i
            continue
    
    return all_klines
```

### 4.2. Cáº¥u trÃºc dá»¯ liá»‡u Klines

```
Binance API tráº£ vá» má»—i kline lÃ  má»™t array:

[
  1499040000000,      // [0]  Open time (timestamp ms)
  "0.01634000",       // [1]  Open price
  "0.80000000",       // [2]  High price
  "0.01575800",       // [3]  Low price
  "0.01577100",       // [4]  Close price
  "148976.11427815",  // [5]  Volume
  1499644799999,      // [6]  Close time (timestamp ms)
  "2434.19055334",    // [7]  Quote asset volume
  308,                // [8]  Number of trades
  "1756.87402397",    // [9]  Taker buy base asset volume
  "28.46694368",      // [10] Taker buy quote asset volume
  "0"                 // [11] Ignore
]

OHLC lÃ  gÃ¬?
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                                                                     â”‚
â”‚    O = Open   : GiÃ¡ má»Ÿ cá»­a (Ä‘áº§u khoáº£ng thá»i gian)                  â”‚
â”‚    H = High   : GiÃ¡ cao nháº¥t trong khoáº£ng thá»i gian                â”‚
â”‚    L = Low    : GiÃ¡ tháº¥p nháº¥t trong khoáº£ng thá»i gian               â”‚
â”‚    C = Close  : GiÃ¡ Ä‘Ã³ng cá»­a (cuá»‘i khoáº£ng thá»i gian)               â”‚
â”‚                                                                     â”‚
â”‚    VÃ­ dá»¥ 1 náº¿n 1-minute:                                           â”‚
â”‚                                                                     â”‚
â”‚         â”‚                                                           â”‚
â”‚     â”€â”€â”€â”€â”¼â”€â”€â”€â”€ High: $93,000                                        â”‚
â”‚         â”‚                                                           â”‚
â”‚      â”Œâ”€â”€â”´â”€â”€â”                                                        â”‚
â”‚      â”‚     â”‚  Close: $92,800                                       â”‚
â”‚      â”‚     â”‚                                                        â”‚
â”‚      â”‚     â”‚  Body (thÃ¢n náº¿n)                                      â”‚
â”‚      â”‚     â”‚                                                        â”‚
â”‚      â”‚     â”‚  Open: $92,500                                        â”‚
â”‚      â””â”€â”€â”¬â”€â”€â”˜                                                        â”‚
â”‚         â”‚                                                           â”‚
â”‚     â”€â”€â”€â”€â”¼â”€â”€â”€â”€ Low: $92,200                                         â”‚
â”‚         â”‚                                                           â”‚
â”‚                                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 4.3. Rate Limiting vÃ  Pagination

```python
# Rate Limiting: TrÃ¡nh bá»‹ Binance block
time.sleep(0.1)  # Nghá»‰ 100ms giá»¯a cÃ¡c request

# Táº¡i sao cáº§n?
# - Binance giá»›i háº¡n 1200 requests/phÃºt
# - Náº¿u gá»i quÃ¡ nhanh sáº½ bá»‹ block IP
# - 0.1s delay = max 600 requests/phÃºt (an toÃ n)

# Pagination: Láº¥y nhiá»u data hÆ¡n limit
while current_start < end_time:
    # Má»—i request láº¥y max 1000 klines
    # Cáº§n loop Ä‘á»ƒ láº¥y háº¿t
    
    # VÃ­ dá»¥: 9 ngÃ y Ã— 24 giá» Ã— 60 phÃºt = 12,960 klines
    # Cáº§n 13 requests (12,960 / 1000)
    
    current_start = klines[-1][6] + 1  # Tiáº¿p tá»¥c tá»« Ä‘iá»ƒm cuá»‘i
```

### 4.4. Xá»­ lÃ½ káº¿t quáº£ tá»« Binance

```python
# Convert timestamp sang datetime
start_ms = int(datetime.combine(fetch_start_date, datetime.min.time()).timestamp() * 1000)
end_ms = int(datetime.combine(today, datetime.max.time()).timestamp() * 1000)

# Giáº£i thÃ­ch:
# datetime.combine(date, time): Káº¿t há»£p date vÃ  time
# datetime.min.time() = 00:00:00.000000
# datetime.max.time() = 23:59:59.999999
# .timestamp() â†’ Unix timestamp (seconds)
# Ã— 1000 â†’ milliseconds (Binance yÃªu cáº§u)

# Fetch data cho cáº£ BTC vÃ  ETH
for symbol in ["BTCUSDT", "ETHUSDT"]:
    print(f"\n  Fetching {symbol}...")
    
    klines = fetch_binance_klines(symbol, "1m", start_ms, end_ms)
    
    if not klines:
        print(f"    âš ï¸  No data fetched")
        continue
    
    # Convert sang Pandas DataFrame
    df_klines = pd.DataFrame(klines, columns=[
        'open_time', 'open', 'high', 'low', 'close', 'volume',
        'close_time', 'quote_volume', 'trades', 'taker_buy_base',
        'taker_buy_quote', 'ignore'
    ])
    
    # Convert timestamp to datetime
    df_klines['open_time'] = pd.to_datetime(df_klines['open_time'], unit='ms')
    
    # Convert string prices to float
    for col_name in ['open', 'high', 'low', 'close', 'volume']:
        df_klines[col_name] = df_klines[col_name].astype(float)
    
    # Convert Pandas â†’ Spark DataFrame
    df_spark = spark.createDataFrame(
        df_klines[['open_time', 'open', 'high', 'low', 'close', 'volume']]
    )
    
    # ThÃªm cá»™t symbol vÃ  date
    df_spark = df_spark.withColumn("symbol", lit(symbol))
    df_spark = df_spark.withColumn("date", to_date(col("open_time")))
```

---

## 5. Xá»­ lÃ½ dá»¯ liá»‡u vá»›i PySpark

### 5.1. STEP 3: Clean Data

```python
# ==================================================================
# STEP 3: CLEAN DATA
# ==================================================================
print("\n[STEP 3] Cleaning data...")

# dropDuplicates: Loáº¡i bá» cÃ¡c dÃ²ng trÃ¹ng láº·p
# Dá»±a trÃªn cá»™t "symbol" vÃ  "open_time"
df_new_clean = df_new_raw.dropDuplicates(["symbol", "open_time"])

clean_rows = df_new_clean.count()
print(f"  âœ… After deduplication: {clean_rows:,} rows")
```

**Táº¡i sao cáº§n deduplication?**

```
CÃ³ thá»ƒ cÃ³ duplicate vÃ¬:
1. Binance API tráº£ vá» overlap data á»Ÿ biÃªn pagination
2. Network issues gÃ¢y retry â†’ duplicate requests
3. Data issues tá»« phÃ­a Binance

VÃ­ dá»¥:
Before dedup:
| symbol   | open_time           | close  |
|----------|---------------------|--------|
| BTCUSDT  | 2025-12-03 00:00:00 | 92000  |
| BTCUSDT  | 2025-12-03 00:00:00 | 92000  |  â† Duplicate!
| BTCUSDT  | 2025-12-03 00:01:00 | 92050  |

After dedup:
| symbol   | open_time           | close  |
|----------|---------------------|--------|
| BTCUSDT  | 2025-12-03 00:00:00 | 92000  |
| BTCUSDT  | 2025-12-03 00:01:00 | 92050  |
```

### 5.2. STEP 4: Daily Aggregation

```python
# ==================================================================
# STEP 4: AGGREGATE TO DAILY OHLC
# ==================================================================
print("\n[STEP 4] Aggregating to daily OHLC...")

df_daily = df_new_clean.groupBy("symbol", "date").agg(
    first("open").alias("open"),     # GiÃ¡ Ä‘áº§u ngÃ y
    max("high").alias("high"),       # GiÃ¡ cao nháº¥t trong ngÃ y
    min("low").alias("low"),         # GiÃ¡ tháº¥p nháº¥t trong ngÃ y
    last("close").alias("close"),    # GiÃ¡ cuá»‘i ngÃ y
    sum("volume").alias("volume")    # Tá»•ng volume trong ngÃ y
)

daily_count = df_daily.count()
print(f"  âœ… Daily aggregation: {daily_count} rows")
```

**Giáº£i thÃ­ch aggregation:**

```
Input: 1-minute data (1440 rows/ngÃ y)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ symbol  â”‚ open_time            â”‚ open   â”‚ high   â”‚ low    â”‚ close  â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ BTCUSDT â”‚ 2025-12-03 00:00:00  â”‚ 92000  â”‚ 92050  â”‚ 91980  â”‚ 92030  â”‚
â”‚ BTCUSDT â”‚ 2025-12-03 00:01:00  â”‚ 92030  â”‚ 92100  â”‚ 92020  â”‚ 92080  â”‚
â”‚ BTCUSDT â”‚ 2025-12-03 00:02:00  â”‚ 92080  â”‚ 92120  â”‚ 92050  â”‚ 92100  â”‚
â”‚ ...     â”‚ ...                  â”‚ ...    â”‚ ...    â”‚ ...    â”‚ ...    â”‚
â”‚ BTCUSDT â”‚ 2025-12-03 23:59:00  â”‚ 93000  â”‚ 93050  â”‚ 92980  â”‚ 93020  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Output: Daily OHLC (1 row/ngÃ y)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ symbol  â”‚ date       â”‚ open   â”‚ high   â”‚ low    â”‚ close  â”‚ volume â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ BTCUSDT â”‚ 2025-12-03 â”‚ 92000  â”‚ 93200  â”‚ 91800  â”‚ 93020  â”‚ 50000  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Aggregation logic:
  open  = first("open")  â†’ GiÃ¡ má»Ÿ cá»­a cá»§a phÃºt Ä‘áº§u tiÃªn
  high  = max("high")    â†’ GiÃ¡ cao nháº¥t cá»§a táº¥t cáº£ phÃºt
  low   = min("low")     â†’ GiÃ¡ tháº¥p nháº¥t cá»§a táº¥t cáº£ phÃºt
  close = last("close")  â†’ GiÃ¡ Ä‘Ã³ng cá»­a cá»§a phÃºt cuá»‘i cÃ¹ng
  volume = sum("volume") â†’ Tá»•ng volume cáº£ ngÃ y
```

---

## 6. Forward Fill - Äiá»n dá»¯ liá»‡u thiáº¿u

### 6.1. Táº¡i sao cáº§n Forward Fill?

```
Váº¥n Ä‘á»: CÃ³ thá»ƒ cÃ³ ngÃ y khÃ´ng cÃ³ giao dá»‹ch

VÃ­ dá»¥:
| date       | close  |
|------------|--------|
| 2025-12-01 | 92000  |
| 2025-12-02 | NULL   |  â† KhÃ´ng cÃ³ data (sÃ n nghá»‰?)
| 2025-12-03 | 93000  |

Giáº£i phÃ¡p: Forward Fill - Äiá»n giÃ¡ trá»‹ tá»« ngÃ y trÆ°á»›c

| date       | close  |
|------------|--------|
| 2025-12-01 | 92000  |
| 2025-12-02 | 92000  |  â† Filled tá»« 01/12
| 2025-12-03 | 93000  |
```

### 6.2. Code Forward Fill

```python
# ==================================================================
# STEP 5: FORWARD FILL MISSING DATES
# ==================================================================
print("\n[STEP 5] Forward filling missing dates...")

# BÆ°á»›c 1: Táº¡o dÃ£y ngÃ y Ä‘áº§y Ä‘á»§
date_range_df = spark.sql(f"""
    SELECT explode(sequence(
        to_date('{fetch_start_date}'),
        to_date('{today}'),
        interval 1 day
    )) as date
""")

# Giáº£i thÃ­ch:
# sequence(start, end, step): Táº¡o array cÃ¡c ngÃ y
# explode: Biáº¿n array thÃ nh nhiá»u rows
#
# VÃ­ dá»¥:
# sequence('2025-12-01', '2025-12-03', 1 day) 
#   â†’ ['2025-12-01', '2025-12-02', '2025-12-03']
# explode â†’ 3 rows

df_filled_list = []

for symbol in ["BTCUSDT", "ETHUSDT"]:
    df_symbol = df_daily.filter(col("symbol") == symbol)
    
    # BÆ°á»›c 2: Cross join vá»›i date range
    # Táº¡o táº¥t cáº£ káº¿t há»£p (symbol, date)
    df_complete = date_range_df.crossJoin(
        df_symbol.select("symbol").distinct()
    )
    
    # BÆ°á»›c 3: Left join vá»›i data thá»±c
    # Nhá»¯ng ngÃ y cÃ³ data â†’ cÃ³ giÃ¡ trá»‹
    # Nhá»¯ng ngÃ y khÃ´ng cÃ³ data â†’ NULL
    df_with_gaps = df_complete.join(df_symbol, ["symbol", "date"], "left")
    
    # BÆ°á»›c 4: Forward fill vá»›i Window function
    window_spec = Window.partitionBy("symbol") \
                        .orderBy("date") \
                        .rowsBetween(Window.unboundedPreceding, 0)
    
    # F.last(col, ignorenulls=True): Láº¥y giÃ¡ trá»‹ khÃ´ng null gáº§n nháº¥t
    for col_name in ["open", "high", "low", "close", "volume"]:
        df_with_gaps = df_with_gaps.withColumn(
            col_name,
            F.last(col(col_name), ignorenulls=True).over(window_spec)
        )
    
    df_filled_list.append(df_with_gaps)
```

### 6.3. Giáº£i thÃ­ch Window Function

```
Window.partitionBy("symbol").orderBy("date").rowsBetween(unboundedPreceding, 0)

Giáº£i thÃ­ch tá»«ng pháº§n:

1. partitionBy("symbol"):
   - Chia data thÃ nh groups theo symbol
   - BTC xá»­ lÃ½ riÃªng, ETH xá»­ lÃ½ riÃªng

2. orderBy("date"):
   - Sáº¯p xáº¿p theo ngÃ y tÄƒng dáº§n

3. rowsBetween(unboundedPreceding, 0):
   - unboundedPreceding = tá»« row Ä‘áº§u tiÃªn
   - 0 = Ä‘áº¿n row hiá»‡n táº¡i
   - Tá»©c lÃ  xem xÃ©t táº¥t cáº£ rows tá»« Ä‘áº§u Ä‘áº¿n hiá»‡n táº¡i

4. F.last(col, ignorenulls=True):
   - Láº¥y giÃ¡ trá»‹ cuá»‘i cÃ¹ng trong window
   - ignorenulls=True: bá» qua cÃ¡c NULL
   - â†’ Láº¥y giÃ¡ trá»‹ gáº§n nháº¥t khÃ´ng pháº£i NULL

VÃ­ dá»¥ minh há»a:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ symbol â”‚ date       â”‚ close  â”‚ Window (unboundedPreceding, 0)       â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ BTC    â”‚ 2025-12-01 â”‚ 92000  â”‚ [92000]          â†’ last = 92000     â”‚
â”‚ BTC    â”‚ 2025-12-02 â”‚ NULL   â”‚ [92000, NULL]    â†’ last = 92000     â”‚
â”‚ BTC    â”‚ 2025-12-03 â”‚ 93000  â”‚ [92000, NULL, 93000] â†’ last = 93000 â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

Káº¿t quáº£ sau forward fill:
â”‚ BTC    â”‚ 2025-12-01 â”‚ 92000  â”‚
â”‚ BTC    â”‚ 2025-12-02 â”‚ 92000  â”‚  â† Filled!
â”‚ BTC    â”‚ 2025-12-03 â”‚ 93000  â”‚
```

---

## 7. Moving Average (MA7, MA30)

### 7.1. Moving Average lÃ  gÃ¬?

```
Moving Average (Trung bÃ¬nh Ä‘á»™ng):
- Trung bÃ¬nh cá»§a N ngÃ y gáº§n nháº¥t
- GiÃºp lÃ m mÆ°á»£t (smooth) Ä‘Æ°á»ng giÃ¡
- Loáº¡i bá» noise, tháº¥y trend rÃµ hÆ¡n

MA7: Trung bÃ¬nh 7 ngÃ y gáº§n nháº¥t
MA30: Trung bÃ¬nh 30 ngÃ y gáº§n nháº¥t

VÃ­ dá»¥ MA7:
| date       | close  | MA7                          |
|------------|--------|------------------------------|
| 2025-11-27 | 91000  | (91000)/1 = 91000           |
| 2025-11-28 | 91500  | (91000+91500)/2 = 91250     |
| 2025-11-29 | 92000  | (91000+91500+92000)/3       |
| 2025-11-30 | 91800  | ...                          |
| 2025-12-01 | 92500  | ...                          |
| 2025-12-02 | 93000  | ...                          |
| 2025-12-03 | 92800  | (91000+...+92800)/7 = 92086 |
```

### 7.2. Code tÃ­nh MA

```python
# ==================================================================
# STEP 6: COMPUTE MA7 AND MA30
# ==================================================================
print("\n[STEP 6] Computing MA7 and MA30...")

# Window cho MA7: 6 rows trÆ°á»›c + row hiá»‡n táº¡i = 7 rows
window_ma7 = Window.partitionBy("symbol") \
                   .orderBy("date") \
                   .rowsBetween(-6, 0)

# Window cho MA30: 29 rows trÆ°á»›c + row hiá»‡n táº¡i = 30 rows
window_ma30 = Window.partitionBy("symbol") \
                    .orderBy("date") \
                    .rowsBetween(-29, 0)

# TÃ­nh trung bÃ¬nh
df_filled = df_filled.withColumn("MA7", avg("close").over(window_ma7))
df_filled = df_filled.withColumn("MA30", avg("close").over(window_ma30))

print(f"  âœ… MA7 and MA30 computed")
```

### 7.3. Giáº£i thÃ­ch Window cho MA

```
rowsBetween(-6, 0):
  -6 = 6 rows trÆ°á»›c row hiá»‡n táº¡i
   0 = row hiá»‡n táº¡i
  â†’ Tá»•ng cá»™ng 7 rows (MA7)

VÃ­ dá»¥ MA7:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ symbol â”‚ date       â”‚ close  â”‚ Window (-6, 0)                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚ BTC    â”‚ 2025-11-27 â”‚ 91000  â”‚ [91000]                â†’ avg=91000 â”‚
â”‚ BTC    â”‚ 2025-11-28 â”‚ 91500  â”‚ [91000,91500]          â†’ avg=91250 â”‚
â”‚ BTC    â”‚ 2025-11-29 â”‚ 92000  â”‚ [91000,91500,92000]    â†’ avg=91500 â”‚
â”‚ BTC    â”‚ 2025-11-30 â”‚ 91800  â”‚ [91000,91500,92000,91800]          â”‚
â”‚ BTC    â”‚ 2025-12-01 â”‚ 92500  â”‚ [91000,91500,92000,91800,92500]    â”‚
â”‚ BTC    â”‚ 2025-12-02 â”‚ 93000  â”‚ [91000,91500,92000,91800,92500,    â”‚
â”‚        â”‚            â”‚        â”‚  93000]                             â”‚
â”‚ BTC    â”‚ 2025-12-03 â”‚ 92800  â”‚ [91000,91500,92000,91800,92500,    â”‚
â”‚        â”‚            â”‚        â”‚  93000,92800] â†’ avg=92086          â”‚
â”‚ BTC    â”‚ 2025-12-04 â”‚ 93200  â”‚ [91500,92000,91800,92500,93000,    â”‚
â”‚        â”‚            â”‚        â”‚  92800,93200] â†’ Slide window!      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

LÆ°u Ã½: Nhá»¯ng ngÃ y Ä‘áº§u cÃ³ Ã­t hÆ¡n 7 rows, MA7 tÃ­nh trÃªn sá»‘ rows cÃ³ sáºµn.
```

---

## 8. Output vÃ  cáº¥u trÃºc dá»¯ liá»‡u

### 8.1. STEP 7: Save to Parquet

```python
# ==================================================================
# STEP 7: SAVE TO daily_filled
# ==================================================================
print("\n[STEP 7] Saving backfill data...")

# ThÃªm cá»™t year Ä‘á»ƒ partition
df_filled = df_filled.withColumn("year", year("date"))

# LÆ°u vá»›i partitioning
output_path = "data_analysis/daily_filled"
df_filled.write \
    .mode("overwrite") \        # Ghi Ä‘Ã¨ náº¿u Ä‘Ã£ tá»“n táº¡i
    .partitionBy("symbol", "year") \  # Partition theo symbol vÃ  nÄƒm
    .parquet(output_path)       # Format Parquet

print(f"  âœ… Saved to {output_path}")
```

### 8.2. Cáº¥u trÃºc thÆ° má»¥c output

```
data_analysis/
â””â”€â”€ daily_filled/
    â”œâ”€â”€ _SUCCESS                    # Marker file (ghi thÃ nh cÃ´ng)
    â”œâ”€â”€ symbol=BTCUSDT/
    â”‚   â”œâ”€â”€ year=2025/
    â”‚   â”‚   â”œâ”€â”€ part-00000-xxx.snappy.parquet
    â”‚   â”‚   â”œâ”€â”€ part-00001-xxx.snappy.parquet
    â”‚   â”‚   â””â”€â”€ ...
    â”‚   â””â”€â”€ year=2024/
    â”‚       â””â”€â”€ ...
    â””â”€â”€ symbol=ETHUSDT/
        â””â”€â”€ year=2025/
            â””â”€â”€ ...

Táº¡i sao partition?
1. Query nhanh hÆ¡n:
   - Query BTC only â†’ chá»‰ Ä‘á»c folder BTCUSDT
   - Query 2025 only â†’ chá»‰ Ä‘á»c folder year=2025

2. Parallel processing:
   - Má»—i partition xá»­ lÃ½ trÃªn 1 executor

3. Dá»… quáº£n lÃ½:
   - XÃ³a data nÄƒm cÅ© dá»… dÃ ng
   - Biáº¿t ngay cÃ³ data symbol nÃ o
```

### 8.3. STEP 8: Update Prophet Input

```python
# ==================================================================
# STEP 8: UPDATE prophet_input
# ==================================================================
print("\n[STEP 8] Updating prophet_input...")

# Prophet yÃªu cáº§u format: ds (date), y (value), symbol
df_prophet = df_filled.select(
    col("date").alias("ds"),    # Äá»•i tÃªn date â†’ ds (Prophet convention)
    col("close").alias("y"),    # Äá»•i tÃªn close â†’ y (target variable)
    "symbol",
    "MA7",
    "MA30"
).orderBy("symbol", "ds")

df_prophet.write \
    .mode("overwrite") \
    .partitionBy("symbol") \
    .parquet("data_analysis/prophet_input")

print(f"  âœ… Prophet input updated")
```

### 8.4. Schema cá»§a output

```
daily_filled schema:
root
 |-- date: date
 |-- symbol: string
 |-- open: double
 |-- high: double
 |-- low: double
 |-- close: double
 |-- volume: double
 |-- MA7: double
 |-- MA30: double
 |-- year: integer (partition column)

prophet_input schema:
root
 |-- ds: date
 |-- y: double
 |-- symbol: string
 |-- MA7: double
 |-- MA30: double
```

---

## 9. CÃ¢u há»i thÆ°á»ng gáº·p

### Q1: Táº¡i sao dÃ¹ng Binance API thay vÃ¬ WebSocket?

```
A: VÃ¬ má»¥c Ä‘Ã­ch khÃ¡c nhau:

Binance REST API (dÃ¹ng trong Batch Layer):
â”œâ”€â”€ Láº¥y dá»¯ liá»‡u Lá»ŠCH Sá»¬
â”œâ”€â”€ CÃ³ thá»ƒ láº¥y data tá»« quÃ¡ khá»©
â”œâ”€â”€ Synchronous (gá»i - Ä‘á»£i - nháº­n)
â””â”€â”€ PhÃ¹ há»£p cho backfill

Binance WebSocket (dÃ¹ng trong Speed Layer):
â”œâ”€â”€ Láº¥y dá»¯ liá»‡u REAL-TIME
â”œâ”€â”€ Chá»‰ láº¥y data tá»« thá»i Ä‘iá»ƒm connect
â”œâ”€â”€ Asynchronous (stream liÃªn tá»¥c)
â””â”€â”€ PhÃ¹ há»£p cho streaming

Batch Layer cáº§n láº¥y data QUÃ KHá»¨ (nhá»¯ng ngÃ y mÃ¡y táº¯t)
â†’ Pháº£i dÃ¹ng REST API
```

### Q2: Limit 1000 records cÃ³ Ä‘á»§ khÃ´ng?

```
A: Äá»§, vÃ¬ ta loop cho Ä‘áº¿n khi háº¿t:

VÃ­ dá»¥: 9 ngÃ y Ã— 1440 phÃºt = 12,960 records

Loop 1: Láº¥y record 1-1000
Loop 2: Láº¥y record 1001-2000
...
Loop 13: Láº¥y record 12001-12960

Má»—i loop cáº­p nháº­t current_start:
  current_start = klines[-1][6] + 1
  (close_time cá»§a record cuá»‘i + 1ms)
```

### Q3: Rate limiting 0.1s cÃ³ Ä‘á»§ an toÃ n?

```
A: CÃ³, tháº­m chÃ­ cÃ³ thá»ƒ nhanh hÆ¡n:

Binance limit:
â”œâ”€â”€ 1200 requests/phÃºt cho IP
â”œâ”€â”€ 10 requests/giÃ¢y

Vá»›i 0.1s delay:
â”œâ”€â”€ Max 10 requests/giÃ¢y
â”œâ”€â”€ = 600 requests/phÃºt
â””â”€â”€ Chá»‰ dÃ¹ng 50% quota â†’ An toÃ n

Náº¿u cáº§n nhanh hÆ¡n, cÃ³ thá»ƒ giáº£m xuá»‘ng 0.05s
NhÆ°ng 0.1s Ä‘á»§ á»•n Ä‘á»‹nh vÃ  trÃ¡nh bá»‹ block
```

### Q4: Forward fill cÃ³ chÃ­nh xÃ¡c khÃ´ng?

```
A: Forward fill lÃ  BEST PRACTICE cho time series:

Táº¡i sao khÃ´ng dÃ¹ng:
â”œâ”€â”€ Backward fill: DÃ¹ng data tÆ°Æ¡ng lai â†’ sai logic
â”œâ”€â”€ Interpolation: Giáº£ Ä‘á»‹nh linear â†’ khÃ´ng phÃ¹ há»£p vá»›i giÃ¡
â”œâ”€â”€ Mean fill: LÃ m máº¥t pattern
â””â”€â”€ Zero fill: Sai hoÃ n toÃ n

Forward fill:
â”œâ”€â”€ Giá»¯ nguyÃªn giÃ¡ cuá»‘i cÃ¹ng biáº¿t Ä‘Æ°á»£c
â”œâ”€â”€ Assumption: "KhÃ´ng cÃ³ thay Ä‘á»•i = giÃ¡ giá»¯ nguyÃªn"
â”œâ”€â”€ Phá»• biáº¿n trong finance
â””â”€â”€ Prophet vÃ  cÃ¡c model time series Ä‘á»u cháº¥p nháº­n
```

### Q5: Táº¡i sao tÃ­nh MA trong Batch Layer?

```
A: Äá»ƒ consistency vÃ  efficiency:

1. Consistency:
   - MA tÃ­nh trÃªn TOÃ€N Bá»˜ timeline
   - Náº¿u tÃ­nh trong Prophet, chá»‰ tÃ­nh trÃªn training data
   
2. Efficiency:
   - TÃ­nh 1 láº§n, dÃ¹ng nhiá»u láº§n
   - KhÃ´ng cáº§n tÃ­nh láº¡i má»—i khi train

3. Feature engineering:
   - MA7, MA30 lÃ  features quan trá»ng
   - CÃ³ thá»ƒ thÃªm nhiá»u features khÃ¡c (RSI, MACD...)
```

---

## ğŸ“š TÃ i liá»‡u tiáº¿p theo

Sau khi hiá»ƒu Batch Layer, tiáº¿p tá»¥c vá»›i:

**WEEK6_03_SPEED_LAYER.md** - Giáº£i thÃ­ch Kafka + Spark Streaming

---

*Táº¡o bá»Ÿi: Big Data Project - Week 6 Documentation*
*Cáº­p nháº­t: 03/12/2025*
