# B√ÅO C√ÅO ƒê·ªí √ÅN M√îN PH√ÇN T√çCH D·ªÆ LI·ªÜU L·ªöN

**TR∆Ø·ªúNG ƒê·∫†I H·ªåC QUY NH∆†N**  
**KHOA C√îNG NGH·ªÜ TH√îNG TIN**

---

**ƒê·ªÅ t√†i:** H·ªá th·ªëng d·ª± ƒëo√°n gi√° Bitcoin v√† Ethereum v·ªõi ki·∫øn tr√∫c Lambda Architecture

**Gi·∫£ng vi√™n h∆∞·ªõng d·∫´n:** Tr·∫ßn Thi√™n Th√†nh  
**Sinh vi√™n th·ª±c hi·ªán:** ƒêo√†n Th·∫ø T√≠n  
**M√£ sinh vi√™n:** 4551190056  
**L·ªõp:** KTPM45  
**M√£ l·ªõp h·ªçc ph·∫ßn:** 251105026501

*Gia Lai, th√°ng 12 nƒÉm 2025*

---

## M·ª§C L·ª§C

- [M·ªû ƒê·∫¶U](#m·ªü-ƒë·∫ßu)
  - [1. L√Ω do ch·ªçn ƒë·ªÅ t√†i](#1-l√Ω-do-ch·ªçn-ƒë·ªÅ-t√†i)
  - [2. B√†i to√°n c·∫ßn gi·∫£i quy·∫øt](#2-b√†i-to√°n-c·∫ßn-gi·∫£i-quy·∫øt)
  - [3. Y√™u c·∫ßu c·ªßa h·ªá th·ªëng](#3-y√™u-c·∫ßu-c·ªßa-h·ªá-th·ªëng)
  - [4. Gi·∫£i ph√°p th·ª±c hi·ªán](#4-gi·∫£i-ph√°p-th·ª±c-hi·ªán)
- [PH·∫¶N 1: THI·∫æT K·∫æ KI·∫æN TR√öC H·ªÜ TH·ªêNG](#ph·∫ßn-1-thi·∫øt-k·∫ø-ki·∫øn-tr√∫c-h·ªá-th·ªëng)
- [PH·∫¶N 2: THU TH·∫¨P D·ªÆ LI·ªÜU](#ph·∫ßn-2-thu-th·∫≠p-d·ªØ-li·ªáu)
- [PH·∫¶N 3: X√ÇY D·ª∞NG H·ªÜ TH·ªêNG](#ph·∫ßn-3-x√¢y-d·ª±ng-h·ªá-th·ªëng)
- [PH·∫¶N 4: TRI·ªÇN KHAI DEMO](#ph·∫ßn-4-tri·ªÉn-khai-demo)
- [PH·∫¶N 5: K·∫æT LU·∫¨N](#ph·∫ßn-5-k·∫øt-lu·∫≠n)
- [T√ÄI LI·ªÜU THAM KH·∫¢O](#t√†i-li·ªáu-tham-kh·∫£o)
- [PH·ª§ L·ª§C](#ph·ª•-l·ª•c)

---

## M·ªû ƒê·∫¶U

### 1. L√Ω do ch·ªçn ƒë·ªÅ t√†i

Trong nh·ªØng nƒÉm g·∫ßn ƒë√¢y, th·ªã tr∆∞·ªùng ti·ªÅn m√£ h√≥a (cryptocurrency) ƒë√£ tr·ªü th√†nh m·ªôt trong nh·ªØng lƒ©nh v·ª±c t√†i ch√≠nh ph√°t tri·ªÉn nhanh nh·∫•t tr√™n th·∫ø gi·ªõi. Bitcoin (BTC) v√† Ethereum (ETH) l√† hai ƒë·ªìng ti·ªÅn m√£ h√≥a c√≥ v·ªën h√≥a th·ªã tr∆∞·ªùng l·ªõn nh·∫•t, thu h√∫t h√†ng tri·ªáu nh√† ƒë·∫ßu t∆∞ tr√™n to√†n c·∫ßu.

**Nh·ªØng l√Ω do ch√≠nh ƒë·ªÉ ch·ªçn ƒë·ªÅ t√†i n√†y:**

1. **Quy m√¥ d·ªØ li·ªáu l·ªõn:** D·ªØ li·ªáu giao d·ªãch cryptocurrency ƒë∆∞·ª£c ghi nh·∫≠n li√™n t·ª•c 24/7 v·ªõi t·∫ßn su·∫•t cao (m·ªói ph√∫t), t·∫°o ra kh·ªëi l∆∞·ª£ng d·ªØ li·ªáu kh·ªïng l·ªì (h√†ng tri·ªáu b·∫£n ghi). ƒê√¢y l√† b√†i to√°n ƒëi·ªÉn h√¨nh ƒë·ªÉ √°p d·ª•ng c√¥ng ngh·ªá Big Data.

2. **Nhu c·∫ßu d·ª± ƒëo√°n gi√°:** Nh√† ƒë·∫ßu t∆∞ lu√¥n mong mu·ªën c√≥ c√¥ng c·ª• d·ª± ƒëo√°n xu h∆∞·ªõng gi√° ƒë·ªÉ ƒë∆∞a ra quy·∫øt ƒë·ªãnh ƒë·∫ßu t∆∞ h·ª£p l√Ω. M√¥ h√¨nh d·ª± ƒëo√°n ch√≠nh x√°c c√≥ gi√° tr·ªã th·ª±c ti·ªÖn cao.

3. **K·∫øt h·ª£p x·ª≠ l√Ω batch v√† real-time:** Th·ªã tr∆∞·ªùng crypto ho·∫°t ƒë·ªông li√™n t·ª•c, ƒë√≤i h·ªèi h·ªá th·ªëng v·ª´a ph√¢n t√≠ch d·ªØ li·ªáu l·ªãch s·ª≠ (batch) v·ª´a x·ª≠ l√Ω d·ªØ li·ªáu th·ªùi gian th·ª±c (streaming) - ph√π h·ª£p v·ªõi ki·∫øn tr√∫c Lambda.

4. **·ª®ng d·ª•ng Machine Learning:** D·ªØ li·ªáu chu·ªói th·ªùi gian c·ªßa gi√° cryptocurrency l√† ƒë·∫ßu v√†o l√Ω t∆∞·ªüng cho c√°c m√¥ h√¨nh d·ª± ƒëo√°n nh∆∞ Prophet, LSTM, ARIMA.

### 2. B√†i to√°n c·∫ßn gi·∫£i quy·∫øt

**B√†i to√°n ch√≠nh:** X√¢y d·ª±ng h·ªá th·ªëng d·ª± ƒëo√°n gi√° Bitcoin (BTCUSDT) v√† Ethereum (ETHUSDT) d·ª±a tr√™n d·ªØ li·ªáu l·ªãch s·ª≠ v√† d·ªØ li·ªáu th·ªùi gian th·ª±c.

**C√°c b√†i to√°n con:**

1. **Thu th·∫≠p d·ªØ li·ªáu:**
   - Thu th·∫≠p d·ªØ li·ªáu l·ªãch s·ª≠ t·ª´ Kaggle (2012-2025, kho·∫£ng 11.5 tri·ªáu b·∫£n ghi)
   - Thu th·∫≠p d·ªØ li·ªáu th·ªùi gian th·ª±c t·ª´ Binance API
   - X·ª≠ l√Ω streaming data qua Apache Kafka

2. **X·ª≠ l√Ω v√† l√†m s·∫°ch d·ªØ li·ªáu:**
   - Chu·∫©n h√≥a schema gi·ªØa c√°c ngu·ªìn d·ªØ li·ªáu kh√°c nhau
   - X·ª≠ l√Ω missing values v√† duplicates
   - Chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu t·ª´ t·∫ßn su·∫•t 1 ph√∫t sang t·∫ßn su·∫•t ng√†y
   - T√≠nh to√°n c√°c ƒë·∫∑c tr∆∞ng b·ªï sung (Moving Average MA7, MA30)

3. **X√¢y d·ª±ng m√¥ h√¨nh d·ª± ƒëo√°n:**
   - Hu·∫•n luy·ªán m√¥ h√¨nh Prophet v·ªõi hyperparameter tuning
   - ƒê√°nh gi√° m√¥ h√¨nh b·∫±ng cross-validation
   - D·ª± ƒëo√°n gi√° v·ªõi ƒë·ªô ch√≠nh x√°c cao (MAPE < 5%)

4. **Tri·ªÉn khai v√† tr·ª±c quan h√≥a:**
   - X√¢y d·ª±ng dashboard t∆∞∆°ng t√°c ƒë·ªÉ hi·ªÉn th·ªã k·∫øt qu·∫£
   - Cho ph√©p ng∆∞·ªùi d√πng kh√°m ph√° d·ª± ƒëo√°n v√† metrics

### 3. Y√™u c·∫ßu c·ªßa h·ªá th·ªëng

**Y√™u c·∫ßu ch·ª©c nƒÉng:**

| STT | Y√™u c·∫ßu | M√¥ t·∫£ |
|-----|---------|-------|
| 1 | X·ª≠ l√Ω d·ªØ li·ªáu batch | X·ª≠ l√Ω d·ªØ li·ªáu l·ªãch s·ª≠ t·ª´ file CSV (11.5 tri·ªáu b·∫£n ghi) |
| 2 | X·ª≠ l√Ω d·ªØ li·ªáu streaming | Thu th·∫≠p v√† x·ª≠ l√Ω d·ªØ li·ªáu real-time t·ª´ Binance API |
| 3 | Aggregate d·ªØ li·ªáu | Chuy·ªÉn ƒë·ªïi d·ªØ li·ªáu 1-ph√∫t th√†nh d·ªØ li·ªáu ng√†y (daily OHLC) |
| 4 | D·ª± ƒëo√°n gi√° | D·ª± ƒëo√°n gi√° BTC/ETH v·ªõi MAPE < 5% |
| 5 | Tr·ª±c quan h√≥a | Dashboard hi·ªÉn th·ªã k·∫øt qu·∫£ d·ª± ƒëo√°n v√† metrics |

**Y√™u c·∫ßu phi ch·ª©c nƒÉng:**

| STT | Y√™u c·∫ßu | M√¥ t·∫£ |
|-----|---------|-------|
| 1 | Kh·∫£ nƒÉng m·ªü r·ªông | Ki·∫øn tr√∫c c√≥ th·ªÉ scale ƒë·ªÉ x·ª≠ l√Ω th√™m nhi·ªÅu cryptocurrency |
| 2 | Hi·ªáu su·∫•t | X·ª≠ l√Ω 11.5 tri·ªáu b·∫£n ghi trong v√≤ng 10 ph√∫t |
| 3 | ƒê·ªô tin c·∫≠y | C√≥ checkpoint v√† fault tolerance cho streaming |
| 4 | T√≠nh module | C√°c th√†nh ph·∫ßn ƒë·ªôc l·∫≠p, d·ªÖ b·∫£o tr√¨ v√† n√¢ng c·∫•p |

### 4. Gi·∫£i ph√°p th·ª±c hi·ªán

ƒê·ªÉ gi·∫£i quy·∫øt c√°c b√†i to√°n tr√™n, ƒë·ªì √°n √°p d·ª•ng **ki·∫øn tr√∫c Lambda Architecture** k·∫øt h·ª£p v·ªõi c√°c c√¥ng ngh·ªá Big Data hi·ªán ƒë·∫°i:

**Ki·∫øn tr√∫c Lambda Architecture:**

```
                    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
                    ‚îÇ           DATA SOURCES                  ‚îÇ
                    ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê      ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê     ‚îÇ
                    ‚îÇ  ‚îÇ  Kaggle  ‚îÇ      ‚îÇ Binance API  ‚îÇ     ‚îÇ
                    ‚îÇ  ‚îÇ   CSV    ‚îÇ      ‚îÇ  Real-time   ‚îÇ     ‚îÇ
                    ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò      ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò     ‚îÇ
                    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                            ‚îÇ                  ‚îÇ
            ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê           ‚îÇ
            ‚ñº                      ‚îÇ           ‚ñº
    ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê              ‚îÇ   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
    ‚îÇ  BATCH LAYER  ‚îÇ              ‚îÇ   ‚îÇ  SPEED LAYER  ‚îÇ
    ‚îÇ               ‚îÇ              ‚îÇ   ‚îÇ               ‚îÇ
    ‚îÇ ‚Ä¢ CSV‚ÜíParquet ‚îÇ              ‚îÇ   ‚îÇ ‚Ä¢ Kafka       ‚îÇ
    ‚îÇ ‚Ä¢ Aggregate   ‚îÇ              ‚îÇ   ‚îÇ ‚Ä¢ Spark       ‚îÇ
    ‚îÇ ‚Ä¢ Backfill    ‚îÇ              ‚îÇ   ‚îÇ   Streaming   ‚îÇ
    ‚îÇ ‚Ä¢ Prophet     ‚îÇ              ‚îÇ   ‚îÇ               ‚îÇ
    ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò              ‚îÇ   ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
            ‚îÇ                      ‚îÇ           ‚îÇ
            ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò           ‚îÇ
                       ‚ñº                       ‚îÇ
               ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê               ‚îÇ
               ‚îÇ SERVING LAYER ‚îÇ‚óÑ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
               ‚îÇ               ‚îÇ
               ‚îÇ ‚Ä¢ Merge data  ‚îÇ
               ‚îÇ ‚Ä¢ Query       ‚îÇ
               ‚îÇ ‚Ä¢ Dashboard   ‚îÇ
               ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

**C√¥ng ngh·ªá s·ª≠ d·ª•ng:**

| Th√†nh ph·∫ßn | C√¥ng ngh·ªá | M·ª•c ƒë√≠ch |
|------------|-----------|----------|
| X·ª≠ l√Ω d·ªØ li·ªáu | Apache Spark 3.5.3 | Distributed processing cho batch v√† streaming |
| L∆∞u tr·ªØ | Parquet | Columnar format, n√©n 40%, query nhanh |
| Message Broker | Apache Kafka 7.5.0 | Streaming data v·ªõi throughput cao |
| Machine Learning | Facebook Prophet 1.2.1 | Time series forecasting |
| Dashboard | Streamlit | Web application t∆∞∆°ng t√°c |
| Ng√¥n ng·ªØ | Python 3.10 | T√≠ch h·ª£p t·ªët v·ªõi Spark, Kafka, Prophet |

**Quy tr√¨nh th·ª±c hi·ªán:**

1. **Phase 1 - Thu th·∫≠p d·ªØ li·ªáu:** T·∫£i d·ªØ li·ªáu t·ª´ Kaggle v√† thi·∫øt l·∫≠p k·∫øt n·ªëi Binance API
2. **Phase 2 - X·ª≠ l√Ω Batch:** Chuy·ªÉn ƒë·ªïi CSV sang Parquet, aggregate 1-min ‚Üí daily, t√≠nh MA7/MA30
3. **Phase 3 - X·ª≠ l√Ω Streaming:** Thi·∫øt l·∫≠p Kafka, t·∫°o Producer/Consumer, x·ª≠ l√Ω real-time
4. **Phase 4 - Machine Learning:** Hu·∫•n luy·ªán Prophet v·ªõi grid search, cross-validation
5. **Phase 5 - Tri·ªÉn khai:** Merge layers, t·∫°o dashboard Streamlit

---

## PH·∫¶N 1: THI·∫æT K·∫æ KI·∫æN TR√öC H·ªÜ TH·ªêNG

### 1.1. T·ªïng quan ki·∫øn tr√∫c Lambda

**Lambda Architecture** l√† m·ªôt ki·∫øn tr√∫c x·ª≠ l√Ω d·ªØ li·ªáu ƒë∆∞·ª£c thi·∫øt k·∫ø ƒë·ªÉ x·ª≠ l√Ω kh·ªëi l∆∞·ª£ng d·ªØ li·ªáu l·ªõn b·∫±ng c√°ch t·∫≠n d·ª•ng c·∫£ ph∆∞∆°ng ph√°p x·ª≠ l√Ω batch v√† stream. Ki·∫øn tr√∫c n√†y ƒë∆∞·ª£c ƒë·ªÅ xu·∫•t b·ªüi Nathan Marz v√† ƒë∆∞·ª£c √°p d·ª•ng r·ªông r√£i trong c√°c h·ªá th·ªëng Big Data.

**Ba t·∫ßng ch√≠nh c·ªßa Lambda Architecture:**

1. **Batch Layer:** X·ª≠ l√Ω to√†n b·ªô d·ªØ li·ªáu l·ªãch s·ª≠, t·∫°o ra batch views ch√≠nh x√°c
2. **Speed Layer:** X·ª≠ l√Ω d·ªØ li·ªáu real-time, b√π ƒë·∫Øp ƒë·ªô tr·ªÖ c·ªßa batch layer
3. **Serving Layer:** K·∫øt h·ª£p k·∫øt qu·∫£ t·ª´ batch v√† speed layer ƒë·ªÉ ph·ª•c v·ª• truy v·∫•n

### 1.2. Gi·∫£i th√≠ch c√°c th√†nh ph·∫ßn trong ki·∫øn tr√∫c

#### 1.2.1. Batch Layer

**Ch·ª©c nƒÉng:** X·ª≠ l√Ω d·ªØ li·ªáu l·ªãch s·ª≠ v·ªõi ƒë·ªô ch√≠nh x√°c cao, kh√¥ng y√™u c·∫ßu th·ªùi gian th·ª±c.

**C√°c th√†nh ph·∫ßn:**

| Th√†nh ph·∫ßn | File | Ch·ª©c nƒÉng |
|------------|------|-----------|
| CSV to Parquet | `convert_to_parquet.py` | Chuy·ªÉn ƒë·ªïi 11.5M rows CSV sang Parquet partitioned |
| Data Cleaning | `preprocess_step1.py` | Forward fill, dedup, aggregate 1-min ‚Üí daily |
| Feature Engineering | `preprocess_step2.py` | T√≠nh MA7/MA30, fill missing days |
| Backfill | `week6_backfill.py` | L·∫•y d·ªØ li·ªáu thi·∫øu t·ª´ Binance API |
| ML Training | `prophet_train.py` | Hu·∫•n luy·ªán m√¥ h√¨nh Prophet |

**Lu·ªìng d·ªØ li·ªáu Batch Layer:**

```
CSV (557 MB, 11.5M rows)
    ‚îÇ
    ‚ñº convert_to_parquet.py
Parquet (335 MB, partitioned by year/month)
    ‚îÇ
    ‚ñº preprocess_step1.py
daily_raw (7,980 rows, OHLCV)
    ‚îÇ
    ‚ñº preprocess_step2.py
daily_filled (7,980 rows + MA7/MA30)
    ‚îÇ
    ‚ñº prophet_train.py
Forecasts + Metrics (MAPE 2.38%)
```

#### 1.2.2. Speed Layer

**Ch·ª©c nƒÉng:** Thu th·∫≠p v√† x·ª≠ l√Ω d·ªØ li·ªáu real-time t·ª´ Binance API, b·ªï sung d·ªØ li·ªáu m·ªõi nh·∫•t.

**C√°c th√†nh ph·∫ßn:**

| Th√†nh ph·∫ßn | File | Ch·ª©c nƒÉng |
|------------|------|-----------|
| Kafka Broker | `docker-compose.yml` | Message broker (Zookeeper + Kafka) |
| Producer | `websocket_producer.py` | Poll Binance API m·ªói 1 gi√¢y, g·ª≠i v√†o Kafka |
| Consumer (Production) | `spark_streaming_consumer.py` | Spark Structured Streaming v·ªõi watermark |
| Consumer (Demo) | `kafka_batch_reader.py` | Batch read t·ª´ Kafka cho demo nhanh |

**Lu·ªìng d·ªØ li·ªáu Speed Layer:**

```
Binance API (https://api.binance.com/api/v3/ticker/24hr)
    ‚îÇ Poll every 1 second
    ‚ñº
Kafka Producer (websocket_producer.py)
    ‚îÇ Topic: crypto_prices
    ‚ñº
Kafka Broker (localhost:9092)
    ‚îÇ
    ‚ñº
Spark Consumer (kafka_batch_reader.py)
    ‚îÇ Aggregate to daily OHLC
    ‚ñº
streaming_output_spark_BATCH/ (Parquet)
```

#### 1.2.3. Serving Layer

**Ch·ª©c nƒÉng:** K·∫øt h·ª£p k·∫øt qu·∫£ t·ª´ Batch Layer v√† Speed Layer, ph·ª•c v·ª• truy v·∫•n v√† hi·ªÉn th·ªã.

**C√°c th√†nh ph·∫ßn:**

| Th√†nh ph·∫ßn | File | Ch·ª©c nƒÉng |
|------------|------|-----------|
| Merge | `week6_merge.py` | Union batch + streaming, dedup, recompute MA |
| Dashboard | `app.py` + `pages/` | Streamlit web application |

**Logic Merge:**

```python
# 1. ƒê·ªçc Batch Layer
df_batch = spark.read.parquet("data_analysis/daily_filled")

# 2. ƒê·ªçc Speed Layer
df_streaming = spark.read.parquet("streaming_output_spark_BATCH")

# 3. Union v√† lo·∫°i b·ªè tr√πng l·∫∑p
df_merged = df_batch.union(df_streaming).dropDuplicates(["symbol", "date"])

# 4. T√≠nh l·∫°i MA7/MA30 cho to√†n b·ªô timeline
df_merged = df_merged.withColumn("ma7", avg("daily_close").over(window_ma7))
df_merged = df_merged.withColumn("ma30", avg("daily_close").over(window_ma30))

# 5. L∆∞u k·∫øt qu·∫£
df_merged.write.mode("overwrite").parquet("data_analysis/daily_filled")
```

### 1.3. Lu·ªìng d·ªØ li·ªáu t·ªïng th·ªÉ

**S∆° ƒë·ªì lu·ªìng d·ªØ li·ªáu:**

```
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ                           DATA SOURCES                                   ‚îÇ
‚îú‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î§
‚îÇ         KAGGLE CSV              ‚îÇ          BINANCE API                  ‚îÇ
‚îÇ  ‚Ä¢ BTCUSDT_1min (7.2M rows)     ‚îÇ  ‚Ä¢ REST API polling                   ‚îÇ
‚îÇ  ‚Ä¢ ETHUSDT_1min (4.3M rows)     ‚îÇ  ‚Ä¢ 1 request/second                   ‚îÇ
‚îÇ  ‚Ä¢ Timeline: 2012-2025          ‚îÇ  ‚Ä¢ Real-time prices                   ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¥‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                 ‚îÇ                                  ‚îÇ
                 ‚ñº                                  ‚ñº
‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
‚îÇ         BATCH LAYER            ‚îÇ  ‚îÇ           SPEED LAYER               ‚îÇ
‚îÇ                                ‚îÇ  ‚îÇ                                     ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ 1. convert_to_parquet.py ‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ 1. websocket_producer.py   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ    CSV ‚Üí Parquet         ‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ    API ‚Üí Kafka             ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ               ‚ñº                ‚îÇ  ‚îÇ                 ‚ñº                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ 2. preprocess_step1.py   ‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ 2. Kafka Broker            ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ    1-min ‚Üí Daily OHLC    ‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ    Topic: crypto_prices    ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ               ‚ñº                ‚îÇ  ‚îÇ                 ‚ñº                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ  ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îÇ
‚îÇ  ‚îÇ 3. preprocess_step2.py   ‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ 3. kafka_batch_reader.py   ‚îÇ   ‚îÇ
‚îÇ  ‚îÇ    Fill gaps + MA7/MA30  ‚îÇ  ‚îÇ  ‚îÇ  ‚îÇ    Kafka ‚Üí Daily OHLC      ‚îÇ   ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ  ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò   ‚îÇ
‚îÇ               ‚ñº                ‚îÇ  ‚îÇ                 ‚îÇ                  ‚îÇ
‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ  ‚îÇ                 ‚îÇ                  ‚îÇ
‚îÇ  ‚îÇ 4. prophet_train.py      ‚îÇ  ‚îÇ  ‚îÇ                 ‚îÇ                  ‚îÇ
‚îÇ  ‚îÇ    Train + Forecast      ‚îÇ  ‚îÇ  ‚îÇ                 ‚îÇ                  ‚îÇ
‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ  ‚îÇ                 ‚îÇ                  ‚îÇ
‚îÇ               ‚îÇ                ‚îÇ  ‚îÇ                 ‚îÇ                  ‚îÇ
‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                ‚îÇ                                     ‚îÇ
                ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
                               ‚ñº
               ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê
               ‚îÇ           SERVING LAYER               ‚îÇ
               ‚îÇ                                       ‚îÇ
               ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
               ‚îÇ  ‚îÇ 1. week6_merge.py               ‚îÇ  ‚îÇ
               ‚îÇ  ‚îÇ    Merge batch + streaming      ‚îÇ  ‚îÇ
               ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
               ‚îÇ                  ‚ñº                    ‚îÇ
               ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
               ‚îÇ  ‚îÇ 2. daily_filled (unified)       ‚îÇ  ‚îÇ
               ‚îÇ  ‚îÇ    Complete timeline + MA       ‚îÇ  ‚îÇ
               ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
               ‚îÇ                  ‚ñº                    ‚îÇ
               ‚îÇ  ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê  ‚îÇ
               ‚îÇ  ‚îÇ 3. Streamlit Dashboard          ‚îÇ  ‚îÇ
               ‚îÇ  ‚îÇ    Visualize forecasts          ‚îÇ  ‚îÇ
               ‚îÇ  ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò  ‚îÇ
               ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò
```

---

## PH·∫¶N 2: THU TH·∫¨P D·ªÆ LI·ªÜU

### 2.1. Ngu·ªìn d·ªØ li·ªáu

H·ªá th·ªëng thu th·∫≠p d·ªØ li·ªáu t·ª´ hai ngu·ªìn ch√≠nh:

#### 2.1.1. D·ªØ li·ªáu l·ªãch s·ª≠ t·ª´ Kaggle

| Th√¥ng tin | BTCUSDT | ETHUSDT | T·ªïng |
|-----------|---------|---------|------|
| **File** | BTCUSDT_1min_2012-2025.csv | ETHUSDT_1min_2017-2025.csv | - |
| **Kho·∫£ng th·ªùi gian** | 01/01/2012 ‚Üí 25/09/2025 | 16/08/2017 ‚Üí 25/09/2025 | - |
| **S·ªë ng√†y** | 5,017 ng√†y | 2,963 ng√†y | 7,980 ng√†y |
| **S·ªë b·∫£n ghi (1-ph√∫t)** | ~7.2 tri·ªáu | ~4.3 tri·ªáu | **~11.5 tri·ªáu** |
| **K√≠ch th∆∞·ªõc file** | 361 MB | 197 MB | **557 MB** |
| **T·∫ßn su·∫•t** | 1 ph√∫t | 1 ph√∫t | - |
| **ƒê·ªãnh d·∫°ng** | CSV | CSV | - |

**ƒê·∫∑c ƒëi·ªÉm d·ªØ li·ªáu Kaggle:**
- D·ªØ li·ªáu OHLCV (Open, High, Low, Close, Volume) cho m·ªói ph√∫t
- C·∫≠p nh·∫≠t tƒ©nh (kh√¥ng real-time)
- Ch·∫•t l∆∞·ª£ng cao, √≠t missing values v√† kh√¥ng c√≥ duplicates

#### 2.1.2. D·ªØ li·ªáu th·ªùi gian th·ª±c t·ª´ Binance API

**Endpoint s·ª≠ d·ª•ng:**

```
GET https://api.binance.com/api/v3/klines
Parameters:
  - symbol: BTCUSDT / ETHUSDT
  - interval: 1m (1 ph√∫t)
  - limit: 1000 (t·ªëi ƒëa m·ªói request)
```

**M·ª•c ƒë√≠ch s·ª≠ d·ª•ng:**

1. **Backfill gaps:** L·∫•y d·ªØ li·ªáu t·ª´ 26/09/2025 ƒë·∫øn 14/12/2025 (80 ng√†y thi·∫øu sau khi t·∫£i Kaggle)
2. **Real-time streaming:** C·∫≠p nh·∫≠t gi√° m·ªõi nh·∫•t m·ªói gi√¢y qua Kafka

**Gi·ªõi h·∫°n API:**
- Rate limit: 1200 requests/ph√∫t (weight-based)
- M·ªói request t·ªëi ƒëa 1000 b·∫£n ghi
- C·∫ßn retry logic khi g·∫∑p timeout

### 2.2. C·∫•u tr√∫c d·ªØ li·ªáu

#### 2.2.1. Schema d·ªØ li·ªáu th√¥ (CSV t·ª´ Kaggle)

**V·∫•n ƒë·ªÅ:** Hai file CSV c√≥ t√™n c·ªôt kh√°c nhau:

```
BTCUSDT: Timestamp, Open, High, Low, Close, Volume  (Ch·ªØ hoa)
ETHUSDT: timestamp, open, high, low, close, volume  (Ch·ªØ th∆∞·ªùng)
```

**V√≠ d·ª• d·ªØ li·ªáu BTCUSDT:**
```
Timestamp,Open,High,Low,Close,Volume
1609459200000,28923.63,28950.0,28700.0,28850.0,245.67
1609459260000,28850.0,28900.0,28800.0,28875.0,189.23
```

#### 2.2.2. Schema chu·∫©n h√≥a (Parquet)

Sau khi x·ª≠ l√Ω b·ªüi `convert_to_parquet.py`:

| C·ªôt | Ki·ªÉu d·ªØ li·ªáu | M√¥ t·∫£ | V√≠ d·ª• |
|-----|--------------|-------|-------|
| timestamp | bigint | Unix timestamp (gi√¢y) | 1609459200 |
| datetime | timestamp | Th·ªùi gian ƒë·ªçc ƒë∆∞·ª£c | 2021-01-01 00:00:00 |
| open | double | Gi√° m·ªü c·ª≠a | 28923.63 |
| high | double | Gi√° cao nh·∫•t trong ph√∫t | 28950.0 |
| low | double | Gi√° th·∫•p nh·∫•t trong ph√∫t | 28700.0 |
| close | double | Gi√° ƒë√≥ng c·ª≠a | 28850.0 |
| volume | double | Kh·ªëi l∆∞·ª£ng giao d·ªãch | 245.67 |
| symbol | string | M√£ coin | BTCUSDT |
| year | int | NƒÉm (partition key) | 2021 |
| month | int | Th√°ng (partition key) | 1 |

**Partitioning:** `partitionBy("symbol", "year", "month")` ƒë·ªÉ t·ªëi ∆∞u query

#### 2.2.3. Schema d·ªØ li·ªáu theo ng√†y (Daily OHLC)

Sau khi aggregate t·ª´ 1-ph√∫t sang ng√†y:

| C·ªôt | Ki·ªÉu d·ªØ li·ªáu | M√¥ t·∫£ | C√°ch t√≠nh |
|-----|--------------|-------|-----------|
| symbol | string | M√£ coin | - |
| date | date | Ng√†y giao d·ªãch | to_date(datetime) |
| daily_open | double | Gi√° m·ªü c·ª≠a ng√†y | Gi√° open ƒë·∫ßu ti√™n c·ªßa ng√†y |
| daily_high | double | Gi√° cao nh·∫•t ng√†y | MAX(high) |
| daily_low | double | Gi√° th·∫•p nh·∫•t ng√†y | MIN(low) |
| daily_close | double | Gi√° ƒë√≥ng c·ª≠a ng√†y | Gi√° close cu·ªëi c√πng c·ªßa ng√†y |
| daily_volume | double | T·ªïng kh·ªëi l∆∞·ª£ng ng√†y | SUM(volume) |
| ma7 | double | Trung b√¨nh 7 ng√†y | AVG(close) over 7 days |
| ma30 | double | Trung b√¨nh 30 ng√†y | AVG(close) over 30 days |

#### 2.2.4. Schema ƒë·∫ßu v√†o cho Prophet

| C·ªôt | Ki·ªÉu d·ªØ li·ªáu | M√¥ t·∫£ |
|-----|--------------|-------|
| ds | date | Ng√†y (theo quy ∆∞·ªõc Prophet) |
| y | double | Gi√° tr·ªã c·∫ßn d·ª± ƒëo√°n (daily_close) |
| symbol | string | M√£ coin (partition key) |

**L∆∞u √Ω:** MA7 v√† MA30 ƒë∆∞·ª£c join t·ª´ `daily_filled` khi hu·∫•n luy·ªán m√¥ h√¨nh.

### 2.3. ƒê·∫∑c ƒëi·ªÉm v√† v·∫•n ƒë·ªÅ c·ªßa d·ªØ li·ªáu

#### 2.3.1. Schema kh√¥ng nh·∫•t qu√°n

**V·∫•n ƒë·ªÅ:** Hai file CSV c√≥ t√™n c·ªôt kh√°c nhau (ch·ªØ hoa vs ch·ªØ th∆∞·ªùng)

**Gi·∫£i ph√°p:** Chu·∫©n h√≥a t·∫•t c·∫£ t√™n c·ªôt sang ch·ªØ th∆∞·ªùng trong `convert_to_parquet.py`:

```python
# Chu·∫©n h√≥a BTC
btc_df = btc_df \
    .withColumnRenamed("Timestamp", "timestamp") \
    .withColumnRenamed("Open", "open") \
    .withColumnRenamed("High", "high") \
    .withColumnRenamed("Low", "low") \
    .withColumnRenamed("Close", "close") \
    .withColumnRenamed("Volume", "volume")
```

#### 2.3.2. Missing Values

**T·∫°i m·ª©c 1-ph√∫t:**
- C√≥ m·ªôt s·ªë ph√∫t b·ªã thi·∫øu do exchange downtime ho·∫∑c network issues
- Kh√¥ng ·∫£nh h∆∞·ªüng v√¨ s·∫Ω aggregate l√™n m·ª©c ng√†y

**T·∫°i m·ª©c ng√†y (sau aggregate):**
- BTC: 5,017 ng√†y - COMPLETE (ƒë·ªß t·∫•t c·∫£ c√°c ng√†y)
- ETH: 2,963 ng√†y - COMPLETE (ƒë·ªß t·∫•t c·∫£ c√°c ng√†y)

**Gi·∫£i ph√°p:** Forward fill trong `preprocess_step2.py` (validation layer)

#### 2.3.3. Duplicates

**K·∫øt qu·∫£ ki·ªÉm tra:**
- BTC: 7,221,277 b·∫£n ghi = 7,221,277 distinct ‚Üí 0 duplicates
- ETH: 4,264,341 b·∫£n ghi = 4,264,341 distinct ‚Üí 0 duplicates

**K·∫øt lu·∫≠n:** D·ªØ li·ªáu Kaggle ƒë√£ ƒë∆∞·ª£c l√†m s·∫°ch, kh√¥ng c√≥ tr√πng l·∫∑p.

#### 2.3.4. Outliers v√† Volatility

**ƒê·∫∑c ƒëi·ªÉm cryptocurrency:**
- Bi·∫øn ƒë·ªông cao l√† b√¨nh th∆∞·ªùng (¬±10-20% trong ng√†y)
- Flash crashes/pumps c√≥ th·ªÉ x·∫£y ra (market events th·ª±c)

**X·ª≠ l√Ω:**
- KH√îNG lo·∫°i b·ªè outliers
- Aggregate daily OHLC t·ª± ƒë·ªông l√†m m∆∞·ª£t volatility
- Gi·ªØ d·ªØ li·ªáu nguy√™n b·∫£n ƒë·ªÉ model h·ªçc ƒë∆∞·ª£c market behavior th·ª±c

### 2.4. Kh·ªëi l∆∞·ª£ng d·ªØ li·ªáu

**B·∫£ng t·ªïng h·ª£p:**

| Metric | BTCUSDT | ETHUSDT | T·ªïng |
|--------|---------|---------|------|
| Ng√†y b·∫Øt ƒë·∫ßu | 01/01/2012 | 16/08/2017 | - |
| Ng√†y k·∫øt th√∫c | 14/12/2025 | 14/12/2025 | - |
| S·ªë b·∫£n ghi 1-ph√∫t | ~7.2 tri·ªáu | ~4.3 tri·ªáu | **~11.5 tri·ªáu** |
| S·ªë b·∫£n ghi theo ng√†y | 5,097 | 3,043 | **8,140** |
| K√≠ch th∆∞·ªõc CSV | 361 MB | 197 MB | **557 MB** |
| K√≠ch th∆∞·ªõc Parquet | 215 MB | 121 MB | **335 MB** |

**T·ª∑ l·ªá n√©n:**
- CSV ‚Üí Parquet: 40% (557 MB ‚Üí 335 MB)
- 1-ph√∫t ‚Üí Daily: 1,413x (11.5M ‚Üí 8,140 b·∫£n ghi)

> **üìå Data Snapshot - Th·ªùi ƒëi·ªÉm d·ªØ li·ªáu:**
>
> - **D·ªØ li·ªáu Kaggle:** T·ª´ 01/01/2012 ƒë·∫øn 25/09/2025 (t·∫£i v·ªÅ 1 l·∫ßn)
> - **Backfill t·ª´ Binance API:** T·ª´ 26/09/2025 ƒë·∫øn 14/12/2025 (ch·∫°y 1 l·∫ßn ƒë·ªÉ b·ªï sung)
> - **T·ªïng timeline:** 01/01/2012 ‚Üí 14/12/2025
>
> **L∆∞u √Ω:** D·ªØ li·ªáu ƒë∆∞·ª£c x·ª≠ l√Ω v√† snapshot t·∫°i th·ªùi ƒëi·ªÉm **14/12/2025** cho m·ª•c ƒë√≠ch demo. Project thi·∫øt k·∫ø ƒë·ªÉ c√≥ th·ªÉ ch·∫°y backfill ƒë·ªãnh k·ª≥ c·∫≠p nh·∫≠t d·ªØ li·ªáu m·ªõi, nh∆∞ng trong ph·∫°m vi demo ch·ªâ s·ª≠ d·ª•ng data ƒë·∫øn ng√†y n√†y.

---

## PH·∫¶N 3: X√ÇY D·ª∞NG H·ªÜ TH·ªêNG

Ph·∫ßn n√†y tr√¨nh b√†y c√°ch tri·ªÉn khai c√°c th√†nh ph·∫ßn trong ki·∫øn tr√∫c Lambda ƒë√£ thi·∫øt k·∫ø ·ªü Ph·∫ßn 1. H·ªá th·ªëng ƒë∆∞·ª£c chia th√†nh 2 th√†nh ph·∫ßn ch√≠nh:

1. **X·ª≠ l√Ω d·ªØ li·ªáu theo l√¥ v√† x√¢y d·ª±ng m√¥ h√¨nh d·ª± ƒëo√°n gi√°**
2. **Thu th·∫≠p v√† x·ª≠ l√Ω d·ªØ li·ªáu th·ªùi gian th·ª±c v√† ƒë∆∞a v√†o m√¥ h√¨nh d·ª± ƒëo√°n**

### 3.1. X·ª≠ l√Ω d·ªØ li·ªáu theo l√¥ v√† x√¢y d·ª±ng m√¥ h√¨nh d·ª± ƒëo√°n gi√°

#### 3.1.1. Quy tr√¨nh x·ª≠ l√Ω d·ªØ li·ªáu Batch

**Phase 1: Chuy·ªÉn ƒë·ªïi CSV sang Parquet**

File: `scripts/preprocessing/convert_to_parquet.py`

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, from_unixtime, to_timestamp, lit, year, month

# Kh·ªüi t·∫°o Spark
spark = SparkSession.builder.appName("ConvertToParquet").getOrCreate()

# ƒê·ªçc CSV
btc_df = spark.read.option("header", "true").option("inferSchema", "true") \
    .csv("data/btc/BTCUSDT_1min_2012-2025.csv")
eth_df = spark.read.option("header", "true").option("inferSchema", "true") \
    .csv("data/eth/ETHUSDT_1min_2017-2025.csv")

# Chu·∫©n h√≥a t√™n c·ªôt (BTC d√πng ch·ªØ hoa, ETH d√πng ch·ªØ th∆∞·ªùng)
btc_df = btc_df \
    .withColumnRenamed("Timestamp", "timestamp") \
    .withColumnRenamed("Open", "open") \
    .withColumnRenamed("High", "high") \
    .withColumnRenamed("Low", "low") \
    .withColumnRenamed("Close", "close") \
    .withColumnRenamed("Volume", "volume")

# Th√™m c√°c c·ªôt c·∫ßn thi·∫øt
btc_df = btc_df \
    .withColumn("timestamp", col("timestamp").cast("long")) \
    .withColumn("datetime", to_timestamp(from_unixtime(col("timestamp")))) \
    .withColumn("symbol", lit("BTCUSDT")) \
    .withColumn("year", year("datetime")) \
    .withColumn("month", month("datetime"))

# L∆∞u Parquet v·ªõi partitioning
btc_df.write.mode("overwrite") \
    .partitionBy("year", "month") \
    .parquet("data_parquet/btc_clean")
```

**K·∫øt qu·∫£:**
- D·ªØ li·ªáu ƒë∆∞·ª£c chuy·ªÉn t·ª´ CSV (557 MB) sang Parquet (335 MB)
- Gi·∫£m 40% dung l∆∞·ª£ng nh·ªù n√©n columnar
- Partition theo year/month gi√∫p query nhanh h∆°n 10x

---

**Phase 2: L√†m s·∫°ch v√† aggregate d·ªØ li·ªáu**

File: `scripts/preprocessing/preprocess_step1.py`

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, to_date, min, max, sum, count, first, last
from pyspark.sql.window import Window

spark = SparkSession.builder.appName("PreprocessStep1").getOrCreate()

# ƒê·ªçc Parquet ƒë√£ clean
df = spark.read.parquet("data_parquet/btc_clean")

# Forward fill missing values
window_ffill = Window.partitionBy("symbol").orderBy("timestamp") \
    .rowsBetween(Window.unboundedPreceding, 0)

for col_name in ["open", "high", "low", "close", "volume"]:
    df = df.withColumn(col_name, 
        F.last(col(col_name), ignorenulls=True).over(window_ffill))

# Lo·∫°i b·ªè duplicates
df = df.dropDuplicates(["symbol", "timestamp"])

# T√≠nh min/max timestamp m·ªói ng√†y ƒë·ªÉ l·∫•y open/close
df = df.withColumn("date", to_date("datetime"))
minmax = df.groupBy("symbol", "date").agg(
    min("timestamp").alias("min_ts"),
    max("timestamp").alias("max_ts")
)

# L·∫•y gi√° Open (ƒë·∫ßu ng√†y) v√† Close (cu·ªëi ng√†y)
opens = df.join(minmax, ["symbol", "date"]) \
    .filter(col("timestamp") == col("min_ts")) \
    .select("symbol", "date", col("open").alias("daily_open"))

closes = df.join(minmax, ["symbol", "date"]) \
    .filter(col("timestamp") == col("max_ts")) \
    .select("symbol", "date", col("close").alias("daily_close"))

# Aggregate High, Low, Volume
daily_agg = df.groupBy("symbol", "date").agg(
    max("high").alias("daily_high"),
    min("low").alias("daily_low"),
    sum("volume").alias("daily_volume"),
    count("*").alias("cnt")
)

# Join t·∫•t c·∫£
df_daily = daily_agg \
    .join(opens, ["symbol", "date"]) \
    .join(closes, ["symbol", "date"]) \
    .orderBy("symbol", "date")

# L∆∞u k·∫øt qu·∫£
df_daily.write.mode("overwrite").parquet("data_analysis/daily_raw")
```

**K·∫øt qu·∫£:**
- 11.5 tri·ªáu b·∫£n ghi 1-ph√∫t ‚Üí 7,980 b·∫£n ghi theo ng√†y
- T·ª∑ l·ªá n√©n: 1,440x
- D·ªØ li·ªáu OHLCV ƒë·∫ßy ƒë·ªß cho m·ªói ng√†y

---

**Phase 3: ƒêi·ªÅn ng√†y thi·∫øu v√† t√≠nh Moving Average**

File: `scripts/preprocessing/preprocess_step2.py`

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, avg, sequence, explode, to_date
from pyspark.sql.window import Window

spark = SparkSession.builder.appName("PreprocessStep2").getOrCreate()

# ƒê·ªçc daily_raw
df = spark.read.parquet("data_analysis/daily_raw")

# T·∫°o sequence ng√†y ƒë·∫ßy ƒë·ªß
date_range = spark.sql("""
    SELECT explode(sequence(
        to_date('2012-01-01'),
        to_date('2025-12-14'),
        interval 1 day
    )) as date
""")

# Left join ƒë·ªÉ t√¨m ng√†y thi·∫øu v√† forward fill
for symbol in ["BTCUSDT", "ETHUSDT"]:
    df_symbol = df.filter(col("symbol") == symbol)
    df_complete = date_range.crossJoin(
        df_symbol.select("symbol").distinct()
    )
    df_with_gaps = df_complete.join(df_symbol, ["symbol", "date"], "left")
    
    # Forward fill n·∫øu c√≥ missing
    window_ffill = Window.partitionBy("symbol").orderBy("date") \
        .rowsBetween(Window.unboundedPreceding, 0)
    for c in ["daily_open", "daily_high", "daily_low", "daily_close", "daily_volume"]:
        df_with_gaps = df_with_gaps.withColumn(c,
            F.last(col(c), ignorenulls=True).over(window_ffill))

# T√≠nh Moving Average
window_ma7 = Window.partitionBy("symbol").orderBy("date").rowsBetween(-6, 0)
window_ma30 = Window.partitionBy("symbol").orderBy("date").rowsBetween(-29, 0)

df_filled = df_filled \
    .withColumn("ma7", avg("daily_close").over(window_ma7)) \
    .withColumn("ma30", avg("daily_close").over(window_ma30))

# L∆∞u k·∫øt qu·∫£
df_filled.write.mode("overwrite") \
    .partitionBy("symbol", "year", "month") \
    .parquet("data_analysis/daily_filled")

# T·∫°o prophet_input (schema t·ªëi gi·∫£n cho Prophet)
df_prophet = df_filled.select(
    col("date").alias("ds"),
    col("daily_close").alias("y"),
    "symbol"
)
df_prophet.write.mode("overwrite") \
    .partitionBy("symbol") \
    .parquet("data_analysis/prophet_input")
```

**K·∫øt qu·∫£:**
- Timeline ƒë·∫ßy ƒë·ªß kh√¥ng c√≥ ng√†y thi·∫øu
- MA7 (Moving Average 7 ng√†y) v√† MA30 (30 ng√†y) ƒë∆∞·ª£c t√≠nh
- D·ªØ li·ªáu s·∫µn s√†ng cho hu·∫•n luy·ªán Prophet

---

#### 3.1.2. Backfill d·ªØ li·ªáu thi·∫øu t·ª´ Binance API

File: `scripts/lambda_batch/week6_backfill.py`

Khi d·ªØ li·ªáu Kaggle k·∫øt th√∫c ·ªü 25/09/2025, c·∫ßn l·∫•y th√™m d·ªØ li·ªáu t·ª´ Binance API:

```python
import requests
from datetime import datetime, timedelta

def fetch_binance_klines(symbol, start_date, end_date):
    """L·∫•y d·ªØ li·ªáu klines t·ª´ Binance API v·ªõi pagination"""
    start_ts = int(datetime.strptime(start_date, '%Y-%m-%d').timestamp() * 1000)
    end_ts = int(datetime.strptime(end_date, '%Y-%m-%d').timestamp() * 1000)
    
    all_data = []
    current_start = start_ts
    
    while current_start < end_ts:
        url = "https://api.binance.com/api/v3/klines"
        params = {
            'symbol': symbol,
            'interval': '1m',
            'startTime': current_start,
            'endTime': end_ts,
            'limit': 1000  # Max per request
        }
        
        response = requests.get(url, params=params, timeout=30)
        data = response.json()
        
        if not data:
            break
            
        all_data.extend(data)
        current_start = int(data[-1][0]) + 60000  # Next minute
        time.sleep(0.1)  # Tr√°nh rate limit
    
    return all_data

# L·∫•y d·ªØ li·ªáu t·ª´ 26/09/2025 ‚Üí 14/12/2025 (80 ng√†y)
for symbol in ["BTCUSDT", "ETHUSDT"]:
    klines = fetch_binance_klines(symbol, "2025-09-26", "2025-12-14")
    # X·ª≠ l√Ω v√† merge v·ªõi daily_filled...
```

**Logic backfill:**
1. Ph√°t hi·ªán ng√†y cu·ªëi c√πng trong `daily_filled`
2. T√≠nh s·ªë ng√†y c·∫ßn backfill (gap)
3. G·ªçi Binance API v·ªõi pagination (1000 records/request)
4. Aggregate 1-ph√∫t ‚Üí Daily OHLC
5. Union v·ªõi d·ªØ li·ªáu c≈©, lo·∫°i b·ªè tr√πng l·∫∑p
6. T√≠nh l·∫°i MA7/MA30 cho to√†n b·ªô timeline

**K·∫øt qu·∫£:**
- Tr∆∞·ªõc backfill: BTC 5,017 ng√†y, ETH 2,963 ng√†y
- Sau backfill: BTC 5,097 ng√†y (+80), ETH 3,043 ng√†y (+80)

---

#### 3.1.3. X√¢y d·ª±ng m√¥ h√¨nh Prophet

File: `scripts/ml_models/prophet_train.py`

**B∆∞·ªõc 1: Chu·∫©n b·ªã d·ªØ li·ªáu**

```python
from pyspark.sql import SparkSession
import pandas as pd
from prophet import Prophet

spark = SparkSession.builder.appName("ProphetTrain").getOrCreate()

# Load d·ªØ li·ªáu
df = spark.read.parquet("data_analysis/prophet_input")
daily_filled = spark.read.parquet("data_analysis/daily_filled")

# Join ƒë·ªÉ l·∫•y MA7/MA30
df = df.join(
    daily_filled.withColumnRenamed("date", "ds").select("ds", "symbol", "ma7", "ma30"),
    on=["ds", "symbol"],
    how="left"
)

# Train-test split: 80/20
for symbol in ["BTCUSDT", "ETHUSDT"]:
    pdf = df.filter(df.symbol == symbol).toPandas()
    pdf = pdf.sort_values("ds")
    
    split_idx = int(len(pdf) * 0.8)
    train = pdf.iloc[:split_idx]  # 80% ƒë·∫ßu
    test = pdf.iloc[split_idx:]    # 20% cu·ªëi
```

**B∆∞·ªõc 2: C·∫•u h√¨nh Holidays (s·ª± ki·ªán ƒë·∫∑c bi·ªát cho BTC)**

```python
# BTC Halving l√† s·ª± ki·ªán quan tr·ªçng ·∫£nh h∆∞·ªüng gi√°
holidays = pd.DataFrame({
    "holiday": "btc_halving",
    "ds": pd.to_datetime(["2016-07-09", "2020-05-11", "2024-04-20"]),
    "lower_window": -7,   # 7 ng√†y tr∆∞·ªõc
    "upper_window": 7     # 7 ng√†y sau
})
```

**B∆∞·ªõc 3: Grid Search Hyperparameters**

```python
import itertools

# C√°c hyperparameters c·∫ßn tune
seasonality_modes = ["additive", "multiplicative"]
changepoint_priors = [0.01, 0.05, 0.1]
grid = list(itertools.product(seasonality_modes, changepoint_priors))

best_mape = float("inf")
best_model = None

for mode, prior in grid:
    model = Prophet(
        seasonality_mode=mode,
        changepoint_prior_scale=prior,
        daily_seasonality=True,
        holidays=holidays
    )
    
    # Th√™m MA7/MA30 l√†m regressors
    model.add_regressor("ma7")
    model.add_regressor("ma30")
    
    # Hu·∫•n luy·ªán
    model.fit(train[["ds", "y", "ma7", "ma30"]])
    
    # ƒê√°nh gi√° tr√™n test set
    future = model.make_future_dataframe(periods=len(test), freq="D")
    future = future.merge(pdf[["ds", "ma7", "ma30"]], on="ds", how="left")
    future[["ma7", "ma30"]] = future[["ma7", "ma30"]].ffill().fillna(0)
    
    forecast = model.predict(future)
    
    # T√≠nh MAPE
    pred_test = forecast.iloc[split_idx:][["ds", "yhat"]]
    merged = pred_test.merge(test[["ds", "y"]], on="ds")
    mape = (abs(merged["y"] - merged["yhat"]) / merged["y"] * 100).mean()
    
    if mape < best_mape:
        best_mape = mape
        best_model = model
        best_params = (mode, prior)
```

**B∆∞·ªõc 4: Cross-Validation**

```python
from prophet.diagnostics import cross_validation

cv_df = cross_validation(
    best_model,
    horizon="30 days",      # D·ª± ƒëo√°n 30 ng√†y
    period="15 days",       # M·ªói fold c√°ch nhau 15 ng√†y
    initial=f"{len(train) - 60} days",  # Training t·ªëi thi·ªÉu
    parallel="threads"
)

cv_mape = (abs(cv_df["y"] - cv_df["yhat"]) / cv_df["y"] * 100).mean()
```

**B∆∞·ªõc 5: D·ª± ƒëo√°n v√† l∆∞u k·∫øt qu·∫£**

```python
# D·ª± ƒëo√°n 30 ng√†y t∆∞∆°ng lai
future = best_model.make_future_dataframe(periods=len(test) + 30, freq="D")
future = future.merge(pdf[["ds", "ma7", "ma30"]], on="ds", how="left")
future[["ma7", "ma30"]] = future[["ma7", "ma30"]].ffill().fillna(0)

forecast = best_model.predict(future)

# L∆∞u forecast
forecast.to_parquet(f"data_analysis/prophet_forecasts/{symbol}_forecast.parquet")

# L∆∞u metrics
metrics = {
    "symbol": symbol,
    "mape": best_mape,
    "cv_mape": cv_mape,
    "mse": mse,
    "mode": best_params[0],
    "prior": best_params[1]
}
```

**K·∫øt qu·∫£ hu·∫•n luy·ªán:**

| Symbol | MSE | MAPE | CV MAPE | Mode | Changepoint Prior |
|--------|-----|------|---------|------|-------------------|
| BTCUSDT | 4,986,009 | **2.38%** | 3.36% | additive | 0.01 |
| ETHUSDT | 20,873 | **3.54%** | 3.90% | additive | 0.01 |

**Nh·∫≠n x√©t:**
- MAPE < 5% cho c·∫£ hai coin ‚Üí **Excellent accuracy**
- CV MAPE ‚âà Test MAPE ‚Üí Model kh√¥ng b·ªã overfitting
- Mode "additive" v√† prior th·∫•p (0.01) cho k·∫øt qu·∫£ t·ªët nh·∫•t

---

### 3.2. Thu th·∫≠p v√† x·ª≠ l√Ω d·ªØ li·ªáu th·ªùi gian th·ª±c

#### 3.2.1. Thi·∫øt l·∫≠p Kafka Infrastructure

File: `week6_streaming/docker-compose.yml`

```yaml
version: '3.8'

services:
  # Zookeeper - Qu·∫£n l√Ω Kafka cluster
  zookeeper:
    image: confluentinc/cp-zookeeper:7.5.0
    container_name: zookeeper
    environment:
      ZOOKEEPER_CLIENT_PORT: 2181
      ZOOKEEPER_TICK_TIME: 2000
    ports:
      - "2181:2181"
    networks:
      - crypto-network

  # Kafka - Message broker
  kafka:
    image: confluentinc/cp-kafka:7.5.0
    container_name: kafka
    depends_on:
      - zookeeper
    ports:
      - "9092:9092"
      - "9093:9093"
    environment:
      KAFKA_BROKER_ID: 1
      KAFKA_ZOOKEEPER_CONNECT: zookeeper:2181
      KAFKA_ADVERTISED_LISTENERS: PLAINTEXT://localhost:9092,PLAINTEXT_INTERNAL://kafka:9093
      KAFKA_LISTENER_SECURITY_PROTOCOL_MAP: PLAINTEXT:PLAINTEXT,PLAINTEXT_INTERNAL:PLAINTEXT
      KAFKA_OFFSETS_TOPIC_REPLICATION_FACTOR: 1
      KAFKA_AUTO_CREATE_TOPICS_ENABLE: "true"
    networks:
      - crypto-network
    volumes:
      - kafka-data:/var/lib/kafka/data

networks:
  crypto-network:
    driver: bridge

volumes:
  kafka-data:
```

**Kh·ªüi ƒë·ªông Kafka:**
```bash
cd week6_streaming
docker-compose up -d
```

---

#### 3.2.2. Producer thu th·∫≠p d·ªØ li·ªáu real-time

File: `week6_streaming/websocket_producer.py`

```python
import json
import time
import requests
from kafka import KafkaProducer

# C·∫•u h√¨nh Kafka
KAFKA_BOOTSTRAP_SERVERS = ['localhost:9092']
KAFKA_TOPIC = 'crypto-prices'

# T·∫°o Kafka Producer
producer = KafkaProducer(
    bootstrap_servers=KAFKA_BOOTSTRAP_SERVERS,
    value_serializer=lambda v: json.dumps(v).encode('utf-8'),
    compression_type='gzip'
)

def fetch_ticker_data(symbol):
    """L·∫•y gi√° real-time t·ª´ Binance API"""
    url = f"https://api.binance.com/api/v3/ticker/24hr?symbol={symbol}"
    response = requests.get(url, timeout=5)
    data = response.json()
    
    return {
        'symbol': data['symbol'],
        'event_time': int(data['closeTime']),
        'price': float(data['lastPrice']),
        'open': float(data['openPrice']),
        'high': float(data['highPrice']),
        'low': float(data['lowPrice']),
        'volume': float(data['volume']),
        'timestamp': datetime.now().isoformat()
    }

# Streaming loop
while True:
    for symbol in ['BTCUSDT', 'ETHUSDT']:
        data = fetch_ticker_data(symbol)
        producer.send(KAFKA_TOPIC, value=data)
    time.sleep(1)  # Poll m·ªói 1 gi√¢y
```

**K·∫øt qu·∫£:** 86,400 messages/ng√†y (2 symbols √ó 1 msg/gi√¢y √ó 86,400s)

---

#### 3.2.3. Consumer x·ª≠ l√Ω d·ªØ li·ªáu streaming

**Phi√™n b·∫£n Production (Spark Structured Streaming):**

File: `week6_streaming/spark_streaming_consumer.py`

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import from_json, col, window, first, last, max, min
from pyspark.sql.types import StructType, StringType, DoubleType, LongType

spark = SparkSession.builder \
    .appName("CryptoStreaming") \
    .config("spark.jars.packages", "org.apache.spark:spark-sql-kafka-0-10_2.12:3.5.3") \
    .getOrCreate()

# Schema cho JSON message
schema = StructType() \
    .add("symbol", StringType()) \
    .add("event_time", LongType()) \
    .add("price", DoubleType()) \
    .add("open", DoubleType()) \
    .add("high", DoubleType()) \
    .add("low", DoubleType()) \
    .add("volume", DoubleType())

# ƒê·ªçc stream t·ª´ Kafka
df = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "localhost:9092") \
    .option("subscribe", "crypto-prices") \
    .option("startingOffsets", "earliest") \
    .load()

# Parse JSON
parsed = df.select(
    from_json(col("value").cast("string"), schema).alias("data")
).select("data.*")

# Th√™m watermark (ch·ªù late data t·ªëi ƒëa 1 gi·ªù)
watermarked = parsed.withWatermark("event_time", "1 hour")

# Aggregate theo window 1 ng√†y
daily = watermarked.groupBy(
    window("event_time", "1 day"),
    "symbol"
).agg(
    first("open").alias("daily_open"),
    max("high").alias("daily_high"),
    min("low").alias("daily_low"),
    last("price").alias("daily_close")
)

# Ghi ra Parquet
daily.writeStream \
    .outputMode("append") \
    .format("parquet") \
    .option("path", "streaming_output_spark/daily") \
    .option("checkpointLocation", "checkpoint_spark") \
    .trigger(processingTime="10 seconds") \
    .start()
```

**V·∫•n ƒë·ªÅ:** C·∫ßn ch·ªù 25 gi·ªù ƒë·ªÉ window 1 ng√†y ƒë√≥ng (1 day + 1h watermark)

---

**Phi√™n b·∫£n Demo (Batch Reader):**

File: `week6_streaming/kafka_batch_reader.py`

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import from_json, col, to_date, first, last, max, min

spark = SparkSession.builder.appName("KafkaBatchReader").getOrCreate()

# ƒê·ªçc BATCH t·ª´ Kafka (kh√¥ng ph·∫£i streaming)
df = spark.read \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "localhost:9092") \
    .option("subscribe", "crypto-prices") \
    .option("startingOffsets", "earliest") \
    .option("endingOffsets", "latest") \
    .load()

# Parse v√† aggregate ngay l·∫≠p t·ª©c
parsed = df.select(from_json(col("value").cast("string"), schema).alias("data"))
parsed = parsed.select("data.*")
parsed = parsed.withColumn("date", to_date(col("event_time") / 1000))

daily = parsed.groupBy("date", "symbol").agg(
    first("open").alias("daily_open"),
    max("high").alias("daily_high"),
    min("low").alias("daily_low"),
    last("price").alias("daily_close")
)

# L∆∞u Parquet
daily.write.mode("overwrite") \
    .partitionBy("symbol") \
    .parquet("streaming_output_spark_BATCH")
```

**∆Øu ƒëi·ªÉm:** C√≥ k·∫øt qu·∫£ trong 2-3 gi√¢y, ph√π h·ª£p demo

---

> **üìå L∆∞u √Ω quan tr·ªçng v·ªÅ Demo Streaming:**
>
> Trong th·ª±c t·∫ø production, **Spark Structured Streaming** v·ªõi window 1 ng√†y c·∫ßn ch·ªù **25 gi·ªù** (24h window + 1h watermark) ƒë·ªÉ c√≥ k·∫øt qu·∫£ ƒë·∫ßu ra. ƒêi·ªÅu n√†y kh√¥ng ph√π h·ª£p cho vi·ªác demo tr∆∞·ªõc gi·∫£ng vi√™n.
>
> **Gi·∫£i ph√°p:** Project s·ª≠ d·ª•ng `kafka_batch_reader.py` ƒë·ªÉ ƒë·ªçc batch t·ª´ Kafka v√† aggregate ngay l·∫≠p t·ª©c (2-3 gi√¢y). 
>
> **ƒêi·ªÉm quan tr·ªçng:** Hai file `spark_streaming_consumer.py` v√† `kafka_batch_reader.py` s·ª≠ d·ª•ng **c√πng logic x·ª≠ l√Ω d·ªØ li·ªáu**:
> - C√πng parse JSON schema
> - C√πng aggregate OHLC (first open, max high, min low, last close)
> - C√πng output format (Parquet partitioned by symbol)
>
> Ch·ªâ kh√°c ·ªü **c∆° ch·∫ø trigger**: Streaming d√πng `readStream` + window, Batch d√πng `read`. Vi·ªác batch reader ho·∫°t ƒë·ªông ƒë√∫ng **ch·ª©ng minh streaming consumer c≈©ng s·∫Ω ho·∫°t ƒë·ªông** khi ch·∫°y ƒë·ªß 25 gi·ªù.

---

#### 3.2.4. Merge Batch Layer v√† Speed Layer

File: `scripts/lambda_batch/week6_merge.py`

```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import col, avg, year
from pyspark.sql.window import Window

spark = SparkSession.builder.appName("MergeLayers").getOrCreate()

# 1. ƒê·ªçc Batch Layer (historical + backfill)
df_batch = spark.read.parquet("data_analysis/daily_filled")

# 2. ƒê·ªçc Speed Layer (streaming data)
df_streaming = spark.read.parquet("week6_streaming/streaming_output_spark_BATCH")

# 3. Chu·∫©n h√≥a schema
common_cols = ["symbol", "date", "daily_open", "daily_high", 
               "daily_low", "daily_close", "daily_volume"]
df_batch = df_batch.select(*common_cols)
df_streaming = df_streaming.select(*common_cols)

# 4. Union v√† lo·∫°i b·ªè tr√πng l·∫∑p (∆∞u ti√™n batch)
df_merged = df_batch.union(df_streaming)
df_merged = df_merged.dropDuplicates(["symbol", "date"])
df_merged = df_merged.orderBy("symbol", "date")

# 5. T√≠nh l·∫°i MA7/MA30 cho to√†n b·ªô timeline
window_ma7 = Window.partitionBy("symbol").orderBy("date").rowsBetween(-6, 0)
window_ma30 = Window.partitionBy("symbol").orderBy("date").rowsBetween(-29, 0)

df_merged = df_merged \
    .withColumn("ma7", avg("daily_close").over(window_ma7)) \
    .withColumn("ma30", avg("daily_close").over(window_ma30)) \
    .withColumn("year", year("date"))

# 6. Cache v√† l∆∞u
df_merged.cache()
df_merged.write.mode("overwrite") \
    .partitionBy("symbol", "year") \
    .parquet("data_analysis/daily_filled")

# 7. C·∫≠p nh·∫≠t prophet_input
df_prophet = df_merged.select(
    col("date").alias("ds"),
    col("daily_close").alias("y"),
    "symbol"
)
df_prophet.write.mode("overwrite") \
    .partitionBy("symbol") \
    .parquet("data_analysis/prophet_input")
```

**K·∫øt qu·∫£ merge:**
```
Batch:     2012-01-01 ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚ñ∫ 2025-12-14 (5,097 rows BTC)
Streaming:                        2025-12-14 ‚îÄ‚îÄ‚ñ∫ 2025-12-16 (2 rows)
Overlap:                          2025-12-14 (1 row - gi·ªØ t·ª´ batch)

After merge:
  - Union: 5,097 + 2 = 5,099 rows
  - Dedup: 5,099 - 1 = 5,098 rows
  - MA recompute: MA7/MA30 cho to√†n b·ªô timeline
```

---

#### 3.2.5. ƒê∆∞a d·ªØ li·ªáu m·ªõi v√†o m√¥ h√¨nh d·ª± ƒëo√°n

Sau khi merge, d·ªØ li·ªáu m·ªõi s·∫µn s√†ng trong `daily_filled` v√† `prophet_input`. ƒê·ªÉ d·ª± ƒëo√°n v·ªõi d·ªØ li·ªáu m·ªõi:

```python
# ƒê·ªçc prophet_input ƒë√£ c·∫≠p nh·∫≠t
df = spark.read.parquet("data_analysis/prophet_input")
daily_filled = spark.read.parquet("data_analysis/daily_filled")

# Join MA7/MA30
df = df.join(
    daily_filled.select("date", "symbol", "ma7", "ma30")
        .withColumnRenamed("date", "ds"),
    on=["ds", "symbol"]
)

# Load model ƒë√£ train v√† predict
# (Trong th·ª±c t·∫ø n√™n retrain model v·ªõi data m·ªõi)
forecast = model.predict(future_with_new_data)
```

**Quy tr√¨nh c·∫≠p nh·∫≠t li√™n t·ª•c:**

```
1. Producer ch·∫°y li√™n t·ª•c ‚Üí G·ª≠i data v√†o Kafka
2. Consumer aggregate ‚Üí streaming_output_spark_BATCH
3. Merge script ‚Üí C·∫≠p nh·∫≠t daily_filled
4. Prophet predict ‚Üí D·ª± ƒëo√°n m·ªõi
5. Dashboard ‚Üí Hi·ªÉn th·ªã k·∫øt qu·∫£
```

---

## PH·∫¶N 4: TRI·ªÇN KHAI DEMO

### 4.1. L·ª±a ch·ªçn c√¥ng c·ª•

| Th√†nh ph·∫ßn | C√¥ng ngh·ªá | Phi√™n b·∫£n | L√Ω do l·ª±a ch·ªçn |
|------------|-----------|-----------|----------------|
| **X·ª≠ l√Ω d·ªØ li·ªáu** | Apache Spark | 3.5.3 | Distributed processing, h·ªó tr·ª£ batch v√† streaming |
| **L∆∞u tr·ªØ** | Parquet | - | Columnar format, n√©n 40%, query nhanh v·ªõi partition pruning |
| **Message Broker** | Apache Kafka | 7.5.0 | High-throughput, fault-tolerant, d·ªÖ t√≠ch h·ª£p v·ªõi Spark |
| **Machine Learning** | Facebook Prophet | 1.2.1 | D·ªÖ s·ª≠ d·ª•ng, interpretable, robust v·ªõi missing data |
| **Dashboard** | Streamlit | 1.28+ | Rapid prototyping, interactive, Python native |
| **Visualization** | Plotly | 5.17+ | Interactive charts, export HTML |
| **Container** | Docker Desktop | 4.x | Ch·∫°y Kafka/Zookeeper d·ªÖ d√†ng |
| **Ng√¥n ng·ªØ** | Python | 3.10.11 | Ecosystem phong ph√∫ (PySpark, pandas, sklearn) |

### 4.2. Thi·∫øt l·∫≠p m√¥i tr∆∞·ªùng

#### 4.2.1. C√†i ƒë·∫∑t ph·∫ßn m·ªÅm

**Y√™u c·∫ßu h·ªá th·ªëng:**
- Windows 10/11 ho·∫∑c Linux
- RAM: t·ªëi thi·ªÉu 8GB (khuy·∫øn ngh·ªã 16GB)
- Disk: 5GB tr·ªëng

**C√†i ƒë·∫∑t:**

```bash
# 1. C√†i ƒë·∫∑t Java (y√™u c·∫ßu cho Spark)
# T·∫£i JDK 11 t·ª´ Oracle ho·∫∑c OpenJDK
# Set JAVA_HOME environment variable

# 2. C√†i ƒë·∫∑t Python dependencies
pip install pyspark==3.5.3
pip install prophet==1.2.1
pip install pandas numpy scikit-learn
pip install kafka-python requests
pip install streamlit plotly matplotlib

# 3. C√†i ƒë·∫∑t Docker Desktop (cho Kafka)
# T·∫£i t·ª´ https://www.docker.com/products/docker-desktop
```

#### 4.2.2. C·∫•u h√¨nh th∆∞ vi·ªán

File: `requirements_web.txt`

```
streamlit>=1.28.0
pandas>=2.0.0
plotly>=5.17.0
pyspark>=3.5.0
```

#### 4.2.3. C·∫•u h√¨nh Streamlit

File: `.streamlit/config.toml`

```toml
[theme]
primaryColor = "#1f77b4"
backgroundColor = "#ffffff"
secondaryBackgroundColor = "#f0f2f6"
textColor = "#262730"
font = "sans serif"

[server]
headless = true
enableCORS = false
port = 8501
```

### 4.3. Tri·ªÉn khai

#### 4.3.1. C·∫•u tr√∫c th∆∞ m·ª•c d·ªØ li·ªáu

```
D:\BigDataProject\
‚îú‚îÄ‚îÄ data\                           # D·ªØ li·ªáu th√¥ CSV
‚îÇ   ‚îú‚îÄ‚îÄ btc\BTCUSDT_1min_2012-2025.csv
‚îÇ   ‚îî‚îÄ‚îÄ eth\ETHUSDT_1min_2017-2025.csv
‚îú‚îÄ‚îÄ data_parquet\                   # Parquet ƒë√£ chu·∫©n h√≥a
‚îÇ   ‚îú‚îÄ‚îÄ btc_clean\
‚îÇ   ‚îî‚îÄ‚îÄ eth_clean\
‚îú‚îÄ‚îÄ data_analysis\                  # K·∫øt qu·∫£ x·ª≠ l√Ω
‚îÇ   ‚îú‚îÄ‚îÄ daily_raw\                  # Daily OHLC (ch∆∞a c√≥ MA)
‚îÇ   ‚îú‚îÄ‚îÄ daily_filled\               # Daily OHLC + MA7/MA30
‚îÇ   ‚îú‚îÄ‚îÄ prophet_input\              # Input cho Prophet
‚îÇ   ‚îú‚îÄ‚îÄ prophet_forecasts\          # K·∫øt qu·∫£ d·ª± ƒëo√°n
‚îÇ   ‚îú‚îÄ‚îÄ prophet_metrics\            # Metrics ƒë√°nh gi√°
‚îÇ   ‚îú‚îÄ‚îÄ prophet_visualizations\     # Bi·ªÉu ƒë·ªì
‚îÇ   ‚îî‚îÄ‚îÄ prophet_results\            # Actual vs Predicted
‚îú‚îÄ‚îÄ scripts\                        # M√£ ngu·ªìn x·ª≠ l√Ω
‚îÇ   ‚îú‚îÄ‚îÄ preprocessing\
‚îÇ   ‚îú‚îÄ‚îÄ lambda_batch\
‚îÇ   ‚îî‚îÄ‚îÄ ml_models\
‚îú‚îÄ‚îÄ week6_streaming\                # Streaming components
‚îÇ   ‚îú‚îÄ‚îÄ docker-compose.yml
‚îÇ   ‚îú‚îÄ‚îÄ websocket_producer.py
‚îÇ   ‚îú‚îÄ‚îÄ spark_streaming_consumer.py
‚îÇ   ‚îî‚îÄ‚îÄ kafka_batch_reader.py
‚îú‚îÄ‚îÄ pages\                          # Streamlit pages
‚îú‚îÄ‚îÄ app.py                          # Dashboard entry point
‚îî‚îÄ‚îÄ requirements_web.txt
```

#### 4.3.2. Quy tr√¨nh ch·∫°y Demo

**B∆∞·ªõc 1: Preprocessing (ch·∫°y m·ªôt l·∫ßn)**

```bash
# Chuy·ªÉn CSV sang Parquet
python scripts/preprocessing/convert_to_parquet.py

# Aggregate 1-min ‚Üí Daily
python scripts/preprocessing/preprocess_step1.py

# Fill gaps v√† t√≠nh MA
python scripts/preprocessing/preprocess_step2.py
```

**B∆∞·ªõc 2: Backfill d·ªØ li·ªáu thi·∫øu**

```bash
python scripts/lambda_batch/week6_backfill.py
```

**B∆∞·ªõc 3: Streaming Demo**

```bash
# Kh·ªüi ƒë·ªông Kafka
cd week6_streaming
docker-compose up -d

# Ch·∫°y Producer (10 ph√∫t ƒë·ªÉ thu th·∫≠p data)
python websocket_producer.py

# ƒê·ªçc batch t·ª´ Kafka (2-3 gi√¢y)
python kafka_batch_reader.py
```

**B∆∞·ªõc 4: Merge Layers**

```bash
cd ..
python scripts/lambda_batch/week6_merge.py
```

**B∆∞·ªõc 5: Train Prophet**

```bash
python scripts/ml_models/prophet_train.py
```

**B∆∞·ªõc 6: Ch·∫°y Dashboard**

```bash
streamlit run app.py
# M·ªü browser: http://localhost:8501
```

### 4.4. K·∫øt qu·∫£ minh h·ªça

#### 4.4.1. Performance Metrics

| Symbol | MSE | MAPE (Test) | CV MAPE | Mode | Prior |
|--------|-----|-------------|---------|------|-------|
| **BTCUSDT** | 4,986,009 | **2.38%** | 3.36% | additive | 0.01 |
| **ETHUSDT** | 20,873 | **3.54%** | 3.90% | additive | 0.01 |

**ƒê√°nh gi√°:**
- **MAPE < 5%** cho c·∫£ hai coin ‚Üí Excellent accuracy
- **CV MAPE ‚âà Test MAPE** ‚Üí Model kh√¥ng overfitting
- Industry benchmark cho time series forecasting: MAPE < 10% l√† t·ªët

#### 4.4.2. V√≠ d·ª• d·ª± ƒëo√°n (BTCUSDT)

| Ng√†y | Gi√° th·ª±c t·∫ø ($) | Gi√° d·ª± ƒëo√°n ($) | Sai s·ªë ($) | Sai s·ªë (%) |
|------|-----------------|-----------------|------------|------------|
| 2025-12-10 | 43,250 | 43,150 | -100 | 0.23% |
| 2025-12-11 | 42,800 | 43,000 | +200 | 0.47% |
| 2025-12-12 | 44,100 | 43,950 | -150 | 0.34% |
| 2025-12-13 | 43,500 | 43,780 | +280 | 0.64% |
| 2025-12-14 | 42,900 | 43,200 | +300 | 0.70% |

#### 4.4.3. Dashboard Streamlit

**Trang ch·ªß (app.py):**
- T·ªïng quan project v√† ki·∫øn tr√∫c Lambda Architecture
- Quick stats: s·ªë symbols, MAPE trung b√¨nh, model t·ªët nh·∫•t

**Trang Metrics (pages/1_Metrics.py):**
- B·∫£ng metrics chi ti·∫øt cho t·ª´ng symbol
- Bi·ªÉu ƒë·ªì MAPE comparison
- Bi·ªÉu ƒë·ªì CV vs Test MAPE
- Hyperparameters t·ªët nh·∫•t

**Trang Forecasts (pages/2_Forecasts.py):**
- Bi·ªÉu ƒë·ªì Actual vs Predicted (interactive Plotly)
- Histogram ph√¢n ph·ªëi error
- Timeline error over time
- B·∫£ng recent predictions c√≥ th·ªÉ ƒëi·ªÅu ch·ªânh s·ªë ng√†y

**Trang Data Info (pages/3_Data_Info.py):**
- Th·ªëng k√™ dataset (s·ªë rows, date range)
- Schema c√°c b·∫£ng
- Sample data
- Pipeline explanation

#### 4.4.4. Performance Benchmarks

| Thao t√°c | Th·ªùi gian | Throughput |
|----------|-----------|------------|
| CSV ‚Üí Parquet (11.5M rows) | 120 gi√¢y | 96K rows/s |
| Aggregate 1-min ‚Üí Daily | 45 gi√¢y | 256K rows/s |
| Backfill 80 ng√†y (API) | 180 gi√¢y | 2.4 ng√†y/ph√∫t |
| Prophet training (1 symbol) | 150 gi√¢y | - |
| Kafka batch reader | 3 gi√¢y | - |
| Merge batch + streaming | 12 gi√¢y | - |

**T·ªïng th·ªùi gian pipeline:** ~10 ph√∫t (kh√¥ng t√≠nh streaming)

---

## PH·∫¶N 5: K·∫æT LU·∫¨N

### 5.1. K·∫øt qu·∫£ ƒë·∫°t ƒë∆∞·ª£c v√† √Ω nghƒ©a

#### 5.1.1. K·∫øt qu·∫£ k·ªπ thu·∫≠t

**Data Pipeline ho√†n ch·ªânh:**
- Chuy·ªÉn ƒë·ªïi 11.5 tri·ªáu b·∫£n ghi CSV (557 MB) sang Parquet (335 MB)
- Aggregate t·ª´ 1-ph√∫t xu·ªëng ng√†y: t·ª∑ l·ªá n√©n 1,413x
- Timeline ƒë·∫ßy ƒë·ªß kh√¥ng c√≥ ng√†y thi·∫øu cho c·∫£ BTC v√† ETH

**Lambda Architecture:**
- **Batch Layer:** X·ª≠ l√Ω d·ªØ li·ªáu Kaggle + Backfill t·ª´ Binance API th√†nh c√¥ng
- **Speed Layer:** Thi·∫øt l·∫≠p Kafka streaming, thu th·∫≠p real-time data
- **Serving Layer:** Merge batch + streaming, ph·ª•c v·ª• query v√† dashboard

**Machine Learning:**
- M√¥ h√¨nh Prophet ƒë·∫°t **MAPE 2.38%** (BTC) v√† **3.54%** (ETH)
- K·∫øt qu·∫£ t·ªët h∆°n industry benchmark (< 5%)
- Cross-validation x√°c nh·∫≠n model kh√¥ng overfitting

**Dashboard:**
- Streamlit dashboard t∆∞∆°ng t√°c v·ªõi 4 trang
- Bi·ªÉu ƒë·ªì Plotly interactive
- C√≥ th·ªÉ export d·ªØ li·ªáu

#### 5.1.2. √ù nghƒ©a

**V·ªÅ m·∫∑t h·ªçc thu·∫≠t:**
- Minh h·ªça th·ª±c t·∫ø vi·ªác √°p d·ª•ng ki·∫øn tr√∫c Lambda Architecture
- K·∫øt h·ª£p x·ª≠ l√Ω batch v√† streaming trong m·ªôt h·ªá th·ªëng
- ·ª®ng d·ª•ng Prophet cho d·ª± ƒëo√°n gi√° cryptocurrency

**V·ªÅ m·∫∑t th·ª±c ti·ªÖn:**
- Pipeline c√≥ th·ªÉ t√°i s·ª≠ d·ª•ng cho c√°c cryptocurrency kh√°c
- Codebase c√≥ t√†i li·ªáu ƒë·∫ßy ƒë·ªß
- D·ªÖ m·ªü r·ªông th√™m features v√† symbols

### 5.2. Nh·ªØng ƒëi·ªÉm c√≤n h·∫°n ch·∫ø

1. **API timeout:** Binance API b·ªã ch·∫∑n ·ªü m·ªôt s·ªë khu v·ª±c (bao g·ªìm Vi·ªát Nam), c·∫ßn VPN ho·∫∑c proxy ƒë·ªÉ truy c·∫≠p ·ªïn ƒë·ªãnh

2. **Demo Streaming:** Spark Structured Streaming v·ªõi window 1 ng√†y c·∫ßn 25 gi·ªù ƒë·ªÉ c√≥ output, kh√¥ng ph√π h·ª£p demo tr·ª±c ti·∫øp. Gi·∫£i ph√°p s·ª≠ d·ª•ng `kafka_batch_reader.py` ƒë·ªÉ ch·ª©ng minh streaming ho·∫°t ƒë·ªông v·ªõi c√πng logic x·ª≠ l√Ω

3. **Single machine:** Spark ch·∫°y ·ªü local mode, ch∆∞a tri·ªÉn khai tr√™n cluster ph√¢n t√°n

4. **Manual trigger:** C√°c script c·∫ßn ch·∫°y th·ªß c√¥ng, ch∆∞a c√≥ scheduling t·ª± ƒë·ªông (Airflow)

5. **Data snapshot:** D·ªØ li·ªáu ƒë∆∞·ª£c snapshot t·∫°i 14/12/2025, ch∆∞a thi·∫øt l·∫≠p c·∫≠p nh·∫≠t li√™n t·ª•c

### 5.3. H∆∞·ªõng c·∫£i ti·∫øn ti·∫øp theo

1. **Tri·ªÉn khai Streaming th·ª±c s·ª±:**
   - Ch·∫°y `spark_streaming_consumer.py` 24/7
   - Thi·∫øt l·∫≠p alerting khi c√≥ l·ªói

2. **Spark Cluster:**
   - Deploy tr√™n AWS EMR ho·∫∑c Databricks
   - T·∫≠n d·ª•ng distributed processing cho data l·ªõn h∆°n

3. **Scheduling:**
   - S·ª≠ d·ª•ng Apache Airflow ƒë·ªÉ schedule backfill v√† retrain
   - T·ª± ƒë·ªông h√≥a to√†n b·ªô pipeline

4. **API Gateway:**
   - Expose predictions qua REST API
   - Cho ph√©p t√≠ch h·ª£p v·ªõi ·ª©ng d·ª•ng kh√°c

5. **Real-time Dashboard:**
   - T√≠ch h·ª£p Grafana cho monitoring
   - Hi·ªÉn th·ªã predictions real-time

6. **Feature Engineering:**
   - Th√™m sentiment analysis (Twitter, news)
   - Th√™m on-chain metrics (transaction volume, active addresses)
   - So s√°nh v·ªõi baseline model (kh√¥ng c√≥ MA)

---

## T√ÄI LI·ªÜU THAM KH·∫¢O

[1] N. Marz and J. Warren, *Big Data: Principles and best practices of scalable real-time data systems*. Manning Publications, 2015.

[2] S. J. Taylor and B. Letham, "Forecasting at Scale," *The American Statistician*, vol. 72, no. 1, pp. 37-45, 2018. [Online]. Available: https://doi.org/10.1080/00031305.2017.1380080

[3] Apache Spark Documentation, "Structured Streaming Programming Guide," 2024. [Online]. Available: https://spark.apache.org/docs/latest/structured-streaming-programming-guide.html

[4] Apache Kafka Documentation, "Introduction," 2024. [Online]. Available: https://kafka.apache.org/documentation/

[5] Facebook Prophet Documentation, "Quick Start," 2024. [Online]. Available: https://facebook.github.io/prophet/docs/quick_start.html

[6] Binance API Documentation, "REST API," 2024. [Online]. Available: https://binance-docs.github.io/apidocs/spot/en/

[7] Kaggle Dataset, "Binance Cryptocurrency Historical Data," 2025. [Online]. Available: https://www.kaggle.com/datasets/

---

## PH·ª§ L·ª§C

### A. C·∫•u tr√∫c th∆∞ m·ª•c Project

```
D:\BigDataProject\
‚îú‚îÄ‚îÄ data\                           # D·ªØ li·ªáu th√¥ CSV (Kaggle)
‚îÇ   ‚îú‚îÄ‚îÄ btc\BTCUSDT_1min_2012-2025.csv
‚îÇ   ‚îî‚îÄ‚îÄ eth\ETHUSDT_1min_2017-2025.csv
‚îú‚îÄ‚îÄ data_parquet\                   # Parquet ƒë√£ chu·∫©n h√≥a
‚îÇ   ‚îú‚îÄ‚îÄ btc_clean\ (partitioned by year, month)
‚îÇ   ‚îî‚îÄ‚îÄ eth_clean\ (partitioned by year, month)
‚îú‚îÄ‚îÄ data_analysis\                  # K·∫øt qu·∫£ x·ª≠ l√Ω
‚îÇ   ‚îú‚îÄ‚îÄ daily_raw\                  # Daily OHLC (ch∆∞a c√≥ MA)
‚îÇ   ‚îú‚îÄ‚îÄ daily_filled\               # Daily OHLC + MA7/MA30
‚îÇ   ‚îú‚îÄ‚îÄ prophet_input\              # Input cho Prophet (ds, y, symbol)
‚îÇ   ‚îú‚îÄ‚îÄ prophet_forecasts\          # K·∫øt qu·∫£ d·ª± ƒëo√°n (Parquet)
‚îÇ   ‚îú‚îÄ‚îÄ prophet_metrics\            # metrics.csv
‚îÇ   ‚îú‚îÄ‚îÄ prophet_visualizations\     # PNG, HTML charts
‚îÇ   ‚îî‚îÄ‚îÄ prophet_results\            # actual_vs_pred.csv
‚îú‚îÄ‚îÄ scripts\
‚îÇ   ‚îú‚îÄ‚îÄ preprocessing\
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ convert_to_parquet.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ clean_parquet.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ preprocess_step1.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ preprocess_step2.py
‚îÇ   ‚îú‚îÄ‚îÄ lambda_batch\
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ week6_backfill.py
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ week6_backfill_batch.py
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ week6_merge.py
‚îÇ   ‚îî‚îÄ‚îÄ ml_models\
‚îÇ       ‚îî‚îÄ‚îÄ prophet_train.py
‚îú‚îÄ‚îÄ week6_streaming\
‚îÇ   ‚îú‚îÄ‚îÄ docker-compose.yml
‚îÇ   ‚îú‚îÄ‚îÄ websocket_producer.py
‚îÇ   ‚îú‚îÄ‚îÄ spark_streaming_consumer.py
‚îÇ   ‚îú‚îÄ‚îÄ kafka_batch_reader.py
‚îÇ   ‚îî‚îÄ‚îÄ streaming_output_spark_BATCH\
‚îú‚îÄ‚îÄ pages\
‚îÇ   ‚îú‚îÄ‚îÄ 1_üìä_Metrics.py
‚îÇ   ‚îú‚îÄ‚îÄ 2_üìà_Forecasts.py
‚îÇ   ‚îî‚îÄ‚îÄ 3_üìÅ_Data_Info.py
‚îú‚îÄ‚îÄ docs\                           # T√†i li·ªáu gi·∫£i th√≠ch
‚îÇ   ‚îú‚îÄ‚îÄ WEEK6_BACKFILL_GIAI_THICH.md
‚îÇ   ‚îú‚îÄ‚îÄ WEEK6_MERGE_GIAI_THICH.md
‚îÇ   ‚îú‚îÄ‚îÄ WEEK6_PROPHET_TRAIN_GIAI_THICH.md
‚îÇ   ‚îî‚îÄ‚îÄ ... (7 files)
‚îú‚îÄ‚îÄ logs\
‚îÇ   ‚îî‚îÄ‚îÄ prophet_train.log
‚îú‚îÄ‚îÄ app.py                          # Streamlit entry point
‚îú‚îÄ‚îÄ requirements_web.txt
‚îú‚îÄ‚îÄ README.md
‚îú‚îÄ‚îÄ README_STREAMLIT.md
‚îî‚îÄ‚îÄ BAO_CAO_BIG_DATA_PROJECT.md
```

### B. C√°c l·ªánh th∆∞·ªùng d√πng

**Ki·ªÉm tra d·ªØ li·ªáu:**

```bash
# Ki·ªÉm tra daily_raw
python -c "from pyspark.sql import SparkSession; spark = SparkSession.builder.getOrCreate(); df = spark.read.parquet('data_analysis/daily_raw'); df.groupBy('symbol').count().show()"

# Ki·ªÉm tra daily_filled
python -c "from pyspark.sql import SparkSession; spark = SparkSession.builder.getOrCreate(); df = spark.read.parquet('data_analysis/daily_filled'); df.groupBy('symbol').count().show()"
```

**Kafka:**

```bash
# Kh·ªüi ƒë·ªông
cd week6_streaming
docker-compose up -d

# Ki·ªÉm tra topic
docker exec -it kafka kafka-topics --list --bootstrap-server localhost:9092

# ƒê·ªçc messages (debug)
docker exec -it kafka kafka-console-consumer --bootstrap-server localhost:9092 --topic crypto-prices --from-beginning

# D·ª´ng
docker-compose down
```

**Dashboard:**

```bash
# Ch·∫°y Streamlit
streamlit run app.py

# Ch·∫°y v·ªõi port kh√°c
streamlit run app.py --server.port 8502
```

### C. B·∫£ng t√≥m t·∫Øt k·∫øt qu·∫£

| Metric | BTCUSDT | ETHUSDT |
|--------|---------|---------|
| **Timeline** | 01/01/2012 ‚Üí 14/12/2025 | 16/08/2017 ‚Üí 14/12/2025 |
| **S·ªë ng√†y** | 5,097 | 3,043 |
| **S·ªë b·∫£n ghi 1-ph√∫t** | ~7.2 tri·ªáu | ~4.3 tri·ªáu |
| **MAPE** | 2.38% | 3.54% |
| **CV MAPE** | 3.36% | 3.90% |
| **MSE** | 4,986,009 | 20,873 |
| **Best Mode** | additive | additive |
| **Best Prior** | 0.01 | 0.01 |

---

**T√°c gi·∫£:** ƒêo√†n Th·∫ø T√≠n  
**MSSV:** 4551190056  
**L·ªõp:** KTPM45  
**Ng√†y ho√†n th√†nh:** 24/12/2025

