# üìä M√É MERMAID CHO B√ÅO C√ÅO

File n√†y ch·ª©a c√°c m√£ Mermaid ƒë·ªÉ v·∫Ω s∆° ƒë·ªì cho b√°o c√°o. B·∫°n c√≥ th·ªÉ copy v√† paste v√†o [Mermaid Live Editor](https://mermaid.live/) ƒë·ªÉ v·∫Ω, sau ƒë√≥ export ra PNG/SVG.

---

## 1. S∆† ƒê·ªí KI·∫æN TR√öC LAMBDA ARCHITECTURE (Ph·∫ßn M·ªü ƒë·∫ßu / Ph·∫ßn 1.1)

**V·ªã tr√≠ ch√®n:** Sau d√≤ng "Ki·∫øn tr√∫c Lambda Architecture:" trong ph·∫ßn M·ªü ƒë·∫ßu

```mermaid
flowchart TB
    subgraph DS["üì• DATA SOURCES"]
        KAGGLE["üìÅ Kaggle CSV<br/>11.5M rows<br/>2012-2025"]
        BINANCE["üåê Binance API<br/>Real-time<br/>Backfill"]
    end

    subgraph BL["‚öôÔ∏è BATCH LAYER"]
        CSV2PQ["CSV ‚Üí Parquet"]
        AGG["Aggregate<br/>1-min ‚Üí Daily"]
        MA["Compute<br/>MA7/MA30"]
        BACKFILL["Backfill<br/>Binance API"]
        PROPHET["üîÆ Prophet<br/>Training"]
    end

    subgraph SL["‚ö° SPEED LAYER"]
        PRODUCER["Producer<br/>Poll 1s"]
        KAFKA["Apache Kafka<br/>crypto-prices"]
        CONSUMER["Spark Consumer<br/>Batch Reader"]
    end

    subgraph SVL["üìä SERVING LAYER"]
        MERGE["Merge<br/>Batch + Speed"]
        DAILY["daily_filled<br/>Unified Data"]
        DASHBOARD["üìà Streamlit<br/>Dashboard"]
    end

    KAGGLE --> CSV2PQ
    BINANCE --> BACKFILL
    BINANCE --> PRODUCER

    CSV2PQ --> AGG --> MA --> BACKFILL --> PROPHET
    PRODUCER --> KAFKA --> CONSUMER

    PROPHET --> MERGE
    CONSUMER --> MERGE
    MERGE --> DAILY --> DASHBOARD

    style DS fill:#e1f5fe
    style BL fill:#fff3e0
    style SL fill:#f3e5f5
    style SVL fill:#e8f5e9
```

---

## 2. S∆† ƒê·ªí DATA PIPELINE (Ph·∫ßn 2.4 ho·∫∑c Ph·∫ßn 3.1)

**V·ªã tr√≠ ch√®n:** Sau ph·∫ßn "Kh·ªëi l∆∞·ª£ng d·ªØ li·ªáu" ho·∫∑c ƒë·∫ßu Ph·∫ßn 3

```mermaid
flowchart LR
    subgraph INPUT["üì• Input"]
        CSV["CSV Files<br/>557 MB<br/>11.5M rows"]
    end

    subgraph PREPROCESSING["‚öôÔ∏è Preprocessing"]
        P1["convert_to_parquet.py<br/>‚Üí Parquet 335MB"]
        P2["preprocess_step1.py<br/>‚Üí daily_raw 7,980 rows"]
        P3["preprocess_step2.py<br/>‚Üí daily_filled + MA"]
    end

    subgraph BATCH["üì¶ Batch Layer"]
        BF["week6_backfill.py<br/>+80 days from API"]
        MG["week6_merge.py<br/>Merge batch+streaming"]
    end

    subgraph ML["üîÆ Machine Learning"]
        PT["prophet_train.py<br/>Grid Search + CV"]
        FC["Forecasts<br/>MAPE 2.38%"]
    end

    subgraph OUTPUT["üìä Output"]
        DASH["Streamlit<br/>Dashboard"]
    end

    CSV --> P1 --> P2 --> P3 --> BF --> MG --> PT --> FC --> DASH

    style INPUT fill:#ffebee
    style PREPROCESSING fill:#e3f2fd
    style BATCH fill:#fff8e1
    style ML fill:#f3e5f5
    style OUTPUT fill:#e8f5e9
```

---

## 3. S∆† ƒê·ªí SPEED LAYER / STREAMING (Ph·∫ßn 3.2)

**V·ªã tr√≠ ch√®n:** ƒê·∫ßu ph·∫ßn 3.2 "Thu th·∫≠p v√† x·ª≠ l√Ω d·ªØ li·ªáu th·ªùi gian th·ª±c"

```mermaid
flowchart LR
    subgraph API["üåê Binance API"]
        TICKER["GET /ticker/24hr<br/>BTCUSDT, ETHUSDT"]
    end

    subgraph PRODUCER["üì§ Producer"]
        WS["websocket_producer.py<br/>Poll every 1 second"]
    end

    subgraph KAFKA["üì® Apache Kafka"]
        TOPIC["Topic: crypto-prices<br/>Port 9092"]
    end

    subgraph CONSUMER["üì• Consumer"]
        STREAM["spark_streaming_consumer.py<br/>‚è±Ô∏è Window 1 day + 1h watermark<br/>‚ö†Ô∏è C·∫ßn 25h cho output"]
        BATCH["kafka_batch_reader.py<br/>‚úÖ Batch mode<br/>‚ö° 2-3 gi√¢y c√≥ k·∫øt qu·∫£"]
    end

    subgraph OUTPUT["üìÅ Output"]
        PARQUET["streaming_output_spark_BATCH/<br/>Daily OHLC Parquet"]
    end

    API --> TICKER --> WS
    WS -->|"JSON messages<br/>86,400/day"| TOPIC
    TOPIC --> STREAM
    TOPIC --> BATCH
    BATCH -->|"Demo Mode"| PARQUET

    style API fill:#e3f2fd
    style PRODUCER fill:#fff3e0
    style KAFKA fill:#fce4ec
    style CONSUMER fill:#f3e5f5
    style OUTPUT fill:#e8f5e9
```

---

## 4. S∆† ƒê·ªí PROPHET TRAINING PROCESS (Ph·∫ßn 3.1.3)

**V·ªã tr√≠ ch√®n:** ƒê·∫ßu ph·∫ßn 3.1.3 "X√¢y d·ª±ng m√¥ h√¨nh Prophet"

```mermaid
flowchart TB
    subgraph DATA["üì• Data Preparation"]
        INPUT["prophet_input<br/>+ daily_filled"]
        SPLIT["Train/Test Split<br/>80% / 20%"]
        HOLIDAY["BTC Halving<br/>Holidays"]
    end

    subgraph TUNING["üîß Hyperparameter Tuning"]
        GRID["Grid Search<br/>6 combinations"]
        MODE["seasonality_mode<br/>additive / multiplicative"]
        PRIOR["changepoint_prior<br/>0.01 / 0.05 / 0.1"]
    end

    subgraph TRAINING["üîÆ Training"]
        REGRESSOR["Add Regressors<br/>MA7, MA30"]
        FIT["model.fit()"]
        PREDICT["model.predict()"]
    end

    subgraph EVAL["üìä Evaluation"]
        MAPE["MAPE Calculation"]
        CV["Cross-Validation<br/>30 days horizon"]
        BEST["Select Best Model"]
    end

    subgraph OUTPUT["üìÅ Output"]
        FORECAST["Forecasts<br/>Parquet"]
        METRICS["Metrics<br/>CSV"]
        VIS["Visualizations<br/>PNG, HTML"]
    end

    INPUT --> SPLIT --> HOLIDAY
    HOLIDAY --> GRID
    GRID --> MODE & PRIOR
    MODE & PRIOR --> REGRESSOR --> FIT --> PREDICT
    PREDICT --> MAPE --> CV --> BEST
    BEST --> FORECAST & METRICS & VIS

    style DATA fill:#e3f2fd
    style TUNING fill:#fff8e1
    style TRAINING fill:#f3e5f5
    style EVAL fill:#ffebee
    style OUTPUT fill:#e8f5e9
```

---

## 5. S∆† ƒê·ªí SERVING LAYER / MERGE (Ph·∫ßn 3.2.4)

**V·ªã tr√≠ ch√®n:** ƒê·∫ßu ph·∫ßn 3.2.4 "Merge Batch Layer v√† Speed Layer"

```mermaid
flowchart TB
    subgraph BATCH["‚öôÔ∏è Batch Layer"]
        DAILY_FILLED["daily_filled/<br/>5,097 rows BTC<br/>3,043 rows ETH"]
    end

    subgraph SPEED["‚ö° Speed Layer"]
        STREAMING["streaming_output_spark_BATCH/<br/>2 rows (m·ªõi)"]
    end

    subgraph MERGE["üîÑ Merge Process"]
        UNION["UNION<br/>batch + streaming"]
        DEDUP["dropDuplicates<br/>(symbol, date)"]
        RECOMPUTE["Recompute<br/>MA7 / MA30"]
        CACHE["Cache DataFrame"]
    end

    subgraph OUTPUT["üìä Output"]
        NEW_DAILY["daily_filled/<br/>Updated"]
        PROPHET_INPUT["prophet_input/<br/>Updated"]
        DASHBOARD["Streamlit<br/>Dashboard"]
    end

    DAILY_FILLED --> UNION
    STREAMING --> UNION
    UNION --> DEDUP --> RECOMPUTE --> CACHE
    CACHE --> NEW_DAILY & PROPHET_INPUT
    NEW_DAILY --> DASHBOARD

    style BATCH fill:#fff3e0
    style SPEED fill:#f3e5f5
    style MERGE fill:#e3f2fd
    style OUTPUT fill:#e8f5e9
```

---

## 6. S∆† ƒê·ªí DEMO WORKFLOW (Ph·∫ßn 4.3.2)

**V·ªã tr√≠ ch√®n:** Thay th·∫ø ho·∫∑c b·ªï sung cho ph·∫ßn "Quy tr√¨nh ch·∫°y Demo"

```mermaid
flowchart TB
    subgraph STEP1["1Ô∏è‚É£ Preprocessing"]
        S1A["convert_to_parquet.py"]
        S1B["preprocess_step1.py"]
        S1C["preprocess_step2.py"]
    end

    subgraph STEP2["2Ô∏è‚É£ Backfill"]
        S2["week6_backfill.py<br/>+80 days API data"]
    end

    subgraph STEP3["3Ô∏è‚É£ Streaming Demo"]
        S3A["docker-compose up -d"]
        S3B["websocket_producer.py<br/>‚è±Ô∏è 10 ph√∫t"]
        S3C["kafka_batch_reader.py<br/>‚è±Ô∏è 2-3 gi√¢y"]
    end

    subgraph STEP4["4Ô∏è‚É£ Merge"]
        S4["week6_merge.py"]
    end

    subgraph STEP5["5Ô∏è‚É£ Train"]
        S5["prophet_train.py<br/>‚è±Ô∏è ~5 ph√∫t"]
    end

    subgraph STEP6["6Ô∏è‚É£ Dashboard"]
        S6["streamlit run app.py<br/>üåê localhost:8501"]
    end

    S1A --> S1B --> S1C --> S2 --> S3A --> S3B --> S3C --> S4 --> S5 --> S6

    style STEP1 fill:#e3f2fd
    style STEP2 fill:#fff8e1
    style STEP3 fill:#f3e5f5
    style STEP4 fill:#ffebee
    style STEP5 fill:#e8f5e9
    style STEP6 fill:#e1f5fe
```

---

## 7. S∆† ƒê·ªí TECHNOLOGY STACK (Ph·∫ßn 4.1)

**V·ªã tr√≠ ch√®n:** Sau b·∫£ng "L·ª±a ch·ªçn c√¥ng c·ª•"

```mermaid
flowchart TB
    subgraph PROCESSING["‚öôÔ∏è Processing"]
        SPARK["Apache Spark 3.5.3<br/>PySpark"]
    end

    subgraph STORAGE["üíæ Storage"]
        PARQUET["Parquet<br/>Columnar Format"]
    end

    subgraph STREAMING["üì® Streaming"]
        KAFKA["Apache Kafka 7.5.0"]
        ZK["Zookeeper"]
    end

    subgraph ML["üîÆ Machine Learning"]
        PROPHET["Facebook Prophet 1.2.1"]
        SKLEARN["scikit-learn"]
    end

    subgraph VIZ["üìä Visualization"]
        STREAMLIT["Streamlit 1.28+"]
        PLOTLY["Plotly"]
        MPL["Matplotlib"]
    end

    subgraph LANG["üêç Language"]
        PYTHON["Python 3.10.11"]
    end

    PYTHON --> SPARK & KAFKA & PROPHET & STREAMLIT
    SPARK --> PARQUET
    KAFKA --> ZK
    PROPHET --> SKLEARN
    STREAMLIT --> PLOTLY & MPL

    style PROCESSING fill:#ff9800,color:#fff
    style STORAGE fill:#4caf50,color:#fff
    style STREAMING fill:#9c27b0,color:#fff
    style ML fill:#2196f3,color:#fff
    style VIZ fill:#f44336,color:#fff
    style LANG fill:#795548,color:#fff
```

---

## 8. S∆† ƒê·ªí KQUA - ACTUAL VS PREDICTED (Ph·∫ßn 4.4)

**V·ªã tr√≠ ch√®n:** Ph·∫ßn k·∫øt qu·∫£ minh h·ªça (n·∫øu c·∫ßn)

```mermaid
xychart-beta
    title "BTCUSDT: Actual vs Predicted (Dec 2025)"
    x-axis ["Dec 10", "Dec 11", "Dec 12", "Dec 13", "Dec 14"]
    y-axis "Price ($)" 42000 --> 45000
    line "Actual" [43250, 42800, 44100, 43500, 42900]
    line "Predicted" [43150, 43000, 43950, 43780, 43200]
```

---

## üìù H∆Ø·ªöNG D·∫™N S·ª¨ D·ª§NG

1. Copy m√£ Mermaid (ph·∫ßn trong ```mermaid ... ```)
2. Truy c·∫≠p [Mermaid Live Editor](https://mermaid.live/)
3. Paste code v√†o editor
4. ƒêi·ªÅu ch·ªânh m√†u s·∫Øc/layout n·∫øu c·∫ßn
5. Export ra PNG ho·∫∑c SVG
6. Ch√®n h√¨nh v√†o b√°o c√°o Word

**L∆∞u √Ω:** M·ªôt s·ªë diagram ph·ª©c t·∫°p c√≥ th·ªÉ c·∫ßn ch·ªânh s·ª≠a th√™m trong editor ƒë·ªÉ ƒë·∫πp h∆°n.
